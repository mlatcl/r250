---
title: "R250: Unsupervised Learning with Gaussian Processes"
venue: "Virtual (Zoom)"
abstract: "<p>In this talk we give an introduction to Unsupervised Learning and Gaussian processes for students who are interested in working with Unsupervised GPs for the the R250 module.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 
date: 2021-01-21
published: 2021-01-21
week: 1
reveal: 01-unsupervised-gaussian-processes.slides.html
ipynb: 01-unsupervised-gaussian-processes.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="setup">Setup</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_notebooks/includes/notebook-setup.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_notebooks/includes/notebook-setup.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>First we download some libraries and files to support the notebook.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> urllib.request</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py&#39;</span>,<span class="st">&#39;mlai.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py&#39;</span>,<span class="st">&#39;teaching_plots.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py&#39;</span>,<span class="st">&#39;gp_tutorial.py&#39;</span>)</span></code></pre></div>
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
<h2 id="pods">pods</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/pods-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/pods-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In Sheffield we created a suite of software tools for ‘Open Data Science’. Open data science is an approach to sharing code, models and data that should make it easier for companies, health professionals and scientists to gain access to data science techniques.</p>
<p>You can also check this blog post on <a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a>.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>sods<span class="op">/</span>ods</span></code></pre></div>
<p>from the command prompt where you can access your python installation.</p>
<p>The code is also available on github: <a href="https://github.com/sods/ods" class="uri">https://github.com/sods/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual manner.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> pods</span></code></pre></div>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian Process Framework in Python</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Gaussian processes are a flexible tool for non-parametric analysis with uncertainty. The GPy software was started in Sheffield to provide a easy to use interface to GPs. One which allowed the user to focus on the modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian process models in python. This allows GPs to be combined with a wide variety of software libraries.</p>
<p>The software itself is available on <a href="https://github.com/SheffieldML/GPy">GitHub</a> and the team welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language, i.e. you specify the model rather than the algorithm. As well as a large range of covariance functions the software allows for non-Gaussian likelihoods, multivariate outputs, dimensionality reduction and approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<h2 id="high-dimensional-data">High Dimensional Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/high-dimensional-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/high-dimensional-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<ul>
<li>USPS Data Set Handwritten Digit</li>
<li>3648 dimensions (64 rows, 57 columns)</li>
<li>Space contains much more than just this digit.</li>
</ul>
<h2 id="usps-samples">USPS Samples</h2>
<ul>
<li>Even if we sample every nanonsecond from now until end of universe you won’t see original six!</li>
</ul>
<h2 id="simple-model-of-digit">Simple Model of Digit</h2>
<ul>
<li>Rotate a prototype</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="im">from</span> scipy.misc <span class="im">import</span> imrotate</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>six_image <span class="op">=</span> np.hstack([np.zeros((rows, <span class="dv">3</span>)), six_image, np.zeros((rows, <span class="dv">4</span>))])</span>
<span id="cb8-2"><a href="#cb8-2"></a>dim_one <span class="op">=</span> np.asarray(six_image.shape)</span>
<span id="cb8-3"><a href="#cb8-3"></a>angles <span class="op">=</span> <span class="bu">range</span>(<span class="dv">360</span>)</span>
<span id="cb8-4"><a href="#cb8-4"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>Y <span class="op">=</span> np.zeros((<span class="bu">len</span>(angles), np.prod(dim_one)))</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="cf">for</span> angle <span class="kw">in</span> angles:</span>
<span id="cb8-7"><a href="#cb8-7"></a>    rot_image <span class="op">=</span> imrotate(six_image, angle, interp<span class="op">=</span><span class="st">&#39;nearest&#39;</span>)</span>
<span id="cb8-8"><a href="#cb8-8"></a>    dim_two <span class="op">=</span> np.asarray(rot_image.shape)</span>
<span id="cb8-9"><a href="#cb8-9"></a>    start <span class="op">=</span> [<span class="bu">int</span>(<span class="bu">round</span>((dim_two[<span class="dv">0</span>] <span class="op">-</span> dim_one[<span class="dv">0</span>])<span class="op">/</span><span class="dv">2</span>)), <span class="bu">int</span>(<span class="bu">round</span>((dim_two[<span class="dv">1</span>] <span class="op">-</span> dim_one[<span class="dv">1</span>])<span class="op">/</span><span class="dv">2</span>))]</span>
<span id="cb8-10"><a href="#cb8-10"></a>    crop_image <span class="op">=</span> rot_image[start[<span class="dv">0</span>]<span class="op">+</span>np.array(<span class="bu">range</span>(dim_one[<span class="dv">0</span>])), start[<span class="dv">1</span>]<span class="op">+</span>np.array(<span class="bu">range</span>(dim_one[<span class="dv">1</span>]))]</span>
<span id="cb8-11"><a href="#cb8-11"></a>    Y[i, :] <span class="op">=</span> crop_image.flatten()</span></code></pre></div>
<h2 id="low-dimensional-manifolds">Low Dimensional Manifolds</h2>
<ul>
<li>Pure rotation is too simple
<ul>
<li>In practice data may undergo several distortions.</li>
</ul></li>
<li>For high dimensional data with <em>structure</em>:
<ul>
<li>We expect fewer distortions than dimensions;</li>
<li>Therefore we expect the data to live on a lower dimensional manifold.</li>
<li>Conclusion: Deal with high dimensional data by looking for a lower dimensional non-linear embedding.</li>
</ul></li>
</ul>
<h1 id="latent-variables">Latent Variables</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/latent-variables.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/latent-variables.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Latent means hidden, and hidden variables are simply <em>unobservable</em> variables. The idea of a latent variable is crucial to the concept of artificial intelligence, machine learning and experimental design. A latent variable could take many forms. We might observe a man walking along a road with a large bag of clothes and we might <em>infer</em> that the man is walking to the laundrette. Our observations are a highly complex data space, the response in our eyes is processed through our visual cortex, the combination of the individual’s limb movememnts and the direction they are walking in all conflate in our heads to cause us to infer that (perhaps) the individual is going to the laundrette. We don’t <em>know</em> that the man is walking to the laundrette, but we have a model of the world that suggests that it’s a likely outcome for the very complex data. In some ways the latent variable can be seen as a <em>compression</em> of this very complex scene. If I were writing a book, I might write that “A man tripped over whilst walking to the laundrette”. In the reader’s mind an image of a man, perhaps laden with dirty clothes, may occur. All these ideas come from our expectations of the world around us. We can make further inference about the man, some of it perhaps plausible others less so. The man may be going to the laundrette because his washing machine is broken, or because he doesn’t have a large enough flat to have a washing machine, or because he’s carrying a duvet, or because he doesn’t like ironing. All of these may <em>increase</em> in probability given our observation, but they are still <em>latent</em> variables. Unless we follow the man back to his appartment, or start making other enquirires about the man, we don’t know the true answer.</p>
<p>It’s clear that to do inference about any complex system we <em>must</em> include latent variables. Latent variables are extremely powerful. In robotics, they are used to represent the <em>state</em> of the robot. The state of the robot may include its position (in x, y coordinates) its speed, its direction of facing. How are <em>these</em> variables unknown to the robot? Well the robot only posesses <em>sensors</em>, it can make observations of the nearest object in a certain direction, and it may have a map of its environment. If we represent the state of the robot as its position on a map, it may be uncertain of that position. If you go walking or running in the hills around Sheffield, you can take a very high quality ordnance survey map with you. However, unless you are a really excellent orienteer, when you are far from any given landmark, you will probably be <em>uncertain</em> about your true position on the map. These states are also latent variables.</p>
<p>In statistical analysis of experiments you try to control for each aspect of the experiment, in particular by <em>randomization</em>. So if I’m interested in the ability of a particular fertilizer to improve the yield of a particular plant I may design an experiment where I apply the fertilizer to some plants (the treatment group) and withold the fertilizer from others (the control group). I then test to see whether the yield from the treatment group is better (or worse) than the control group. I may find that I have an excellent yield for the treatment group. However, what if I’d (unknowlingly) planted all my treatment plants in a sunny part of the field, and all the control plants in a shady part of the field. That would also be a latent variable, in this case known as a <em>confounder</em>. In statistical experimental design <em>randomization</em> is used to attempt to eliminate the correlated effects of these confounders: you aim to ensure that if these confounders <em>do</em> exist their effects are not correlated with treatment and contorl. This is known as a <a href="http://en.wikipedia.org/wiki/Randomized_controlled_trial">randomized control trial</a>.</p>
<p>Greek philosophers worried a great deal about what was knowable and what was unknowable. Adherents of <a href="http://en.wikipedia.org/wiki/Skepticism">philosophical Skeptisism</a> were inspired by the idea that since your senses sometimes give you contradictory information, they cannot be trusted, and in extreme cases they chose to <em>ignore</em> their senses. This is an acknowledgement that very often the true state of the world cannot be known with precision. Unfortunately, these philosophers didn’t have a good understanding of probability, so they were unable to encapsulate their ideas through a <em>degree</em> of belief.</p>
<p>We often use language to express the compression of a complex behavior or patterns in a simpler way, for example we talk about motives as a useful distallation for a perhaps very complex patter of behavior. In physics we use principles of causation and simple laws to describe the world around us. Such motives or underlying principles are difficult to observe directly, our conclusions about them emerge over a period of time by observing indirect consequences of the latent variables.</p>
<p>Epistemic uncertainty allows us to deal with these worries by associating our degree of belief about the state of the world with a probaiblity distribution. This core idea underpins state space modelling, probabilistic graphical models and the wider field of latent variable modelling. In this session we are going to explore the idea in a simple linear system and see how it relates to <em>factor analysis</em> and <em>principal component analysis</em>.</p>
<h1 id="your-personality">Your Personality</h1>
<p>At the beginning of the 20th century there was a great deal of interest amoungst psychologists in formalizing patterns of thought. The approach they used became known as factor analysis. The principle is that we observe a potentially high dimensional vector of characteristics about an individual. To formalize this, social scientists designed questionaires. We can envisage many questions that we may ask, but the assumption is that underlying these questions there are only a few traits that dictate the behavior. These models are known as latent trait models and the analysis is sometimes known as factor analysis. The idea is that there are a few characteristic traits that we are looking to discern. These traits or factors can be extracted by assimilating the high dimensional characteristics of the individual into a few latent factors.</p>
<h2 id="factor-analysis-model">Factor Analysis Model</h2>
<p>This causes us to consider a model as follows, if we are given a high dimensional vector of features (perhaps questionaire answers) associated with an individual, <span class="math inline"><strong>y</strong></span>, we assume that these factors are actually generated from a low dimensional vector latent traits, or latent variables, which determine the personality. <br /><span class="math display"><strong>y</strong> = <strong>f</strong>(<strong>z</strong>) + <strong>ϵ</strong>,</span><br /> where <span class="math inline"><strong>f</strong>(<strong>z</strong>)</span> is a <em>vector valued</em> function that is dependent on the latent traits and <span class="math inline"><strong>ϵ</strong></span> is some corrupting noise. For simplicity, we assume that the function is given by a <em>linear</em> relationship, <br /><span class="math display"><strong>f</strong>(<strong>z</strong>) = <strong>W</strong><strong>z</strong></span><br /> where we have introduced a matrix <span class="math inline"><strong>W</strong></span> that is sometimes referred to as the <em>factor loadings</em> but we also immediately see is related to our <em>multivariate linear regression</em> models from the . That is because our vector valued function is of the form <br /><span class="math display">$$
\mathbf{f}(\mathbf{ z}) =
\begin{bmatrix} f_1(\mathbf{ z}) \\ f_2(\mathbf{ z}) \\ \vdots \\
f_p(\mathbf{ z})\end{bmatrix}
$$</span><br /> where there are <span class="math inline"><em>p</em></span> features associated with the individual. If we consider any of these functions individually we have a prediction function that looks like a regression model, <br /><span class="math display"><em>f</em><sub><em>j</em></sub>(<strong>z</strong>) = <strong>w</strong><sub><em>j</em>, :</sub><sup>⊤</sup><strong>z</strong>,</span><br /> for each element of the vector valued function, where <span class="math inline"><strong>w</strong><sub>:,<em>j</em></sub></span> is the <span class="math inline"><em>j</em></span>th column of the matrix <span class="math inline"><strong>W</strong></span>. In that context each column of <span class="math inline"><strong>W</strong></span> is a vector of <em>regression weights</em>. This is a multiple input and multiple output regression. Our inputs (or covariates) have dimensionality greater than 1 and our outputs (or response variables) also have dimensionality greater than one. Just as in a standard regression, we are assuming that we don’t observe the function directly (note that this <em>also</em> makes the function a <em>type</em> of latent variable), but we observe some corrupted variant of the function, where the corruption is given by <span class="math inline"><strong>ϵ</strong></span>. Just as in linear regression we can assume that this corruption is given by Gaussian noise, where the noise for the <span class="math inline"><em>j</em></span>th element of <span class="math inline"><strong>y</strong></span> is by, <br /><span class="math display"><em>ϵ</em><sub><em>j</em></sub> ∼ 𝒩(0,<em>σ</em><sub><em>j</em></sub><sup>2</sup>).</span><br /> Of course, just as in a regression problem we also need to make an assumption across the individual data points to form our full likelihood. Our data set now consists of many observations of <span class="math inline"><strong>y</strong></span> for diffetent individuals. We store these observations in a <em>design matrix</em>, <span class="math inline"><strong>Y</strong></span>, where each <em>row</em> of <span class="math inline"><strong>Y</strong></span> contains the observation for one individual. To emphasize that <span class="math inline"><strong>y</strong></span> is a vector derived from a row of <span class="math inline"><strong>Y</strong></span> we represent the observation of the features associated with the <span class="math inline"><em>i</em></span>th individual by <span class="math inline"><strong>y</strong><sub><em>i</em>, :</sub></span>, and place each individual in our data matrix, <br /><span class="math display">$$
\mathbf{Y}
= \begin{bmatrix} \mathbf{ y}_{1, :}^\top \\ \mathbf{ y}_{2, :}^\top \\ \vdots \\
\mathbf{ y}_{n, :}^\top\end{bmatrix},
$$</span><br /> where we have <span class="math inline"><em>n</em></span> data points. Our data matrix therefore has <span class="math inline"><em>n</em></span> rows and <span class="math inline"><em>p</em></span> columns. The point to notice here is that each data obsesrvation appears as a row vector in the design matrix (thus the transpose operation inside the brackets). Our prediction functions are now actually a <em>matrix value</em> function, <br /><span class="math display"><strong>F</strong> = <strong>Z</strong><strong>W</strong><sup>⊤</sup>,</span><br /> where for each matrix the data points are in the rows and the data features are in the columns. This implies that if we have <span class="math inline"><em>q</em></span> inputs to the function we have <span class="math inline"><strong>F</strong> ∈ ℜ<sup><em>n</em> × <em>p</em></sup></span>, <span class="math inline"><strong>W</strong> ∈ ℜ<sup><em>p</em> × <em>q</em></sup></span> and <span class="math inline"><strong>Z</strong> ∈ ℜ<sup><em>n</em> × <em>q</em></sup></span>.</p>
<h3 id="exercise-0">Exercise 0</h3>
<p>Show that, given all the definitions above, if, <br /><span class="math display"><strong>F</strong> = <strong>Z</strong><strong>W</strong><sup>⊤</sup></span><br /> and the elements of the vector valued function <span class="math inline"><strong>F</strong></span> are given by <br /><span class="math display"><em>f</em><sub><em>i</em>, <em>j</em></sub> = <em>f</em><sub><em>j</em></sub>(<strong>z</strong><sub><em>i</em>, :</sub>),</span><br /> where <span class="math inline"><strong>z</strong><sub><em>i</em>, :</sub></span> is the <span class="math inline"><em>i</em></span>th row of the latent variables, <span class="math inline"><strong>Z</strong></span>, then show that <br /><span class="math display"><em>f</em><sub><em>j</em></sub>(<strong>z</strong><sub><em>i</em>, :</sub>) = <strong>w</strong><sub><em>j</em>, :</sub><sup>⊤</sup><strong>z</strong><sub><em>i</em>, :</sub></span><br /></p>
<h2 id="latent-variables-1">Latent Variables</h2>
<p>The difference between this model and a multiple output regression is that in the regression case we are provided with the covariates <span class="math inline"><strong>Z</strong></span>, here they are <em>latent variables</em>. These variables are unknown. Just as we have done in the past for unknowns, we now treat them with a probability distribution. In <em>factor analysis</em> we assume that the latent variables have a Gaussian density which is independent across both across the latent variables associated with the different data points, and across those associated with different data features, so we have, <br /><span class="math display"><em>x</em><sub><em>i</em>, <em>j</em></sub> ∼ 𝒩(0,1),</span><br /> and we can write the density governing the latent variable associated with a single point as, <br /><span class="math display"><strong>z</strong><sub><em>i</em>, :</sub> ∼ 𝒩(<strong>0</strong>,<strong>I</strong>).</span><br /> If we consider the values of the function for the <span class="math inline"><em>i</em></span>th data point as <br /><span class="math display"><strong>f</strong><sub><em>i</em>, :</sub> = <strong>f</strong>(<strong>z</strong><sub><em>i</em>, :</sub>) = <strong>W</strong><strong>z</strong><sub><em>i</em>, :</sub></span><br /> then we can use the rules for multivariate Gaussian relationships to write that <br /><span class="math display"><strong>f</strong><sub><em>i</em>, :</sub> ∼ 𝒩(<strong>0</strong>,<strong>W</strong><strong>W</strong><sup>⊤</sup>)</span><br /> which implies that the distribution for <span class="math inline"><strong>y</strong><sub><em>i</em>, :</sub></span> is given by <br /><span class="math display"><strong>y</strong><sub><em>i</em>, :</sub> =  ∼ 𝒩(<strong>0</strong>,<strong>W</strong><strong>W</strong><sup>⊤</sup>+<strong>Σ</strong>)</span><br /> where <span class="math inline"><strong>Σ</strong></span> the covariance of the noise variable, <span class="math inline"><em>ϵ</em><sub><em>i</em>, :</sub></span> which for factor analysis is a diagonal matrix (because we have assumed that the noise was <em>independent</em> across the features), <br /><span class="math display">$$
\boldsymbol{\Sigma} = \begin{bmatrix}\sigma^2_{1} &amp; 0 &amp; 0 &amp; 0\\
0 &amp; \sigma^2_{2} &amp; 0 &amp; 0\\
                                     0 &amp; 0 &amp; \ddots &amp;
0\\
                                     0 &amp; 0 &amp; 0 &amp; \sigma^2_p\end{bmatrix}.
$$</span><br /> For completeness, we could also add in a <em>mean</em> for the data vector <span class="math inline"><strong>μ</strong></span>, <br /><span class="math display"><strong>y</strong><sub><em>i</em>, :</sub> = <strong>W</strong><strong>z</strong><sub><em>i</em>, :</sub> + <strong>μ</strong> + <strong>ϵ</strong><sub><em>i</em>, :</sub></span><br /> which would give our marginal distribution for <span class="math inline"><strong>y</strong><sub><em>i</em>, :</sub></span> a mean <span class="math inline"><strong>μ</strong></span>. However, the maximum likelihood solution for <span class="math inline"><strong>μ</strong></span> turns out to equal the empirical mean of the data, <br /><span class="math display">$$
\boldsymbol{ \mu}= \frac{1}{n} \sum_{i=1}^n
\mathbf{ y}_{i, :},
$$</span><br /> <em>regardless</em> of the form of the covariance, <span class="math inline"><strong>C</strong> = <strong>W</strong><strong>W</strong><sup>⊤</sup> + <strong>Σ</strong></span>. As a result it is very common to simply preprocess the data and ensure it is zero mean. We will follow that convention for this session.</p>
<p>The prior density over latent variables is independent, and the likelihood is independent, that means that the marginal likelihood here is also independent over the data points. Factor analysis was developed mainly in psychology and the social sciences for understanding personality and intelligence. <a href="http://en.wikipedia.org/wiki/Charles_Spearman">Charles Spearman</a> was concerned with the measurements of “the abilities of man” and is credited with the earliest version of factor analysis.</p>
<h1 id="principal-component-analysis">Principal Component Analysis</h1>
<p>In 1933 <a href="http://en.wikipedia.org/wiki/Harold_Hotelling">Harold Hotelling</a> published on <em>principal component analysis</em> the first mention of this approach <span class="citation" data-cites="Hotelling:analysis33">(Hotelling 1933)</span>. Hotelling’s inspiration was to provide mathematical foundation for factor analysis methods that were by then widely used within psychology and the social sciences. His model was a factor analysis model, but he considered the noiseless ‘limit’ of the model. In other words he took <span class="math inline"><em>σ</em><sub><em>i</em></sub><sup>2</sup> → 0</span> so that he had <br /><span class="math display"><strong>y</strong><sub><em>i</em>, :</sub> ∼ lim<sub><em>σ</em><sup>2</sup> → 0</sub>𝒩(<strong>0</strong>,<strong>W</strong><strong>W</strong><sup>⊤</sup>+<em>σ</em><sup>2</sup><strong>I</strong>).</span><br /> The paper had two unfortunate effects. Firstly, the resulting model is no longer valid probablistically, because the covariance of this Gaussian is ‘degenerate’. Because <span class="math inline"><strong>W</strong><strong>W</strong><sup>⊤</sup></span> has rank of at most <span class="math inline"><em>q</em></span> where <span class="math inline"><em>q</em> &lt; <em>p</em></span> (due to the dimensionality reduction) the determinant of the covariance is zero, meaning the inverse doesn’t exist so the density, <br /><span class="math display">$$
p(\mathbf{ y}_{i, :}|\mathbf{W}) =
\lim_{\sigma^2 \rightarrow 0} \frac{1}{(2\pi)^\frac{p}{2}
|\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}|^{-1}}
\exp\left(-\frac{1}{2}\mathbf{ y}_{i, :}\left[\mathbf{W}\mathbf{W}^\top+ \sigma^2
\mathbf{I}\right]^{-1}\mathbf{ y}_{i, :}\right),
$$</span><br /> is <em>not</em> valid for <span class="math inline"><em>q</em> &lt; <em>p</em></span> (where <span class="math inline"><strong>W</strong> ∈ ℜ<sup><em>p</em> × <em>q</em></sup></span>). This mathematical consequence is a probability density which has no ‘support’ in large regions of the space for <span class="math inline"><strong>y</strong><sub><em>i</em>, :</sub></span>. There are regions for which the probability of <span class="math inline"><strong>y</strong><sub><em>i</em>, :</sub></span> is zero. These are any regions that lie off the hyperplane defined by mapping from <span class="math inline"><strong>z</strong></span> to <span class="math inline"><strong>y</strong></span> with the matrix <span class="math inline"><strong>W</strong></span>. In factor analysis the noise corruption, <span class="math inline"><strong>ϵ</strong></span>, allows for points to be found away from the hyperplane. In Hotelling’s PCA the noise variance is zero, so there is only support for points that fall precisely on the hyperplane. Secondly, Hotelling explicity chose to rename factor analysis as principal component analysis, arguing that the factors social scientist sought were different in nature to the concept of a mathematical factor. This was unfortunate because the factor loadings, <span class="math inline"><strong>W</strong></span> can also be seen as factors in the mathematical sense because the model Hotelling defined is a Gaussian model with covariance given by <span class="math inline"><strong>C</strong> = <strong>W</strong><strong>W</strong><sup>⊤</sup></span> so <span class="math inline"><strong>W</strong></span> is a <em>factor</em> of the covariance in the mathematical sense, as well as a factor loading.</p>
<p>However, the paper had one great advantage over standard approaches to factor analysis. Despite the fact that the model was a special case that is subsumed by the more general approach of factor analysis it is this special case that leads to a particular algorithm, namely that the factor loadings (or principal components as Hotelling referred to them) are given by an <em>eigenvalue decomposition</em> of the empirical covariance matrix.</p>
<p><br /><span class="math display"><strong>y</strong><sub><em>i</em>, :</sub> = <strong>W</strong><strong>z</strong><sub><em>i</em>, :</sub> + <strong>ϵ</strong><sub><em>i</em>, :</sub>,  <strong>z</strong><sub><em>i</em>, :</sub> ∼ 𝒩(<strong>0</strong>,<strong>I</strong>),  <strong>ϵ</strong><sub><em>i</em>, :</sub> ∼ 𝒩(<strong>0</strong>,<em>σ</em><sup>2</sup><strong>I</strong>)</span><br /></p>
<p><br /><span class="math display"><strong>W</strong><strong>z</strong><sub><em>i</em>, :</sub> ∼ 𝒩(<strong>0</strong>,<strong>W</strong><strong>W</strong><sup>⊤</sup>)</span><br /></p>
<p><br /><span class="math display"><strong>W</strong><strong>z</strong><sub><em>i</em>, :</sub> + <strong>ϵ</strong><sub><em>i</em>, :</sub> ∼ 𝒩(<strong>0</strong>,<strong>W</strong><strong>W</strong><sup>⊤</sup>+<em>σ</em><sup>2</sup><strong>I</strong>)</span><br /></p>
<p><strong>Probabilistic PCA Max. Likelihood Soln</strong> (<span class="citation" data-cites="Tipping:probpca99">Tipping and Bishop (1999b)</span>)</p>
%
<p><br /><span class="math display">$$p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i, :}|\mathbf{0},\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\right)$$</span><br /></p>
<h2 id="eigenvalue-decomposition">Eigenvalue Decomposition</h2>
<p>Eigenvalue problems are widespreads in physics and mathematics, they are often written as a matrix/vector equation but we prefer to write them as a full matrix equation. In an eigenvalue problem you are looking to find a matrix of eigenvectors, <span class="math inline"><strong>U</strong></span> and a <em>diagonal</em> matrix of eigenvalues, <span class="math inline"><strong>Λ</strong></span> that satisfy the <em>matrix</em> equation <br /><span class="math display"><strong>A</strong><strong>U</strong> = <strong>U</strong><strong>Λ</strong>.</span><br /> where <span class="math inline"><strong>A</strong></span> is your matrix of interest. This equation is not trivially solvable through matrix inverse because matrix multiplication is not <a href="http://en.wikipedia.org/wiki/Commutative_property">commutative</a>, so premultiplying by <span class="math inline"><strong>U</strong><sup> − 1</sup></span> gives <br /><span class="math display"><strong>U</strong><sup> − 1</sup><strong>A</strong><strong>U</strong> = <strong>Λ</strong>,</span><br /> where we remember that <span class="math inline"><strong>Λ</strong></span> is a <em>diagonal</em> matrix, so the eigenvectors can be used to <em>diagonalise</em> the matrix. When performing the eigendecomposition on a Gaussian covariances, diagonalisation is very important because it returns the covariance to a form where there is no correlation between points.</p>
<h2 id="positive-definite">Positive Definite</h2>
<p>We are interested in the case where <span class="math inline"><strong>A</strong></span> is a covariance matrix, which implies it is <em>positive definite</em>. A positive definite matrix is one for which the inner product, <br /><span class="math display"><strong>w</strong><sup>⊤</sup><strong>C</strong><strong>w</strong></span><br /> is positive for <em>all</em> values of the vector <span class="math inline"><strong>w</strong></span> other than the zero vector. One way of creating a positive definite matrix is to assume that the symmetric and positive definite matrix <span class="math inline"><strong>C</strong> ∈ ℜ<sup><em>p</em> × <em>p</em></sup></span> is factorised into, <span class="math inline"><strong>A</strong><em>i</em><em>n</em>ℜ<sup><em>p</em> × <em>p</em></sup></span>, a <em>full rank</em> matrix, so that <br /><span class="math display"><strong>C</strong> = <strong>A</strong><sup>⊤</sup><strong>A</strong>.</span><br /> This ensures that <span class="math inline"><strong>C</strong></span> must be positive definite because <br /><span class="math display"><strong>w</strong><sup>⊤</sup><strong>C</strong><strong>w</strong> = <strong>w</strong><sup>⊤</sup><strong>A</strong><sup>⊤</sup><strong>A</strong><strong>w</strong></span><br /> and if we now define a new <em>vector</em> <span class="math inline"><strong>b</strong></span> as <br /><span class="math display"><strong>b</strong> = <strong>A</strong><strong>w</strong></span><br /> we can now rewrite as <br /><span class="math display"><strong>w</strong><sup>⊤</sup><strong>C</strong><strong>w</strong> = <strong>b</strong><sup>⊤</sup><strong>b</strong> = ∑<sub><em>i</em></sub><em>b</em><sub><em>i</em></sub><sup>2</sup></span><br /> which, since it is a sum of squares, is positive or zero. The constraint that <span class="math inline"><strong>A</strong></span> must be <em>full rank</em> ensures that there is no vector <span class="math inline"><strong>w</strong></span>, other than the zero vector, which causes the vector <span class="math inline"><strong>b</strong></span> to be all zeros.</p>
<h3 id="exercise-0-1">Exercise 0</h3>
<p>If <span class="math inline"><strong>C</strong> = <strong>A</strong><sup>⊤</sup><strong>A</strong></span> then express <span class="math inline"><em>c</em><sub><em>i</em>, <em>j</em></sub></span>, the value of the element at the <span class="math inline"><em>i</em></span>th row and the <span class="math inline"><em>j</em></span>th column of <span class="math inline"><strong>C</strong></span>, in terms of the columns of <span class="math inline"><strong>A</strong></span>. Use this to show that (i) the matrix is symmetric and (ii) the matrix has positive elements along its diagonal.</p>
<h2 id="eigenvectors-of-a-symmetric-matric">Eigenvectors of a Symmetric Matric</h2>
<p>Symmetric matrices have <em>orthonormal</em> eigenvectors. This means that <span class="math inline"><strong>U</strong></span> is an <a href="http://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrix</a>, <span class="math inline"><strong>U</strong><sup>⊤</sup><strong>U</strong> = <strong>I</strong></span>. This implies that <span class="math inline"><strong>u</strong><sub>:,<em>i</em></sub><sup>⊤</sup><strong>u</strong><sub>:,<em>j</em></sub></span> is equal to 0 if <span class="math inline"><em>i</em> ≠ <em>j</em></span> and 1 if <span class="math inline"><em>i</em> = <em>j</em></span>.</p>
<h2 id="two-important-gaussian-properties">Two Important Gaussian Properties</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/univariate-gaussian-properties.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/univariate-gaussian-properties.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The Gaussian density has many important properties, but for the moment we’ll review two of them.</p>
<h2 id="sum-of-gaussians">Sum of Gaussians</h2>
<p>If we assume that a variable, <span class="math inline"><em>y</em><sub><em>i</em></sub></span>, is sampled from a Gaussian density,</p>
<p><br /><span class="math display"><em>y</em><sub><em>i</em></sub> ∼ 𝒩(<em>μ</em><sub><em>i</em></sub>,<em>σ</em><sub><em>i</em></sub><sup>2</sup>)</span><br /></p>
<p>Then we can show that the sum of a set of variables, each drawn independently from such a density is also distributed as Gaussian. The mean of the resulting density is the sum of the means, and the variance is the sum of the variances,</p>
<p><br /><span class="math display">$$
\sum_{i=1}^{n} y_i \sim \mathcal{N}\left(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2\right)
$$</span><br /></p>
<p>Since we are very familiar with the Gaussian density and its properties, it is not immediately apparent how unusual this is. Most random variables, when you add them together, change the family of density they are drawn from. For example, the Gaussian is exceptional in this regard. Indeed, other random variables, if they are independently drawn and summed together tend to a Gaussian density. That is the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem"><em>central limit theorem</em></a> which is a major justification for the use of a Gaussian density.</p>
<h2 id="scaling-a-gaussian">Scaling a Gaussian</h2>
<p>Less unusual is the <em>scaling</em> property of a Gaussian density. If a variable, <span class="math inline"><em>y</em></span>, is sampled from a Gaussian density,</p>
<p><br /><span class="math display"><em>y</em> ∼ 𝒩(<em>μ</em>,<em>σ</em><sup>2</sup>)</span><br /> and we choose to scale that variable by a <em>deterministic</em> value, <span class="math inline"><em>w</em></span>, then the <em>scaled variable</em> is distributed as</p>
<p><br /><span class="math display"><em>w</em><em>y</em> ∼ 𝒩(<em>w</em><em>μ</em>,<em>w</em><sup>2</sup><em>σ</em><sup>2</sup>).</span><br /> Unlike the summing properties, where adding two or more random variables independently sampled from a family of densitites typically brings the summed variable <em>outside</em> that family, scaling many densities leaves the distribution of that variable in the same <em>family</em> of densities. Indeed, many densities include a <em>scale</em> parameter (e.g. the <a href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma density</a>) which is purely for this purpose. In the Gaussian the standard deviation, <span class="math inline"><em>σ</em></span>, is the scale parameter. To see why this makes sense, let’s consider, <br /><span class="math display"><em>z</em> ∼ 𝒩(0,1),</span><br /> then if we scale by <span class="math inline"><em>σ</em></span> so we have, <span class="math inline"><em>y</em> = <em>σ</em><em>z</em></span>, we can write, <br /><span class="math display"><em>y</em> = <em>σ</em><em>z</em> ∼ 𝒩(0,<em>σ</em><sup>2</sup>)</span><br /></p>
<p>Let’s first of all review the properties of the multivariate Gaussian distribution that make linear Gaussian models easier to deal with. We’ll return to the, perhaps surprising, result on the parameters within the nonlinearity, <span class="math inline"><strong>θ</strong></span>, shortly.</p>
<p>To work with linear Gaussian models, to find the marginal likelihood all you need to know is the following rules. If <br /><span class="math display"><strong>y</strong> = <strong>W</strong><strong>x</strong> + <strong>ϵ</strong>,</span><br /> where <span class="math inline"><strong>y</strong></span>, <span class="math inline"><strong>x</strong></span> and <span class="math inline"><strong>ϵ</strong></span> are vectors and we assume that <span class="math inline"><strong>x</strong></span> and <span class="math inline"><strong>ϵ</strong></span> are drawn from multivariate Gaussians, <br /><span class="math display">$$
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{ \mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim \mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
$$</span><br /> then we know that <span class="math inline"><strong>y</strong></span> is also drawn from a multivariate Gaussian with, <br /><span class="math display"><strong>y</strong> ∼ 𝒩(<strong>W</strong><strong>μ</strong>,<strong>W</strong><strong>C</strong><strong>W</strong><sup>⊤</sup>+<strong>Σ</strong>).</span><br /></p>
<p>With appropriately defined covariance, <span class="math inline"><strong>Σ</strong></span>, this is actually the marginal likelihood for Factor Analysis, or Probabilistic Principal Component Analysis <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop 1999b)</span>, because we integrated out the inputs (or <em>latent</em> variables they would be called in that case).</p>
<h2 id="principal-component-analysis-1">Principal Component Analysis</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/principal-component-analysis.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/principal-component-analysis.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<ul>
<li><p>PCA (<span class="citation" data-cites="Hotelling:analysis33">Hotelling (1933)</span>) is a linear embedding.</p></li>
<li><p>Today its presented as:</p>
<ul>
<li>Rotate to find ‘directions’ in data with maximal variance.</li>
<li>How do we find these directions?</li>
</ul></li>
<li><p>Algorithmically we do this by diagonalizing the sample covariance matrix <br /><span class="math display">$$
\mathbf{S}=\frac{1}{n}\sum_{i=1}^n\left(\mathbf{ y}_{i, :}-\boldsymbol{ \mu}\right)\left(\mathbf{ y}_{i, :} - \boldsymbol{ \mu}\right)^\top
$$</span><br /></p></li>
<li><p>Find directions in the data, <span class="math inline"><strong>z</strong> = <strong>U</strong><strong>y</strong></span>, for which variance is maximized.</p></li>
<li><p>Solution is found via constrained optimisation (which uses <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>): <br /><span class="math display"><em>L</em>(<strong>u</strong><sub>1</sub>,<em>λ</em><sub>1</sub>) = <strong>u</strong><sub>1</sub><sup>⊤</sup><strong>S</strong><strong>u</strong><sub>1</sub> + <em>λ</em><sub>1</sub>(1−<strong>u</strong><sub>1</sub><sup>⊤</sup><strong>u</strong><sub>1</sub>)</span><br /></p></li>
<li><p>Gradient with respect to <span class="math inline"><strong>u</strong><sub>1</sub></span> <br /><span class="math display">$$\frac{\text{d}L\left(\mathbf{u}_{1},\lambda_{1}\right)}{\text{d}\mathbf{u}_{1}}=2\mathbf{S}\mathbf{u}_{1}-2\lambda_{1}\mathbf{u}_{1}$$</span><br /> rearrange to form <br /><span class="math display"><strong>S</strong><strong>u</strong><sub>1</sub> = <em>λ</em><sub>1</sub><strong>u</strong><sub>1</sub>.</span><br /> Which is known as an <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"><em>eigenvalue problem</em></a>.</p></li>
<li><p>Further directions that are <em>orthogonal</em> to the first can also be shown to be eigenvectors of the covariance.</p></li>
<li><p>Represent data, <span class="math inline"><strong>Y</strong></span>, with a lower dimensional set of latent variables <span class="math inline"><strong>Z</strong></span>.</p></li>
<li><p>Assume a linear relationship of the form <br /><span class="math display"><strong>y</strong><sub><em>i</em>, :</sub> = <strong>W</strong><strong>z</strong><sub><em>i</em>, :</sub> + <strong>ϵ</strong><sub><em>i</em>, :</sub>,</span><br /> where <br /><span class="math display"><strong>ϵ</strong><sub><em>i</em>, :</sub> ∼ 𝒩(<strong>0</strong>,<em>σ</em><sup>2</sup><strong>I</strong>)</span><br /></p></li>
</ul>
<p><strong>Probabilistic PCA</strong></p>
<ul>
<li>Define <em>linear-Gaussian relationship</em> between latent variables and data.</li>
<li><strong>Standard</strong> Latent variable approach:
<ul>
<li>Define Gaussian prior over <em>latent space</em>, <span class="math inline"><strong>Z</strong></span>.</li>
</ul></li>
<li>Integrate out <em>latent variables</em>.</li>
</ul>
<p><br /><span class="math display">$$
p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^2\mathbf{I}\right)
$$</span><br /></p>
<p><br /><span class="math display">$$
p\left(\mathbf{Z}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ z}_{i,:}|\mathbf{0},\mathbf{I}\right)
$$</span><br /></p>
<p><br /><span class="math display">$$
p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{0},\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\right)
$$</span><br /></p>
<h1 id="probabilistic-pca">Probabilistic PCA</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/probabilistic-pca.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/probabilistic-pca.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In 1997 <a href="http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf">Tipping and Bishop</a> <span class="citation" data-cites="Tipping:pca97">(Tipping and Bishop 1999a)</span> and <a href="https://www.cs.nyu.edu/~roweis/papers/empca.pdf">Roweis</a> <span class="citation" data-cites="Roweis:SPCA97">(Roweis, n.d.)</span> independently revisited Hotelling’s model and considered the case where the noise variance was finite, but <em>shared</em> across all output dimensons. Their model can be thought of as a factor analysis where <br /><span class="math display"><strong>Σ</strong> = <em>σ</em><sup>2</sup><strong>I</strong>.</span><br /> This leads to a marginal likelihood of the form <br /><span class="math display">$$
p(\mathbf{Y}|\mathbf{W}, \sigma^2)
= \prod_{i=1}^n\mathcal{N}\left(\mathbf{ y}_{i, :}|\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right)
$$</span><br /> where the limit of <span class="math inline"><em>σ</em><sup>2</sup> → 0</span> is <em>not</em> taken. This defines a proper probabilistic model. Tippping and Bishop then went on to prove that the <em>maximum likelihood</em> solution of this model with respect to <span class="math inline"><strong>W</strong></span> is given by an eigenvalue problem. In the probabilistic PCA case the eigenvalues and eigenvectors are given as follows. <br /><span class="math display"><strong>W</strong> = <strong>U</strong><strong>L</strong><strong>R</strong><sup>⊤</sup></span><br /> where <span class="math inline"><strong>U</strong></span> is the eigenvectors of the empirical covariance matrix <br /><span class="math display">$$
\mathbf{S} = \sum_{i=1}^n(\mathbf{ y}_{i, :} - \boldsymbol{ \mu})(\mathbf{ y}_{i,
:} - \boldsymbol{ \mu})^\top,
$$</span><br /> which can be written <span class="math inline">$\mathbf{S} = \frac{1}{n} \mathbf{Y}^\top\mathbf{Y}$</span> if the data is zero mean. The matrix <span class="math inline"><strong>L</strong></span> is diagonal and is dependent on the <em>eigenvalues</em> of <span class="math inline"><strong>S</strong></span>, <span class="math inline"><strong>Λ</strong></span>. If the <span class="math inline"><em>i</em></span>th diagonal element of this matrix is given by <span class="math inline"><em>λ</em><sub><em>i</em></sub></span> then the corresponding element of <span class="math inline"><strong>L</strong></span> is <br /><span class="math display">$$
\ell_i = \sqrt{\lambda_i - \sigma^2}
$$</span><br /> where <span class="math inline"><em>σ</em><sup>2</sup></span> is the noise variance. Note that if <span class="math inline"><em>σ</em><sup>2</sup></span> is larger than any particular eigenvalue, then that eigenvalue (along with its corresponding eigenvector) is <em>discarded</em> from the solution.</p>
<h2 id="python-implementation-of-probabilistic-pca">Python Implementation of Probabilistic PCA</h2>
<p>We will now implement this algorithm in python.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># probabilistic PCA algorithm</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="kw">def</span> ppca(Y, q):</span>
<span id="cb10-3"><a href="#cb10-3"></a>    <span class="co"># remove mean</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>    Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb10-5"><a href="#cb10-5"></a></span>
<span id="cb10-6"><a href="#cb10-6"></a>    <span class="co"># Comute covariance</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>    S <span class="op">=</span> np.dot(Y_cent.T, Y_cent)<span class="op">/</span>Y.shape[<span class="dv">0</span>]</span>
<span id="cb10-8"><a href="#cb10-8"></a>    lambd, U <span class="op">=</span> np.linalg.eig(S)</span>
<span id="cb10-9"><a href="#cb10-9"></a></span>
<span id="cb10-10"><a href="#cb10-10"></a>    <span class="co"># Choose number of eigenvectors</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>    sigma2 <span class="op">=</span> np.<span class="bu">sum</span>(lambd[q:])<span class="op">/</span>(Y.shape[<span class="dv">1</span>]<span class="op">-</span>q)</span>
<span id="cb10-12"><a href="#cb10-12"></a>    l <span class="op">=</span> np.sqrt(lambd[:q]<span class="op">-</span>sigma2)</span>
<span id="cb10-13"><a href="#cb10-13"></a>    W <span class="op">=</span> U[:, :q]<span class="op">*</span>l[<span class="va">None</span>, :]</span>
<span id="cb10-14"><a href="#cb10-14"></a>    <span class="cf">return</span> W, sigma2</span></code></pre></div>
<p>In practice we may not wish to compute the eigenvectors of the covariance matrix directly. This is because it requires us to estimate the covariance, which involves a sum of squares term, before estimating the eigenvectors. We can estimate the eigenvectors directly either through <a href="http://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> or <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a>. We saw a similar issue arise when , where we also wished to avoid computation of <span class="math inline"><strong>Z</strong><sup>⊤</sup><strong>Z</strong></span> (or in the case of  <span class="math inline"><strong>Φ</strong><sup>⊤</sup><strong>Φ</strong></span>).</p>
<h1 id="posterior-for-principal-component-analysis">Posterior for Principal Component Analysis</h1>
<p>Under the latent variable model justification for principal component analysis, we are normally interested in inferring something about the latent variables given the data. This is the distribution, <br /><span class="math display"><em>p</em>(<strong>z</strong><sub><em>i</em>, :</sub>|<strong>y</strong><sub><em>i</em>, :</sub>)</span><br /> for any given data point. Determining this density turns out to be very similar to the approach for determining the Bayesian posterior of <span class="math inline"><strong>w</strong></span> in Bayesian linear regression, only this time we place the prior density over <span class="math inline"><strong>z</strong><sub><em>i</em>, :</sub></span> instead of <span class="math inline"><strong>w</strong></span>. The posterior is proportional to the joint density as follows, <br /><span class="math display"><em>p</em>(<strong>z</strong><sub><em>i</em>, :</sub>|<strong>y</strong><sub><em>i</em>, :</sub>) ∝ <em>p</em>(<strong>y</strong><sub><em>i</em>, :</sub>|<strong>W</strong>, <strong>z</strong><sub><em>i</em>, :</sub>, <em>σ</em><sup>2</sup>)<em>p</em>(<strong>z</strong><sub><em>i</em>, :</sub>)</span><br /> And as in the Bayesian linear regression case we first consider the log posterior, <br /><span class="math display">log <em>p</em>(<strong>z</strong><sub><em>i</em>, :</sub>|<strong>y</strong><sub><em>i</em>, :</sub>) = log <em>p</em>(<strong>y</strong><sub><em>i</em>, :</sub>|<strong>W</strong>, <strong>z</strong><sub><em>i</em>, :</sub>, <em>σ</em><sup>2</sup>) + log <em>p</em>(<strong>z</strong><sub><em>i</em>, :</sub>) + const</span><br /> where the constant is not dependent on <span class="math inline"><strong>z</strong></span>. As before we collect the quadratic terms in <span class="math inline"><strong>z</strong><sub><em>i</em>, :</sub></span> and we assemble them into a Gaussian density over <span class="math inline"><strong>z</strong></span>. <br /><span class="math display">$$
\log p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) =
-\frac{1}{2\sigma^2} (\mathbf{ y}_{i, :} - \mathbf{W}\mathbf{ z}_{i,
:})^\top(\mathbf{ y}_{i, :} - \mathbf{W}\mathbf{ z}_{i, :}) - \frac{1}{2}
\mathbf{ z}_{i, :}^\top \mathbf{ z}_{i, :} + \text{const}
$$</span><br /></p>
<h3 id="exercise-0-2">Exercise 0</h3>
<p>Multiply out the terms in the brackets. Then collect the quadratic term and the linear terms together. Show that the posterior has the form <br /><span class="math display"><strong>z</strong><sub><em>i</em>, :</sub>|<strong>W</strong> ∼ 𝒩(<strong>μ</strong><sub><em>x</em></sub>,<strong>C</strong><sub><em>x</em></sub>)</span><br /> where <br /><span class="math display"><strong>C</strong><sub><em>x</em></sub> = (<em>σ</em><sup> − 2</sup><strong>W</strong><sup>⊤</sup><strong>W</strong>+<strong>I</strong>)<sup> − 1</sup></span><br /> and <br /><span class="math display"><strong>μ</strong><sub><em>x</em></sub> = <strong>C</strong><sub><em>x</em></sub><em>σ</em><sup> − 2</sup><strong>W</strong><sup>⊤</sup><strong>y</strong><sub><em>i</em>, :</sub></span><br /> Compare this to the posterior for the Bayesian linear regression from last week, do they have similar forms? What matches and what differs?</p>
<h2 id="python-implementation-of-the-posterior">Python Implementation of the Posterior</h2>
<p>Now let’s implement the system in code.</p>
<h3 id="exercise-1">Exercise 1</h3>
<p>Use the values for <span class="math inline"><strong>W</strong></span> and <span class="math inline"><em>σ</em><sup>2</sup></span> you have computed, along with the data set <span class="math inline"><strong>Y</strong></span> to compute the posterior density over <span class="math inline"><strong>Z</strong></span>. Write a function of the form</p>
<p>python mu_x, C_x = posterior(Y, W, sigma2)} where <code>mu_x</code> and <code>C_x</code> are the posterior mean and posterior covariance for the given <span class="math inline"><strong>Y</strong></span>.</p>
<p>Don’t forget to subtract the mean of the data <code>Y</code> inside your function before computing the posterior: remember we assumed at the beginning of our analysis that the data had been centred (i.e. the mean was removed).}{20}</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Question 4 Answer Code</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="co"># Write code for you answer to this question in this box</span></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="co"># Do not delete these comments, otherwise you will get zero for this answer.</span></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># Make sure your code has run and the answer is correct *before* submitting your notebook for marking.</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="kw">def</span> posterior(Y, W, sigma2):</span>
<span id="cb11-8"><a href="#cb11-8"></a>    Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb11-9"><a href="#cb11-9"></a>    <span class="co"># Compute posterior over X</span></span>
<span id="cb11-10"><a href="#cb11-10"></a>    C_x <span class="op">=</span> </span>
<span id="cb11-11"><a href="#cb11-11"></a>    mu_x <span class="op">=</span> </span>
<span id="cb11-12"><a href="#cb11-12"></a>    <span class="cf">return</span> mu_x, C_x</span></code></pre></div>
<h2 id="numerically-stable-and-efficient-version">Numerically Stable and Efficient Version</h2>
<p>Just as we saw for  and  computation of a matrix such as <span class="math inline"><strong>Y</strong><sup>⊤</sup><strong>Y</strong></span> (or its centred version) can be a bad idea in terms of loss of numerical accuracy. Fortunately, we can find the eigenvalues and eigenvectors of the matrix <span class="math inline"><strong>Y</strong><sup>⊤</sup><strong>Y</strong></span> without direct computation of the matrix. This can be done with the <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition"><em>singular value decomposition</em></a>. The singular value decompsition takes a matrix, <span class="math inline"><strong>Z</strong></span> and represents it in the form, <br /><span class="math display"><strong>Z</strong> = <strong>U</strong><strong>Λ</strong><strong>V</strong><sup>⊤</sup></span><br /> where <span class="math inline"><strong>U</strong></span> is a matrix of orthogonal vectors in the columns, meaning <span class="math inline"><strong>U</strong><sup>⊤</sup><strong>U</strong> = <strong>I</strong></span>. It has the same number of rows and columns as <span class="math inline"><strong>Z</strong></span>. The matrices <span class="math inline"><strong>Λ</strong></span> and <span class="math inline"><strong>V</strong></span> are both square with dimensionality given by the number of columns of <span class="math inline"><strong>Z</strong></span>. The matrix <span class="math inline"><strong>Λ</strong></span> is <em>diagonal</em> and <span class="math inline"><strong>V</strong></span> is an orthogonal matrix so <span class="math inline"><strong>V</strong><sup>⊤</sup><strong>V</strong> = <strong>V</strong><strong>V</strong><sup>⊤</sup> = <strong>I</strong></span>. The eigenvalues of the matrix <span class="math inline"><strong>Y</strong><sup>⊤</sup><strong>Y</strong></span> are then given by the singular values of the matrix <span class="math inline"><strong>Y</strong><sup>⊤</sup></span> squared and the eigenvectors are given by <span class="math inline"><strong>U</strong></span>.</p>
<h2 id="solution-for-mathbfw">Solution for <span class="math inline"><strong>W</strong></span></h2>
<p>Given the singular value decomposition of <span class="math inline"><strong>Y</strong></span> then we have <br /><span class="math display"><strong>W</strong> = <strong>U</strong><strong>L</strong><strong>R</strong><sup>⊤</sup></span><br /> where <span class="math inline"><strong>R</strong></span> is an arbitrary rotation matrix. This implies that the posterior is given by <br /><span class="math display"><strong>C</strong><sub><em>x</em></sub> = [<em>σ</em><sup> − 2</sup><strong>R</strong><strong>L</strong><sup>2</sup><strong>R</strong><sup>⊤</sup>+<strong>I</strong>]<sup> − 1</sup></span><br /> because <span class="math inline"><strong>U</strong><sup>⊤</sup><strong>U</strong> = <strong>I</strong></span>. Since, by convention, we normally take <span class="math inline"><strong>R</strong> = <strong>I</strong></span> to ensure that the principal components are orthonormal we can write <br /><span class="math display"><strong>C</strong><sub><em>x</em></sub> = [<em>σ</em><sup> − 2</sup><strong>L</strong><sup>2</sup>+<strong>I</strong>]<sup> − 1</sup></span><br /> which implies that <span class="math inline"><strong>C</strong><sub><em>x</em></sub></span> is actually diagonal with elements given by <br /><span class="math display">$$
c_i = \frac{\sigma^2}{\sigma^2 + \ell^2_i}
$$</span><br /> and allows us to write <br /><span class="math display"><strong>μ</strong><sub><em>x</em></sub> = [<strong>L</strong><sup>2</sup> + <em>σ</em><sup>2</sup><strong>I</strong>]<sup> − 1</sup><strong>L</strong><strong>U</strong><sup>⊤</sup><strong>y</strong><sub><em>i</em>, :</sub></span><br /> <br /><span class="math display"><strong>μ</strong><sub><em>x</em></sub> = <strong>D</strong><strong>U</strong><sup>⊤</sup><strong>y</strong><sub><em>i</em>, :</sub></span><br /> where <span class="math inline"><strong>D</strong></span> is a diagonal matrix with diagonal elements given by <span class="math inline">$d_{i} = \frac{\ell_i}{\sigma^2 + \ell_i^2}$</span>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># probabilistic PCA algorithm using SVD</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="kw">def</span> ppca(Y, q, center<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb13-3"><a href="#cb13-3"></a>    <span class="co">&quot;&quot;&quot;Probabilistic PCA through singular value decomposition&quot;&quot;&quot;</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>    <span class="co"># remove mean</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>    <span class="cf">if</span> center:</span>
<span id="cb13-6"><a href="#cb13-6"></a>        Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb13-7"><a href="#cb13-7"></a>    <span class="cf">else</span>:</span>
<span id="cb13-8"><a href="#cb13-8"></a>        Y_cent <span class="op">=</span> Y</span>
<span id="cb13-9"><a href="#cb13-9"></a>        </span>
<span id="cb13-10"><a href="#cb13-10"></a>    <span class="co"># Comute singluar values, discard &#39;R&#39; as we will assume orthogonal</span></span>
<span id="cb13-11"><a href="#cb13-11"></a>    U, sqlambd, _ <span class="op">=</span> sp.linalg.svd(Y_cent.T,full_matrices<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-12"><a href="#cb13-12"></a>    lambd <span class="op">=</span> (sqlambd<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>Y.shape[<span class="dv">0</span>]</span>
<span id="cb13-13"><a href="#cb13-13"></a>    <span class="co"># Compute residual and extract eigenvectors</span></span>
<span id="cb13-14"><a href="#cb13-14"></a>    sigma2 <span class="op">=</span> np.<span class="bu">sum</span>(lambd[q:])<span class="op">/</span>(Y.shape[<span class="dv">1</span>]<span class="op">-</span>q)</span>
<span id="cb13-15"><a href="#cb13-15"></a>    ell <span class="op">=</span> np.sqrt(lambd[:q]<span class="op">-</span>sigma2)</span>
<span id="cb13-16"><a href="#cb13-16"></a>    <span class="cf">return</span> U[:, :q], ell, sigma2</span>
<span id="cb13-17"><a href="#cb13-17"></a></span>
<span id="cb13-18"><a href="#cb13-18"></a><span class="kw">def</span> posterior(Y, U, ell, sigma2, center<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb13-19"><a href="#cb13-19"></a>    <span class="co">&quot;&quot;&quot;Posterior computation for the latent variables given the eigendecomposition.&quot;&quot;&quot;</span></span>
<span id="cb13-20"><a href="#cb13-20"></a>    <span class="cf">if</span> center:</span>
<span id="cb13-21"><a href="#cb13-21"></a>        Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb13-22"><a href="#cb13-22"></a>    <span class="cf">else</span>:</span>
<span id="cb13-23"><a href="#cb13-23"></a>        Y_cent <span class="op">=</span> Y</span>
<span id="cb13-24"><a href="#cb13-24"></a>    C_x <span class="op">=</span> np.diag(sigma2<span class="op">/</span>(sigma2<span class="op">+</span>ell<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb13-25"><a href="#cb13-25"></a>    d <span class="op">=</span> ell<span class="op">/</span>(sigma2<span class="op">+</span>ell<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-26"><a href="#cb13-26"></a>    mu_x <span class="op">=</span> np.dot(Y_cent, U)<span class="op">*</span>d[<span class="va">None</span>, :]</span>
<span id="cb13-27"><a href="#cb13-27"></a>    <span class="cf">return</span> mu_x, C_x</span></code></pre></div>
<h2 id="robot-navigation-example">Robot Navigation Example</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/robot-wireless-ppca.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/robot-wireless-ppca.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>In the next example we will load in data from a robot navigation problem. The data consists of wireless access point strengths as recorded by a robot performing a loop around the University of Washington’s Computer Science department in Seattle. The robot records all the wireless access points it can cache and stores their signal strength.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>data <span class="op">=</span> pods.datasets.robot_wireless()</span>
<span id="cb15-2"><a href="#cb15-2"></a>Y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb15-3"><a href="#cb15-3"></a>Y.shape</span></code></pre></div>
<p>There are 215 observations of 30 different access points. In this case the model is suggesting that the access point signal strength should be linearly dependent on the location in the map. In other words we are expecting the access point strength for the <span class="math inline"><em>j</em></span>th access point at robot position <span class="math inline"><em>x</em><sub><em>i</em>, :</sub></span> to be represented by <span class="math inline"><em>y</em><sub><em>i</em>, <em>j</em></sub> = <strong>w</strong><sub><em>j</em>, :</sub><sup>⊤</sup><strong>z</strong><sub><em>i</em>, :</sub> + <em>ϵ</em><sub><em>i</em>, <em>j</em></sub></span>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>q <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>U, ell, sigma2 <span class="op">=</span> ppca(Y, q)</span>
<span id="cb17-3"><a href="#cb17-3"></a>mu_x, C_x <span class="op">=</span> posterior(Y, U, ell, sigma2)</span>
<span id="cb17-4"><a href="#cb17-4"></a></span>
<span id="cb17-5"><a href="#cb17-5"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb17-6"><a href="#cb17-6"></a>ax.plot(mu_x[:, <span class="dv">0</span>], mu_x[:, <span class="dv">1</span>], <span class="st">&#39;rx-&#39;</span>)</span>
<span id="cb17-7"><a href="#cb17-7"></a>ax.set_title(<span class="st">&#39;Latent Variable: Robot Inferred Locations&#39;</span>)</span>
<span id="cb17-8"><a href="#cb17-8"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb17-9"><a href="#cb17-9"></a>W <span class="op">=</span> U<span class="op">*</span>ell[<span class="va">None</span>, :]</span>
<span id="cb17-10"><a href="#cb17-10"></a>ax.plot(W[:, <span class="dv">0</span>], W[:, <span class="dv">1</span>], <span class="st">&#39;bo&#39;</span>)</span>
<span id="cb17-11"><a href="#cb17-11"></a>ax.set_title(<span class="st">&#39;Access Point Inferred Locations&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>U, ell, sigma2 <span class="op">=</span> ppca(Y.T, q)</span></code></pre></div>
<h1 id="interpretations-of-principal-component-analysis">Interpretations of Principal Component Analysis</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/ppca-interpretations.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/ppca-interpretations.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<h2 id="relationship-to-matrix-factorization">Relationship to Matrix Factorization</h2>
<p>We can use the robot naviation example to realise that PCA (and factor analysis) are very reminiscient of the  that we used for introducing objective functions. In that system we used slightly different notation, <span class="math inline"><strong>u</strong><sub><em>i</em>, :</sub></span> for <em>user</em> location in our metaphorical library and <span class="math inline"><strong>v</strong><sub><em>j</em>, :</sub></span> for <em>item</em> location in our metaphorical library. To see how these systems are somewhat analagous, now let us think about the user as the robot and the items as the wifi access points. We can plot the relative location of both. This process is known as “SLAM”: simultaneous <em>localisation</em> and <em>mapping</em>. A latent variable model of the type we have developed is one way of performing SLAM. We have an estimate of the <em>landmarks</em> in the system (in this case WIFI access points) and we have an estimate of the robot position. These are analagous to the estimate of the user’s position and the estimate of the items positions in the library. In the matrix factorisation example users are informing us what items they are ‘close’ to by expressing their preferences, in the robot localization example the robot is informing us what access point it is close to by measuring signal strength.</p>
<p>From a personal perspective, I find this analogy quite comforting. I think it is very arguable that one of the mechanisms through which we (as humans) may have developed higher reasoning is through the need to navigate around our environment, identifying landmarks and associating them with our search for food. If such a system were to exist, the idea that it could be readily adapted to other domains such as categorising the nature of the different foodstuffs we were able to forage is intriguing.</p>
<p>From an algorithmic perspective, we also can now realise that matrix factorization and latent variable modelling are effectively the same thing. The only difference is the objective function and our probabilistic (or lack of probabilistic) treatment of the variables. But the prediction function for both systems, <br /><span class="math display"><em>f</em><sub><em>i</em>, <em>j</em></sub> = <strong>u</strong><sub><em>i</em>, :</sub><sup>⊤</sup><strong>v</strong><sub><em>j</em>, :</sub></span><br /> for matrix factorization or <br /><span class="math display"><em>f</em><sub><em>i</em>, <em>j</em></sub> = <strong>z</strong><sub><em>i</em>, :</sub><sup>⊤</sup><strong>w</strong><sub><em>j</em>, :</sub></span><br /> for probabilistic PCA and factor analysis are the same.</p>
<h2 id="other-interpretations-of-pca-separating-model-and-algorithm">Other Interpretations of PCA: Separating Model and Algorithm</h2>
<p>Since Hotelling first introduced his perspective on factor analysis as PCA there has been somewhat of a conflation of the idea of the model underlying PCA (for which it was very clear that Hotelling was inspired by Factor Analysis) and the algorithm that is used to fit that model: the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors of an ellipsoid have been known since the middle of the 19th century as the principal axes of the elipsoid, and they arise through the following additional ideas: seeking the orthogonal directions of <em>maximum variance</em> in a dataset. Pearson in 1901 arrived at the same algorithm driven by a desire to seek a <em>symmetric regression</em> between two covariate/response variables <span class="math inline"><em>x</em></span> and <span class="math inline"><em>y</em></span> <span class="citation" data-cites="Pearson:01">(Pearson 1901)</span>. He is, therefore, often credited with the invention of principal component analysis, but to me this seems disengenous. His aim was very different from Hotellings, it was just happened that the optimal solution for his model was coincident with that of Hotelling. The approach is also known as the <a href="http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem">Karhunen Loeve Transform</a> in stochastic process theory and in classical multidimensional scaling the same operation can be shown to be minimising a particular objective function based on interpoint distances in the data and the latent space (see the section on Classical Multidimensional Scaling in <a href="http://store.elsevier.com/Multivariate-Analysis/Kanti-%20Mardia/isbn-9780124712522/">Mardia, Kent and Bibby</a>) <span class="citation" data-cites="Mardia:multivariate79">(Mardia, Kent, and Bibby 1979)</span>. One of my own contributions to machine learning was deriving yet another model whose linear variant was solved by finding the principal subspace of the covariance matrix (an approach I termed dual probabilistic PCA or probabilistic principal coordinate analysis). Finally, the approach is sometimes referred to simply as singular value decomposition (SVD). The singular value decomposition of a data set has the following form, <br /><span class="math display"><strong>Y</strong> = <strong>V</strong><strong>Λ</strong><strong>U</strong><sup>⊤</sup></span><br /> where <span class="math inline"><strong>V</strong> ∈ ℜ<sup><em>n</em> × <em>n</em></sup></span> and <span class="math inline"><strong>U</strong><sup>∈</sup>ℜ<sup><em>p</em> × <em>p</em></sup></span> are square orthogonal matrices and <span class="math inline"><strong>Λ</strong><sup><em>n</em> × <em>p</em></sup></span> is zero apart from its first <span class="math inline"><em>p</em></span> diagonal entries. Singularvalue decomposition gives a diagonalisation of the covariance matrix, because under the SVD we have <br /><span class="math display"><strong>Y</strong><sup>⊤</sup><strong>Y</strong> = <strong>U</strong><strong>Λ</strong><strong>V</strong><sup>⊤</sup><strong>V</strong><strong>Λ</strong><strong>U</strong><sup>⊤</sup> = <strong>U</strong><strong>Λ</strong><sup>2</sup><strong>U</strong><sup>⊤</sup></span><br /> where <span class="math inline"><strong>Λ</strong><sup>2</sup></span> is now the eigenvalues of the covariane matrix and <span class="math inline"><strong>U</strong></span> are the eigenvectors. So performing the SVD can simply be seen as another approach to determining the principal components.</p>
<h2 id="separating-model-and-algorithm">Separating Model and Algorithm</h2>
<p>I’ve given a fair amount of personal thought to this situation and my own opinion that this confusion about method arises because of a conflation of model and algorithm. The model of Hotelling, that which he termed principal component analysis, was really a variant of factor analysis, and it was unfortunate that he chose to rename it. However, the algorithm he derived was a very convenient way of optimising a (simplified) factor analysis, and it’s therefore become very popular. The algorithm is also the optimal solution for many other models of the data, even some which might seem initally to be unrelated (e.g. seeking directions of maximum variance). It is only through the mathematics of this linear system (which also contains some intersting symmetries) that all these ides become related. However, as soon as we choose to non-linearise the system (e.g. through basis functions) we find that each of the non-linear intepretations we can derive for the different models each leads to a very different algorithm (if such an algorithm is possible). For example <a href="http://web.stanford.edu/~hastie/Papers/Principal_Curves.pdf">principal curves</a> of <span class="citation" data-cites="Hastie:pcurves89">Hastie and Stuetzle (1989)</span> attempt to non-linearise the maximum variance interpretation, <a href="http://en.wikipedia.org/wiki/Kernel_principal_component_analysis">kernel PCA</a> of <span class="citation" data-cites="Scholkopf:nonlinear98">Schölkopf, Smola, and Müller (1998)</span> uses basis functions to form the eigenvalue problem in a nonlinear space, and my own work in this area <a href="http://jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf">non-linearises the dual probabilistic PCA</a> <span class="citation" data-cites="Lawrence:pnpca05">(Lawrence 2005)</span>.</p>
<p>My conclusion is that when you are doing machine learning you should always have it clear in your mind what your <em>model</em> is and what your <em>algorithm</em> is. You can recognise your model because it normally contains a prediction function and an objective function. The algorithm on the other hand is the sequence of steps you implement on the computer to solve for the parameters of this model. For efficient implementation, we often modify our model to allow for faster algorithms, and this is a perfectly valid pragmatist’s approach, so conflation of model and algorithm is not always a bad thing. But for clarity of thinking and understanding it is necessary to maintain the separation and to maintain a handle on when and why we perform the conflation.</p>
<h1 id="pca-in-practice">PCA in Practice</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/pca-in-practice.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/pca-in-practice.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Principal component analysis is so effective in practice that there has almost developed a mini-industry in renaming the method itself (which is ironic, given its origin). In particular <a href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">Latent Semantic Indexing</a> in text processing is simply PCA on a particular representation of the term frequencies of the document. There is a particular fad to rename the eigenvectors after the nature of the data you are examining, perhaps initially triggered by <a href="http://www.face-rec.org/algorithms/PCA/jcn.pdf">Turk and Pentland’s</a> paper on eigenfaces, but also with <a href="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1_2007_8/kuhn-%20junqua-eigenvoice-icslp1998.pdf">eigenvoices</a> and <a href="http://www.biomedcentral.com/1752-0509/1/54">eigengenes</a>. This seems to be an instantiation of a wider, and hopefully subconcious, tendency in academia to attempt to differentiate one idea from the same idea in related fields in order to emphasise the novelty. The unfortunate result is somewhat of a confusing literature for relatively simple model. My recommendations would be as follows. If you have multivariate data, applying some form of principal component would seem to be a very good idea as a first step. Even if you intend to later perform classification or regression on your data, it can give you understanding of the structure of the underlying data and help you to develop your intuitions about the nature of your data. Intelligent plotting and interaction with your data is always a good think, and for high dimensional data that means that you need some way of visualisation, PCA is typically a good starting point.</p>
<h1 id="ppca-marginal-likelihood">PPCA Marginal Likelihood</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/ppca-marginal-likelihood.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/ppca-marginal-likelihood.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>We have developed the posterior density over the latent variables given the data and the parameters, and due to symmetries in the underlying prediction function, it has a very similar form to its sister density, the posterior of the weights given the data from Bayesian regression. Two key differences are as follows. If we were to do a Bayesian multiple output regression we would find that the marginal likelihood of the data is independent across the features and correlated across the data, <br /><span class="math display">$$
p(\mathbf{Y}|\mathbf{Z})
= \prod_{j=1}^p \mathcal{N}\left(\mathbf{ y}_{:, j}|\mathbf{0},
\alpha\mathbf{Z}\mathbf{Z}^\top + \sigma^2 \mathbf{I}\right)
$$</span><br /> where <span class="math inline"><strong>y</strong><sub>:,<em>j</em></sub></span> is a column of the data matrix and the independence is across the <em>features</em>, in probabilistic PCA the marginal likelihood has the form, <br /><span class="math display">$$
p(\mathbf{Y}|\mathbf{W}) = \prod_{i=1}^n\mathcal{N}\left(\mathbf{ y}_{i,
:}|\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right)
$$</span><br /> where <span class="math inline"><strong>y</strong><sub><em>i</em>, :</sub></span> is a row of the data matrix <span class="math inline"><strong>Y</strong></span> and the independence is across the data points.</p>
<h1 id="computation-of-the-log-likelihood">Computation of the Log Likelihood</h1>
<p>The quality of the model can be assessed using the log likelihood of this Gaussian form. <br /><span class="math display">$$
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2} \log \left|
\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right| -\frac{1}{2}
\sum_{i=1}^n\mathbf{ y}_{i, :}^\top \left(\mathbf{W}\mathbf{W}^\top + \sigma^2
\mathbf{I}\right)^{-1} \mathbf{ y}_{i, :} +\text{const}
$$</span><br /> but this can be computed more rapidly by exploiting the low rank form of the covariance covariance, <span class="math inline"><strong>C</strong> = <strong>W</strong><strong>W</strong><sup>⊤</sup> + <em>σ</em><sup>2</sup><strong>I</strong></span> and the fact that <span class="math inline"><strong>W</strong> = <strong>U</strong><strong>L</strong><strong>R</strong><sup>⊤</sup></span>. Specifically, we first use the decomposition of <span class="math inline"><strong>W</strong></span> to write: <br /><span class="math display">$$
-\frac{n}{2} \log \left| \mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right|
= -\frac{n}{2} \sum_{i=1}^q \log (\ell_i^2 + \sigma^2) - \frac{n(p-q)}{2}\log
\sigma^2,
$$</span><br /> where <span class="math inline">ℓ<sub><em>i</em></sub></span> is the <span class="math inline"><em>i</em></span>th diagonal element of <span class="math inline"><strong>L</strong></span>. Next, we use the <a href="http://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> which allows us to write the inverse as a quantity which contains another inverse in a smaller matrix: <br /><span class="math display">$$
(\sigma^2 \mathbf{I}+ \mathbf{W}\mathbf{W}^\top)^{-1} =
\sigma^{-2}\mathbf{I}-\sigma^{-4}\mathbf{W}{\underbrace{(\mathbf{I}+\sigma^{-2}\mathbf{W}^\top\mathbf{W})}_{\mathbf{C}_x}}^{-1}\mathbf{W}^\top
$$</span><br /> So, it turns out that the original inversion of the <span class="math inline"><em>p</em> × <em>p</em></span> matrix can be done by forming a quantity which contains the inversion of a <span class="math inline"><em>q</em> × <em>q</em></span> matrix which, moreover, turns out to be the quantity <span class="math inline"><strong>C</strong><sub><em>x</em></sub></span> of the posterior.</p>
<p>Now, we put everything together to obtain: <br /><span class="math display">$$
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2} \sum_{i=1}^q
\log (\ell_i^2 + \sigma^2)
- \frac{n(p-q)}{2}\log \sigma^2 - \frac{1}{2} \text{tr}\left(\mathbf{Y}^\top \left(
\sigma^{-2}\mathbf{I}-\sigma^{-4}\mathbf{W}\mathbf{C}_x
\mathbf{W}^\top \right) \mathbf{Y}\right) + \text{const},
$$</span><br /> where we used the fact that a scalar sum can be written as <span class="math inline">$\sum_{i=1}^n\mathbf{ y}_{i,:}^\top \mathbf{K}\mathbf{ y}_{i,:} = \text{tr}\left(\mathbf{Y}^\top \mathbf{K}\mathbf{Y}\right)$</span>, for any matrix <span class="math inline"><strong>K</strong></span> of appropriate dimensions. We now use the properties of the trace <span class="math inline">tr(<strong>A</strong>+<strong>B</strong>) = tr(<strong>A</strong>) + tr(<strong>B</strong>)</span> and <span class="math inline">tr(<em>c</em><strong>A</strong>) = <em>c</em>tr(<strong>A</strong>)</span>, where <span class="math inline"><em>c</em></span> is a scalar and <span class="math inline"><strong>A</strong>, <strong>B</strong></span> matrices of compatible sizes. Therefore, the final log likelihood takes the form: <br /><span class="math display">$$
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2}
\sum_{i=1}^q \log (\ell_i^2 + \sigma^2) - \frac{n(p-q)}{2}\log \sigma^2 -
\frac{\sigma^{-2}}{2} \text{tr}\left(\mathbf{Y}^\top \mathbf{Y}\right)
+\frac{\sigma^{-4}}{2} \text{tr}\left(\mathbf{B}\mathbf{C}_x\mathbf{B}^\top\right) +
\text{const}
$$</span><br /> where we also defined <span class="math inline"><strong>B</strong> = <strong>Y</strong><sup>⊤</sup><strong>W</strong></span>. Finally, notice that <span class="math inline">tr(<strong>Y</strong><strong>Y</strong><sup>⊤</sup>) = tr(<strong>Y</strong><sup>⊤</sup><strong>Y</strong>)</span> can be computed faster as the sum of all the elements of <span class="math inline"><strong>Y</strong> ∘ <strong>Y</strong></span>, where <span class="math inline">∘</span> denotes the element-wise (or <a href="http://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard</a> product.</p>
<h2 id="difficulty-for-probabilistic-approaches">Difficulty for Probabilistic Approaches</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/non-linear-difficulty.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/non-linear-difficulty.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The challenge for composition of probabilistic models is that you need to propagate a probability densities through non linear mappings. This allows you to create broader classes of probability density. Unfortunately it renders the resulting densities <em>intractable</em>.</p>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg" width="80%" style=" ">
</object>
</div>
<div id="nonlinear-mapping-3d-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;nonlinear-mapping-3d-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nonlinear-mapping-3d-plot-caption" class="caption-frame">
<p>Figure: A two dimensional grid mapped into three dimensions to form a two dimensional manifold.</p>
</div>
</div>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg" width="80%" style=" ">
</object>
</div>
<div id="non-linear-mapping-2d-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;non-linear-mapping-2d-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="non-linear-mapping-2d-plot-caption" class="caption-frame">
<p>Figure: A one dimensional line mapped into two dimensions by two separate independent functions. Each point can be mapped exactly through the mappings.</p>
</div>
</div>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
<div id="gaussian-through-nonlinear-magnify" class="magnify" onclick="magnifyFigure(&#39;gaussian-through-nonlinear&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gaussian-through-nonlinear-caption" class="caption-frame">
<p>Figure: A Gaussian density over the input of a non linear function leads to a very non Gaussian output. Here the output is multimodal.</p>
</div>
</div>
<h1 id="dual-probabilistic-pca-and-gp-lvm">Dual Probabilistic PCA and GP-LVM</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/gplvm.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/gplvm.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<h2 id="dual-probabilistic-pca">Dual Probabilistic PCA</h2>
<p><strong>Probabilistic PCA</strong></p>
<ul>
<li>We have seen that PCA has a probabilistic interpretation <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop 1999b)</span>.</li>
<li>It is difficult to `non-linearise’ directly.</li>
<li>GTM and Density Networks are an attempt to do so.</li>
</ul>
<p><strong>Dual Probabilistic PCA</strong></p>
<ul>
<li>There is an alternative probabilistic interpretation of PCA <span class="citation" data-cites="Lawrence:pnpca05">(Lawrence 2005)</span>.</li>
<li>This interpretation can be made non-linear.</li>
<li>The result is non-linear probabilistic PCA.</li>
</ul>
<table>
<tr>
<td width="45%">
<p><strong>Dual Probabilistic PCA</strong></p>
<ul>
<li>Define <em>linear-Gaussian relationship</em> between latent variables and data.</li>
<li><strong>Novel</strong> Latent variable approach:</li>
<li>Define Gaussian prior over , <span class="math inline"><strong>W</strong></span>.</li>
<li>Integrate out <em>parameters</em>.
</td>
<td width="45%">
 {}</li>
</ul>
</td>
</tr>
</table>
<p><strong>DualDual</strong> Probabilistic PCA Max. Likelihood Soln <span class="citation" data-cites="Lawrence:gplvm03">(Lawrence, n.d., Lawrence:pnpca05)</span><span class="citation" data-cites="Lawrence:gplvm03">(Lawrence, n.d., Lawrence:pnpca05)</span><span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop 1999b)</span></p>
<pre><code> \includegraphics&lt;1&gt;[width=0.25\textwidth]{../../../gplvm/tex/diagrams/gplvmGraph}</code></pre>
<p><br /><span class="math display">$$
p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{p}\mathcal{N}\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\right)
$$</span><br /></p>
<p><br /><span class="math display">$$
p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{p}\mathcal{N}\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{K}\right),\quad\quad\mathbf{K}=\mathbf{Z}\mathbf{\mathbf{Z}}^{\top}+\sigma^{2}\mathbf{I}
$$</span><br /> <br /><span class="math display">$$
\log p\left(\mathbf{Y}|\mathbf{Z}\right)=-\frac{p}{2}\log\left|\mathbf{K}\right|-\frac{1}{2}\text{tr}\left(\mathbf{K}^{-1}\mathbf{Y}\mathbf{Y}^{\top}\right)+\mbox{const.}
$$</span><br /> If <span class="math inline"><strong>U</strong><sub><em>q</em></sub><sup>′</sup></span> are first <span class="math inline"><em>q</em></span> principal eigenvectors of <span class="math inline"><em>p</em><sup> − 1</sup><strong>Y</strong><strong>Y</strong><sup>⊤</sup></span> and the corresponding eigenvalues are <span class="math inline"><em>Λ</em><sub><em>q</em></sub></span>, <br /><span class="math display">$$
\mathbf{Z}=\mathbf{U^{\prime}}_{q}\mathbf{L}\mathbf{R}^{\top},\quad\quad\mathbf{L}=\left(\Lambda_{q}-\sigma^{2}\mathbf{I}\right)^{\frac{1}{2}}
$$</span><br /> where <span class="math inline"><strong>R</strong></span> is an arbitrary rotation matrix. </p>
<p><strong>The Eigenvalue Problems are equivalent</strong></p>
<ul>
<li><p>Solution for Probabilistic PCA (solves for the mapping) <br /><span class="math display"><strong>Y</strong><sup>⊤</sup><strong>Y</strong><strong>U</strong><sub><em>q</em></sub> = <strong>U</strong><sub><em>q</em></sub><em>Λ</em><sub><em>q</em></sub>      <strong>W</strong> = <strong>U</strong><sub><em>q</em></sub><strong>L</strong><strong>V</strong><sup>⊤</sup></span><br /></p></li>
<li><p>Solution for Dual Probabilistic PCA (solves for the latent positions) <br /><span class="math display"><strong>Y</strong><strong>Y</strong><sup>⊤</sup><strong>U</strong><sub><em>q</em></sub><sup>′</sup> = <strong>U</strong><sub><em>q</em></sub><sup>′</sup><em>Λ</em><sub><em>q</em></sub>      <strong>Z</strong> = <strong>U</strong><sub><em>q</em></sub><sup>′</sup><strong>L</strong><strong>V</strong><sup>⊤</sup></span><br /></p></li>
<li><p>Equivalence is from <br /><span class="math display">$$
\mathbf{U}_{q}=\mathbf{Y}^{\top}\mathbf{U}_{q}^{\prime}\Lambda_{q}^{-\frac{1}{2}}
  $$</span><br /></p></li>
</ul>
<h2 id="gaussian-processes">Gaussian Processes</h2>
<p><strong>Prior for Functions</strong></p>
<ul>
<li><p>Probability Distribution over Functions</p></li>
<li><p>Functions are infinite dimensional.</p>
<ul>
<li>Prior distribution over <em>instantiations</em> of the function: finite dimensional objects.</li>
<li>Can prove by induction that GP is ‘consistent’.</li>
</ul></li>
<li><p>Mean and Covariance Functions</p></li>
<li><p>Instead of mean and covariance matrix, GP is defined by mean function and covariance function.</p>
<ul>
<li>Mean function often taken to be zero or constant.</li>
<li>Covariance function must be <em>positive definite</em>.</li>
<li>Class of valid covariance functions is the same as the class of <em>Mercer kernels</em>.</li>
</ul></li>
</ul>
<p><strong>Zero mean Gaussian Process</strong></p>
<ul>
<li><p>A (zero mean) Gaussian process likelihood is of the form<br /><span class="math display"><em>p</em>(<strong>y</strong>|<strong>Z</strong>) = <em>N</em>(<strong>y</strong>|<strong>0</strong>,<strong>K</strong>),</span><br /> where <span class="math inline"><strong>K</strong></span> is the covariance function or .</p></li>
<li><p>The  with noise has the form<br /><span class="math display"><strong>K</strong> = <strong>Z</strong><strong>Z</strong><sup>⊤</sup> + <em>σ</em><sup>2</sup><strong>I</strong></span><br /></p></li>
<li><p>Priors over non-linear functions are also possible.</p>
<ul>
<li>To see what functions look like, we can sample from the prior process.</li>
</ul>
<p>%</p></li>
</ul>
<p><strong>Posterior Distribution over Functions</strong></p>
<ul>
<li><p>Gaussian processes are often used for regression.</p></li>
<li><p>We are given a known inputs <span class="math inline"><strong>Z</strong></span> and targets <span class="math inline"><strong>Y</strong></span>.</p></li>
<li><p>We assume a prior distribution over functions by selecting a kernel.</p></li>
<li><p>Combine the prior with data to get a  distribution over functions.</p>
<p>%</p>
<p></p>
<p>                 </p>
<p>\begin{columns}[c]</p>
<p></p>
<p><strong>Dual Probabilistic PCA</strong></p>
<p></p>
<p></p>
<pre><code>\includegraphics&lt;1-&gt;[width=0.5\textwidth]{../../../gplvm/tex/diagrams/gplvmGraph}</code></pre>
<p></p></li>
</ul>
<p><br /><span class="math display">$$
 p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}N\left(\mathbf{ y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^{2}\mathbf{I}\right)$$</span><br /> <br /><span class="math display">$$
 p\left(\mathbf{W}\right)=\prod_{i=1}^{d}N\left(\mathbf{ w}_{i,:}|\mathbf{0},\mathbf{I}\right)$$</span><br /> </p>
<p><strong>RBF Kernel</strong></p>
<ul>
<li><p>The RBF kernel has the form <span class="math inline"><em>k</em><sub><em>i</em>, <em>j</em></sub> = <em>k</em>(<strong>z</strong><sub><em>i</em>, :</sub>,<strong>z</strong><sub><em>j</em>, :</sub>),</span> where <br /><span class="math display">$$
k\left(\mathbf{ z}_{i,:},\mathbf{ z}_{j,:}\right)=\alpha\exp\left(-\frac{\left(\mathbf{ z}_{i,:}-\mathbf{ z}_{j,:}\right)^{\top}\left(\mathbf{ z}_{i,:}-\mathbf{ z}_{j,:}\right)}{2\ell^{2}}\right).
$$</span><br /></p></li>
<li><p>No longer possible to optimise wrt <span class="math inline"><strong>Z</strong></span> via an eigenvalue problem.</p></li>
<li><p>Instead find gradients with respect to <span class="math inline"><strong>Z</strong>, <em>α</em>, ℓ</span> and <span class="math inline"><em>σ</em><sup>2</sup></span> and optimise using gradient methods.</p></li>
</ul>
<h2 id="oil-data">Oil Data</h2>
<div class="figure">
<div id="-figure" class="figure-frame">

</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>{</p>
<p><strong>Nearest Neighbour error in <span class="math inline"><strong>Z</strong></span></strong></p>
<ul>
<li>Nearest neighbour classification in latent space.</li>
</ul>
<p><em>cf</em> 2 errors in data space.</p>
<h2 id="stick-man-data">Stick Man Data</h2>
<p><code>demStick1</code></p>
%
<h2 id="applications">Applications</h2>
<ul>
<li><p>Style based inverse kinematics <span class="citation" data-cites="Grochow:styleik04">(Grochow et al. 2004)</span>.</p></li>
<li><p>Prior distributions for tracking <span class="citation" data-cites="Urtasun:3dpeople06">(Urtasun, Fleet, and Fua 2006, Urtasun:priors05)</span>.</p></li>
<li><p>Assisted drawing <span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo 2006)</span>.</p></li>
<li><p>GPLVM based on a dual probabilistic interpretation of PCA.</p></li>
<li><p>Straightforward to non-linearise it using Gaussian processes.</p></li>
<li><p>Result is a non-linear probabilistic PCA.</p></li>
<li><p><em>Optimise latent variables</em> rather than integrate them out.</p></li>
</ul>
<h2 id="getting-started-and-downloading-data">Getting Started and Downloading Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/gplvm-tutorial-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/gplvm-tutorial-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="im">import</span> GPy</span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="im">import</span> string</span></code></pre></div>
<p>The following code is for plotting and to prepare the bigger models for later useage. If you are interested, you can have a look, but this is not essential.</p>
<p>For this lab, we’ll use a data set containing all handwritten digits from <span class="math inline">0⋯9</span> handwritten, provided by <span class="citation" data-cites="deCampos-character09">de Campos, Babu, and Varma (2009)</span>. We will only use some of the digits for the demonstrations in this lab class, but you can edit the code below to select different subsets of the digit data as you wish.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>which <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">9</span>] <span class="co"># which digits to work on</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>data <span class="op">=</span> pods.datasets.decampos_digits(which_digits<span class="op">=</span>which)</span>
<span id="cb22-3"><a href="#cb22-3"></a>Y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb22-4"><a href="#cb22-4"></a>labels <span class="op">=</span> data[<span class="st">&#39;str_lbls&#39;</span>]</span></code></pre></div>
<p>You can try to plot some of the digits using <code>plt.matshow</code> (the digit images have size <code>16x16</code>).</p>
<h2 id="principal-component-analysis-2">Principal Component Analysis</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/gplvm-tutorial.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/gplvm-tutorial.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Principal component analysis (PCA) finds a rotation of the observed outputs, such that the rotated principal component (PC) space maximizes the variance of the data observed, sorted from most to least important (most to least variable in the corresponding PC).</p>
<p>In order to apply PCA in an easy way, we have included a PCA module in pca.py. You can import the module by import &lt;path.to.pca&gt; (without the ending .py!). To run PCA on the digits we have to reshape (Hint: np.reshape ) digits .</p>
<ul>
<li>What is the right shape <span class="math inline"><em>n</em> × <em>p</em></span> to use?</li>
</ul>
<p>We will call the reshaped observed outputs <span class="math inline"><strong>Y</strong></span> in the following.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>Yn <span class="op">=</span> Y<span class="co">#Y-Y.mean()</span></span></code></pre></div>
<p>Now let’s run PCA on the reshaped dataset <span class="math inline"><strong>Y</strong></span>:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="im">from</span> GPy.util <span class="im">import</span> pca</span>
<span id="cb24-2"><a href="#cb24-2"></a>p <span class="op">=</span> pca.pca(Y) <span class="co"># create PCA class with digits dataset</span></span></code></pre></div>
<p>The resulting plot will show the lower dimensional representation of the digits in 2 dimensions.</p>
<h2 id="gaussian-process-latent-variable-model">Gaussian Process Latent Variable Model</h2>
<p>The Gaussian Process Latent Variable Model (GP-LVM) <span class="citation" data-cites="Lawrence:pnpca05">(Lawrence 2005)</span> embeds PCA into a Gaussian process framework, where the latent inputs <span class="math inline"><strong>Z</strong></span> are learnt as hyperparameters and the mapping variables <span class="math inline"><strong>W</strong></span> are integrated out. The advantage of this interpretation is it allows PCA to be generalized in a non linear way by replacing the resulting <em>linear</em> covariance witha non linear covariance. But first, let’s see how GPLVM is equivalent to PCA using an automatic relevance determination (ARD, see e.g. <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span>) linear kernel:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>input_dim <span class="op">=</span> <span class="dv">4</span> <span class="co"># How many latent dimensions to use</span></span>
<span id="cb25-2"><a href="#cb25-2"></a>kernel <span class="op">=</span> GPy.kern.Linear(input_dim, ARD<span class="op">=</span><span class="va">True</span>) <span class="co"># ARD kernel</span></span>
<span id="cb25-3"><a href="#cb25-3"></a>m <span class="op">=</span> GPy.models.GPLVM(Yn, input_dim<span class="op">=</span>input_dim, kernel<span class="op">=</span>kernel)</span>
<span id="cb25-4"><a href="#cb25-4"></a></span>
<span id="cb25-5"><a href="#cb25-5"></a>m.optimize(messages<span class="op">=</span><span class="dv">1</span>, max_iters<span class="op">=</span><span class="dv">1000</span>) <span class="co"># optimize for 1000 iterations</span></span></code></pre></div>
<p>As you can see the solution with a linear kernel is the same as the PCA solution with the exception of rotational changes and axis flips.</p>
<p>For the sake of time, the solution you see was only running for 1000 iterations, thus it might not be converged fully yet. The GP-LVM proceeds by iterative optimization of the <em>inputs</em> to the covariance. As we saw in the lecture earlier, for the linear covariance, these latent points can be optimized with an eigenvalue problem, but generally, for non-linear covariance functions, we are obliged to use gradient based optimization.</p>
<h2 id="cmu-mocap-database">CMU Mocap Database</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/cmu-mocap-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/cmu-mocap-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Motion capture data from the CMU motion capture data base <span class="citation" data-cites="CMU-mocap03">(CMU Motion Capture Labb 2003)</span>.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="im">import</span> pods</span></code></pre></div>
<p>You can download any subject and motion from the data set. Here we will download motion <code>01</code> from subject <code>35</code>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>subject<span class="op">=</span><span class="st">&#39;35&#39;</span> </span>
<span id="cb27-2"><a href="#cb27-2"></a>motion<span class="op">=</span>[<span class="st">&#39;01&#39;</span>]</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>data <span class="op">=</span> pods.datasets.cmu_mocap(subject, motion)</span></code></pre></div>
<p>The data dictionary contains the keys ‘Y’ and ‘skel’, which represent the data and the skeleton..</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>data[<span class="st">&#39;Y&#39;</span>].shape</span></code></pre></div>
<p>The data was used in the hierarchical GP-LVM paper <span class="citation" data-cites="Lawrence:hgplvm07">(Lawrence and Moore 2007)</span> in an experiment that was also recreated in the Deep Gaussian process paper <span class="citation" data-cites="Damianou:deepgp13">(Damianou and Lawrence 2013)</span>.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="bu">print</span>(data[<span class="st">&#39;citation&#39;</span>])</span></code></pre></div>
<p>And extra information about the data is included, as standard, under the keys <code>info</code> and <code>details</code>.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="bu">print</span>(data[<span class="st">&#39;info&#39;</span>])</span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="bu">print</span>()</span>
<span id="cb31-3"><a href="#cb31-3"></a><span class="bu">print</span>(data[<span class="st">&#39;details&#39;</span>])</span></code></pre></div>
<p>The original data has the figure moving across the floor during the motion capture sequence. We can make the figure walk ‘in place’, by setting the x, y, z positions of the root node to zero. This makes it easier to visualize the result.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="co"># Make figure move in place.</span></span>
<span id="cb32-2"><a href="#cb32-2"></a>data[<span class="st">&#39;Y&#39;</span>][:, <span class="dv">0</span>:<span class="dv">3</span>] <span class="op">=</span> <span class="fl">0.0</span></span></code></pre></div>
<p>We can also remove the mean of the data.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>Y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb33-2"><a href="#cb33-2"></a>Y_mean <span class="op">=</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb33-3"><a href="#cb33-3"></a>Y_std <span class="op">=</span> Y.std(<span class="dv">0</span>)</span>
<span id="cb33-4"><a href="#cb33-4"></a>Yhat <span class="op">=</span> (Y<span class="op">-</span>Y_mean)<span class="op">/</span>Y_std</span></code></pre></div>
<p>Now we create the GP-LVM model.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a>model <span class="op">=</span> GPy.models.GPLVM(Yhat, <span class="dv">2</span>)</span></code></pre></div>
<p>Now we optimize the model.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>model.optimize(messages<span class="op">=</span><span class="va">True</span>, max_f_eval<span class="op">=</span><span class="dv">10000</span>)</span></code></pre></div>
<h2 id="example-latent-doodle-space">Example: Latent Doodle Space</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/latent-doodle-space.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/latent-doodle-space.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="latent-doodle-space-figure" class="figure-frame">
<iframe width height src="https://player.vimeo.com/video/3235882#t=" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>
</iframe>
</div>
<div id="latent-doodle-space-magnify" class="magnify" onclick="magnifyFigure(&#39;latent-doodle-space&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="latent-doodle-space-caption" class="caption-frame">
<p>Figure: The latent doodle space idea of <span class="citation" data-cites="Baxter:doodle06">Baxter and Anjyo (2006)</span> manages to build a smooth mapping across very sparse data.</p>
</div>
</div>
<p><strong>Generalization with much less Data than Dimensions</strong></p>
<ul>
<li><p>Powerful uncertainly handling of GPs leads to surprising properties.</p></li>
<li><p>Non-linear models can be used where there are fewer data points than dimensions <em>without overfitting</em>.</p></li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo 2006)</span></span></p>
<h2 id="example-continuous-character-control">Example: Continuous Character Control</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/character-control.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/character-control.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<ul>
<li>Graph diffusion prior for enforcing connectivity between motions. <br /><span class="math display">log <em>p</em>(<strong>X</strong>) = <em>w</em><sub><em>c</em></sub>∑<sub><em>i</em>, <em>j</em></sub>log <em>K</em><sub><em>i</em><em>j</em></sub><sup><em>d</em></sup></span><br /> with the graph diffusion kernel <span class="math inline"><strong>K</strong><sup><em>d</em></sup></span> obtain from <br /><span class="math display"><em>K</em><sub><em>i</em><em>j</em></sub><sup><em>d</em></sup> = exp (<em>β</em><strong>H</strong>)   with   <strong>H</strong> =  − <strong>T</strong><sup> − 1/2</sup><strong>L</strong><strong>T</strong><sup> − 1/2</sup></span><br /> the graph Laplacian, and <span class="math inline"><strong>T</strong></span> is a diagonal matrix with <span class="math inline"><em>T</em><sub><em>i</em><em>i</em></sub> = ∑<sub><em>j</em></sub><em>w</em>(<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>)</span>, <br /><span class="math display">$$L_{ij} = \begin{cases} \sum_k w(\mathbf{ x}_i,\mathbf{ x}_k) &amp; \text{if $i=j$}
\\
-w(\mathbf{ x}_i,\mathbf{ x}_j) &amp;\text{otherwise.}
\end{cases}$$</span><br /> and <span class="math inline"><em>w</em>(<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) = ||<strong>x</strong><sub><em>i</em></sub> − <strong>x</strong><sub><em>j</em></sub>||<sup> − <em>p</em></sup></span> measures similarity.</li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span></span></p>
<h2 id="character-control-results">Character Control: Results</h2>
<div class="figure">
<div id="charcter-control-gplvm-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/hr3pdDl5IAg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="charcter-control-gplvm-magnify" class="magnify" onclick="magnifyFigure(&#39;charcter-control-gplvm&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="charcter-control-gplvm-caption" class="caption-frame">
<p>Figure: Character control in the latent space described the the GP-LVM <span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span>.</p>
</div>
</div>
<h2 id="data-for-blastocyst-development-in-mice-single-cell-taqman-arrays">Data for Blastocyst Development in Mice: Single Cell TaqMan Arrays</h2>
<p>Now we analyze some single cell data from <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span>. Tey performed qPCR TaqMan array on single cells from the developing blastocyst in mouse. The data is taken from the early stages of development when the Blastocyst is forming. At the 32 cell stage the data is already separated into the trophectoderm (TE) which goes onto form the placenta and the inner cellular mass (ICM). The ICM further differentiates into the epiblast (EPI)—which gives rise to the endoderm, mesoderm and ectoderm—and the primitive endoderm (PE) which develops into the amniotic sack. <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> selected 48 genes for expression measurement. They labelled the resulting cells and their labels are included as an aide to visualization.</p>
<p>They first visualized their data using principal component analysis. In the first two principal components this fails to separate the domains. This is perhaps because the principal components are dominated by the variation in the 64 cell systems. This in turn may be because there are more cells from the data set in that regime, and may be because the natural variation is greater. We first recreate their visualization using principal component analysis.</p>
<p>In this notebook we will perform PCA on the original data, showing that the different regimes do not separate.</p>
<p>Next we load in the data. We’ve provided a convenience function for loading in the data with <code>pods</code>. It is loaded in as a <code>pandas</code> DataFrame. This allows us to summarize it with the <code>describe</code> attribute.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>data <span class="op">=</span> pods.datasets.singlecell()</span>
<span id="cb38-2"><a href="#cb38-2"></a>Y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb38-3"><a href="#cb38-3"></a>Y.describe</span></code></pre></div>
<h2 id="principal-component-analysis-3">Principal Component Analysis</h2>
<p>Now we follow <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> in performing PCA on the data. Rather than calling a ‘PCA routine’, here we break the algorithm down into its steps: compute the data covariance, compute the eigenvalues and eigenvectors and sort according to magnitude of eigenvalue. Because we want to visualize the data, we’ve chose to compute the eigenvectors of the <em>inner product matrix</em> rather than the covariance matrix. This allows us to plot the eigenvalues directly. However, this is less efficient (in this case because the number of genes is smaller than the number of data) than computing the eigendecomposition of the covariance matrix.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># obtain a centred version of data.</span></span>
<span id="cb40-2"><a href="#cb40-2"></a>centredY <span class="op">=</span> Y <span class="op">-</span> Y.mean()</span>
<span id="cb40-3"><a href="#cb40-3"></a><span class="co"># compute inner product matrix</span></span>
<span id="cb40-4"><a href="#cb40-4"></a>C <span class="op">=</span> np.dot(centredY,centredY.T)</span>
<span id="cb40-5"><a href="#cb40-5"></a><span class="co"># perform eigendecomposition</span></span>
<span id="cb40-6"><a href="#cb40-6"></a>V, U <span class="op">=</span> np.linalg.eig(C)</span>
<span id="cb40-7"><a href="#cb40-7"></a><span class="co"># sort eigenvalues and vectors according to size</span></span>
<span id="cb40-8"><a href="#cb40-8"></a>ind <span class="op">=</span> V.argsort()</span>
<span id="cb40-9"><a href="#cb40-9"></a>ev <span class="op">=</span> V[ind[::<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb40-10"><a href="#cb40-10"></a>U <span class="op">=</span> U[:, ind[::<span class="op">-</span><span class="dv">1</span>]]</span></code></pre></div>
<p>To visualize the result, we now construct a simple helper function. This will ensure that the plots have the same legends as the GP-LVM plots we use below.</p>
<h2 id="pca-result">PCA Result</h2>
<p>Now, using the helper function we can plot the results with appropriate labels.</p>
<div class="figure">
<div id="singlecell-data-pca-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/singlecell-data-pca.svg" width="60%" style=" ">
</object>
</div>
<div id="singlecell-data-pca-magnify" class="magnify" onclick="magnifyFigure(&#39;singlecell-data-pca&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="singlecell-data-pca-caption" class="caption-frame">
<p>Figure: First two principal compoents of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data.</p>
</div>
</div>
<h2 id="gp-lvm-on-the-data">GP-LVM on the Data</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/singlecell-gplvm.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gplvm/includes/singlecell-gplvm.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Work done as a collaboration between Max Zwiessele, Oliver Stegle and Neil D. Lawrence.</p>
<p>Then, we follow <span class="citation" data-cites="Buettner:resolving12">Buettner and Theis (2012)</span> in applying the GP-LVM to the data. There is a slight pathology in the result, one which they fixed by using priors that were dependent on the developmental stage. We then show how the Bayesian GP-LVM doesn’t exhibit those pathologies and gives a nice results that seems to show the lineage of the cells.</p>
<p>They used modified prior to ensure that small differences between cells at the same differential stage were preserved. Here we apply a standard GP-LVM (no modified prior) to the data.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>kernel <span class="op">=</span> GPy.kern.RBF(<span class="dv">2</span>)<span class="op">+</span>GPy.kern.Bias(<span class="dv">2</span>)</span>
<span id="cb42-2"><a href="#cb42-2"></a>model <span class="op">=</span> GPy.models.GPLVM(Y.values, <span class="dv">2</span>, kernel<span class="op">=</span>kernel)</span>
<span id="cb42-3"><a href="#cb42-3"></a>model.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="figure">
<div id="singlecell-gplvm-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/singlecell-gplvm.svg" width="60%" style=" ">
</object>
</div>
<div id="singlecell-gplvm-magnify" class="magnify" onclick="magnifyFigure(&#39;singlecell-gplvm&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="singlecell-gplvm-caption" class="caption-frame">
<p>Figure: Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with the GP-LVM.</p>
</div>
</div>
<div class="figure">
<div id="singlecell-gplvm-ard-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/singlecell-gplvm-ard.svg" width="80%" style=" ">
</object>
</div>
<div id="singlecell-gplvm-ard-magnify" class="magnify" onclick="magnifyFigure(&#39;singlecell-gplvm-ard&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="singlecell-gplvm-ard-caption" class="caption-frame">
<p>Figure: The ARD parameters of the GP-LVM for the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data.</p>
</div>
</div>
<h2 id="blastocyst-data-isomap">Blastocyst Data: Isomap</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/singlecell-isomap.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/singlecell-isomap.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Isomap first builds a neighbourhood graph, and then uses distances along this graph to approximate the geodesic distance between points. These distances are then visualized by performing classical multidimensional scaling (which involves computing the eigendecomposition of the centred distance matrix). As the neighborhood size is increased to match the data, principal component analysis is recovered (or strictly speaking, principal coordinate analysis). The fewer the neighbors, the more ‘non-linear’ the isomap embeddings are.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="im">import</span> sklearn.manifold</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>n_neighbors <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb44-2"><a href="#cb44-2"></a>model <span class="op">=</span> sklearn.manifold.Isomap(n_neighbors<span class="op">=</span>n_neighbors, n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb44-3"><a href="#cb44-3"></a>X <span class="op">=</span> model.fit_transform(Y)</span></code></pre></div>
<div class="figure">
<div id="singlecell-isomap-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/singlecell-isomap.svg" width="60%" style=" ">
</object>
</div>
<div id="singlecell-isomap-magnify" class="magnify" onclick="magnifyFigure(&#39;singlecell-isomap&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="singlecell-isomap-caption" class="caption-frame">
<p>Figure: Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with Isomap.</p>
</div>
</div>
<h2 id="blastocyst-data-locally-linear-embedding">Blastocyst Data: Locally Linear Embedding</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/singlecell-lle.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_dimred/includes/singlecell-lle.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Next we try locally linear embedding. In locally linear embedding a neighborhood is also computed. Each point is then reconstructed by it’s neighbors using a linear weighting. This implies a locally linear patch is being fitted to the data in that region. These patches are assimilated into a large <span class="math inline"><em>n</em> × <em>n</em></span> matrix and a lower dimensional data set which reflects the same relationships is then sought. Quite a large number of neighbours needs to be selected for the data to not collapse in on itself. When a large number of neighbours is selected the embedding is more linear and begins to look like PCA. However, the algorithm does <em>not</em> converge to PCA in the limit as the number of neighbors approaches <span class="math inline"><em>n</em></span>.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="im">import</span> sklearn.manifold</span></code></pre></div>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a>n_neighbors <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb46-2"><a href="#cb46-2"></a>model <span class="op">=</span> sklearn.manifold.LocallyLinearEmbedding(n_neighbors<span class="op">=</span>n_neighbors, n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb46-3"><a href="#cb46-3"></a>X <span class="op">=</span> model.fit_transform(Y)</span></code></pre></div>
<div class="figure">
<div id="singlecell-lle-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/singlecell-lle.svg" width="60%" style=" ">
</object>
</div>
<div id="singlecell-lle-magnify" class="magnify" onclick="magnifyFigure(&#39;singlecell-lle&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="singlecell-lle-caption" class="caption-frame">
<p>Figure: Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with a locally linear embedding.</p>
</div>
</div>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Baxter:doodle06">
<p>Baxter, William V., and Ken-Ichi Anjyo. 2006. “Latent Doodle Space.” In <em>EUROGRAPHICS</em>, 25:477–85. 3. Vienna, Austria. <a href="https://doi.org/10.1111/j.1467-8659.2006.00967.x">https://doi.org/10.1111/j.1467-8659.2006.00967.x</a>.</p>
</div>
<div id="ref-Bishop:book06">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. springer.</p>
</div>
<div id="ref-Buettner:resolving12">
<p>Buettner, Florian, and Fabian J. Theis. 2012. “A Novel Approach for Resolving Differences in Single-Cell Gene Expression Patterns from Zygote to Blastocyst.” <em>Bioinformatics</em> 28 (18): i626–i632. <a href="https://doi.org/10.1093/bioinformatics/bts385">https://doi.org/10.1093/bioinformatics/bts385</a>.</p>
</div>
<div id="ref-CMU-mocap03">
<p>CMU Motion Capture Labb. 2003. “The CMU Mocap Database.” Available from http://mocap.cs.cmu.edu.</p>
</div>
<div id="ref-Damianou:deepgp13">
<p>Damianou, Andreas, and Neil D. Lawrence. 2013. “Deep Gaussian Processes.” In, 31:207–15.</p>
</div>
<div id="ref-deCampos-character09">
<p>de Campos, Teófilo E., Bodla Rakesh Babu, and Manik Varma. 2009. “Character Recognition in Natural Images.” In <em>Proceedings of the Fourth International Conference on Computer Vision Theory and Applications - Volume 2: VISAPP, (Visigrapp 2009)</em>, 273–80. INSTICC; SciTePress. <a href="https://doi.org/10.5220/0001770102730280">https://doi.org/10.5220/0001770102730280</a>.</p>
</div>
<div id="ref-Grochow:styleik04">
<p>Grochow, Keith, Steven L. Martin, Aaron Hertzmann, and Zoran Popovic. 2004. “Style-Based Inverse Kinematics.” In <em>ACM Transactions on Graphics (Siggraph 2004)</em>, 522–31. <a href="https://doi.org/10.1145/1186562.1015755">https://doi.org/10.1145/1186562.1015755</a>.</p>
</div>
<div id="ref-Guo:fate10">
<p>Guo, Guoji, Mikael Huss, Guo Qing Tong, Chaoyang Wang, Li Li Sun, Neil D. Clarke, and Paul Robsonemail. 2010. “Resolution of Cell Fate Decisions Revealed by Single-Cell Gene Expression Analysis from Zygote to Blastocyst.” <em>Developmental Cell</em> 18 (4): 675–85. <a href="https://doi.org/10.1016/j.devcel.2010.02.012">https://doi.org/10.1016/j.devcel.2010.02.012</a>.</p>
</div>
<div id="ref-Hastie:pcurves89">
<p>Hastie, Trevor, and W. Stuetzle. 1989. “Principal Curves.” <em>Journal of the American Statistical Association</em> 84 (406): 502–16.</p>
</div>
<div id="ref-Hotelling:analysis33">
<p>Hotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” <em>Journal of Educational Psychology</em> 24 (6): 417–41.</p>
</div>
<div id="ref-Lawrence:pnpca05">
<p>Lawrence, Neil D. 2005. “Probabilistic Non-Linear Principal Component Analysis with Gaussian Process Latent Variable Models.” <em>Journal of Machine Learning Research</em> 6 (November): 1783–1816.</p>
</div>
<div id="ref-Lawrence:gplvm03">
<p>———. n.d. “Gaussian Process Models for Visualisation of High Dimensional Data.” In, 329–36.</p>
</div>
<div id="ref-Lawrence:hgplvm07">
<p>Lawrence, Neil D., and Andrew J. Moore. 2007. “Hierarchical Gaussian Process Latent Variable Models.” In, 481–88.</p>
</div>
<div id="ref-Levine:control12">
<p>Levine, Segey, Jack M. Wang, Alexis Haraux, Zoran Popović, and Vladlen Koltun. 2012. “Continuous Character Control with Low-Dimensional Embeddings.” <em>ACM Transactions on Graphics (SIGGRAPH 2012)</em> 31 (4).</p>
</div>
<div id="ref-Mardia:multivariate79">
<p>Mardia, Kantilal V., John T. Kent, and John M. Bibby. 1979. <em>Multivariate Analysis</em>. London: Academic Press.</p>
</div>
<div id="ref-Pearson:01">
<p>Pearson, Karl. 1901. “On Lines and Planes of Closest Fit to Systems of Points in Space.” <em>The London, Edinburgh and Dublin Philosophical Magazine and Journal of Science, Sixth Series</em> 2: 559–72.</p>
</div>
<div id="ref-Roweis:SPCA97">
<p>Roweis, Sam T. n.d. “EM Algorithms for PCA and SPCA.” In, 626–32.</p>
</div>
<div id="ref-Scholkopf:nonlinear98">
<p>Schölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller. 1998. “Nonlinear Component Analysis as a Kernel Eigenvalue Problem.” <em>Neural Computation</em> 10: 1299–1319. <a href="https://doi.org/10.1162/089976698300017467">https://doi.org/10.1162/089976698300017467</a>.</p>
</div>
<div id="ref-Tipping:pca97">
<p>Tipping, Michael E., and Christopher M. Bishop. 1999a. “Mixtures of Probabilistic Principal Component Analysers.” <em>Neural Computation</em> 11 (2): 443–82.</p>
</div>
<div id="ref-Tipping:probpca99">
<p>———. 1999b. “Probabilistic Principal Component Analysis.” <em>Journal of the Royal Statistical Society, B</em> 6 (3): 611–22. <a href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a>.</p>
</div>
<div id="ref-Urtasun:3dpeople06">
<p>Urtasun, Raquel, David J. Fleet, and Pascal Fua. 2006. “3D People Tracking with Gaussian Process Dynamical Models.” In <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, 238–45. New York, U.S.A.: IEEE Computer Society Press. <a href="https://doi.org/10.1109/CVPR.2006.15">https://doi.org/10.1109/CVPR.2006.15</a>.</p>
</div>
</div>

