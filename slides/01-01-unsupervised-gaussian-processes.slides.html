<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2021-01-21">
  <title>R250: Unsupervised Learning with Gaussian Processes I</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">R250: Unsupervised Learning with Gaussian Processes
I</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil
D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2021-01-21</time></p>
  <p class="venue" style="text-align:center">Virtual (Zoom)</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
</section>
<section id="high-dimensional-data" class="slide level2">
<h2>High Dimensional Data</h2>
<ul>
<li>USPS Data Set Handwritten Digit</li>
<li>3648 dimensions (64 rows, 57 columns)</li>
<li>Space contains much more than just this digit.</li>
</ul>
</section>
<section id="usps-samples" class="slide level2">
<h2>USPS Samples</h2>
<script>
showDivs(0, 'dem-six-sample');
</script>
<p><small></small>
<input id="range-dem-six-sample" type="range" min="0" max="4" value="0" onchange="setDivs('dem-six-sample')" oninput="setDivs('dem-six-sample')">
<button onclick="plusDivs(-1, 'dem-six-sample')">❮</button>
<button onclick="plusDivs(1, 'dem-six-sample')">❯</button></p>
<div class="dem-six-sample" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six000.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-sample" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six001.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-sample" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six002.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-sample" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six003.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<ul>
<li>Even if we sample every nanonsecond from now until end of universe
you won’t see original six!</li>
</ul>
</section>
<section id="simple-model-of-digit" class="slide level2">
<h2>Simple Model of Digit</h2>
<ul>
<li>Rotate a prototype</li>
</ul>
<script>
showDivs(1, 'dem-six-rotate');
</script>
<p><small></small>
<input id="range-dem-six-rotate" type="range" min="1" max="6" value="1" onchange="setDivs('dem-six-rotate')" oninput="setDivs('dem-six-rotate')">
<button onclick="plusDivs(-1, 'dem-six-rotate')">❮</button>
<button onclick="plusDivs(1, 'dem-six-rotate')">❯</button></p>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six_rotate001.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six_rotate002.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six_rotate003.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six_rotate004.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six_rotate005.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_six_rotate006.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="dem-six-mainfold-print-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_manifold_print001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/dem_manifold_print002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The rotated sixes projected onto the first two principal components of
the ‘rotated data set’. The data lives on a one dimensional manifold in
the 3,648 dimensional space.
</aside>
</section>
<section id="low-dimensional-manifolds" class="slide level2">
<h2>Low Dimensional Manifolds</h2>
<ul>
<li>Pure rotation is too simple
<ul>
<li>In practice data may undergo several distortions.</li>
</ul></li>
<li>For high dimensional data with <em>structure</em>:
<ul>
<li>We expect fewer distortions than dimensions;</li>
<li>Therefore we expect the data to live on a lower dimensional
manifold.</li>
<li>Conclusion: Deal with high dimensional data by looking for a lower
dimensional non-linear embedding.</li>
</ul></li>
</ul>
</section>
<section id="dimensionality-reduction" class="slide level2">
<h2>Dimensionality Reduction</h2>
<ul>
<li>Compress the data by replacing the original data with reduced number
of continuous variables.</li>
</ul>
<div class="figure">
<div id="marionette-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/marionette.svg" width="40%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Thinking of dimensionality reduction as a marionette. We observe the
high dimensional pose of the puppet, <span
class="math inline">\(\mathbf{ x}\)</span>, but the movement of the
puppeteer’s hand, <span class="math inline">\(\mathbf{ z}\)</span>
remains hidden to us. Dimensionality reduction aims to recover those
hidden movements which generated the observations.
</aside>
</section>
<section id="dimensionality-reduction-1" class="slide level2">
<h2>Dimensionality Reduction</h2>
<ul>
<li>Position of each body part of a marionette could be thought of as
our data, <span class="math inline">\(\mathbf{ x}_i\)</span>.</li>
<li>Each data point is the 3-D co-ordinates of all the different body
parts</li>
<li>Movement of parts determined by puppeteer via strings.</li>
<li>For a simple puppet with one stick can move the stick up and down,
left and right and twist.</li>
</ul>
</section>
<section id="dimensionality-reduction-2" class="slide level2">
<h2>Dimensionality Reduction</h2>
<ul>
<li>This gives three parameters in the puppeteers control.</li>
<li>Implies that the puppet we see moving is controlled by only 3
variables.</li>
<li>These 3 variables are often called the hidden or <em>latent
variables</em>.</li>
<li>Assume similar for real world data, observations are derived from
lower dimensional underlying process</li>
</ul>
</section>
<section id="examples-in-social-sciences" class="slide level2">
<h2>Examples in Social Sciences</h2>
<ul>
<li>Underpins <em>psychological scoring</em> such as <em>IQ</em> or
<em>personality tests</em></li>
<li>Myers-Briggs assumes personality is four dimensional.</li>
<li>Political belief (left/right wing).</li>
<li>Also language modelling has taken similar approaches: <a
href="https://arxiv.org/abs/1301.3781">word2vec</a></li>
</ul>
</section>
<section id="gaussian-variables-and-linear-dimensionality-reduction"
class="slide level2">
<h2>Gaussian Variables and Linear Dimensionality Reduction</h2>
<ul>
<li>Return to non-linear shortly.</li>
<li>Now: Linear dimensionality reduction.</li>
<li>First: Review Gaussian density properties.</li>
</ul>
</section>
<section id="two-important-gaussian-properties" class="slide level2">
<h2>Two Important Gaussian Properties</h2>
</section>
<section id="sum-of-gaussians" class="slide level2">
<h2>Sum of Gaussians</h2>
<div class="fragment">
<div style="text-align:left">
Sum of Gaussian variables is also Gaussian.
</div>
<p><span class="math display">\[y_i \sim
\mathcal{N}\left(\mu_i,\sigma_i^2\right)\]</span></p>
</div>
<div class="fragment">
<div style="text-align:left">
And the sum is distributed as
</div>
<p><span class="math display">\[
\sum_{i=1}^{n} y_i \sim
\mathcal{N}\left(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2\right)
\]</span></p>
</div>
<div class="fragment">
<p><small>(<em>Aside</em>: As sum increases, sum of non-Gaussian, finite
variance variables is also Gaussian because of <a
href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit
theorem</a>.)</small></p>
</div>
</section>
<section id="scaling-a-gaussian" class="slide level2">
<h2>Scaling a Gaussian</h2>
<div class="fragment">
<div style="text-align:left">
Scaling a Gaussian leads to a Gaussian.
</div>
</div>
<div class="fragment">
<p><span class="math display">\[y\sim
\mathcal{N}\left(\mu,\sigma^2\right)\]</span></p>
</div>
<div class="fragment">
<div style="text-align:left">
And the scaled variable is distributed as
</div>
<p><span class="math display">\[wy\sim \mathcal{N}\left(w\mu,w^2
\sigma^2\right).\]</span></p>
<!-- SECTION Linear Latent Variable Models -->
</div>
</section>
<section id="linear-latent-variable-models" class="slide level2">
<h2>Linear Latent Variable Models</h2>
</section>
<section id="multivariate-gaussian-properties" class="slide level2">
<h2>Multivariate Gaussian Properties</h2>
<ul>
<li><p>If <span class="math display">\[
\mathbf{ y}= \mathbf{W}\mathbf{ x}+ \boldsymbol{ \epsilon},
\]</span></p></li>
<li><p>Assume <span class="math display">\[
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{
\mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim
\mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
\]</span></p></li>
<li><p>Then <span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{ \Sigma}\right).
\]</span> If <span class="math inline">\(\boldsymbol{
\Sigma}=\sigma^2\mathbf{I}\)</span>, this is Probabilistic PCA <span
class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop,
1999)</span>.</p></li>
</ul>
<!-- SECTION Latent Variables -->
</section>
<section id="latent-variables" class="slide level2">
<h2>Latent Variables</h2>
<!-- SECTION Your Personality -->
</section>
<section id="your-personality" class="slide level2">
<h2>Your Personality</h2>
</section>
<section id="factor-analysis-model" class="slide level2">
<h2>Factor Analysis Model</h2>
<p><span class="math display">\[
\mathbf{ y}= \mathbf{f}(\mathbf{ z}) + \boldsymbol{ \epsilon},
\]</span></p>
<p><span class="math display">\[
\mathbf{f}(\mathbf{ z}) = \mathbf{W}\mathbf{ z}
\]</span></p>
</section>
<section id="closely-related-to-linear-regression" class="slide level2">
<h2>Closely Related to Linear Regression</h2>
<p><span class="math display">\[
\mathbf{f}(\mathbf{ z}) =
\begin{bmatrix} f_1(\mathbf{ z}) \\ f_2(\mathbf{ z}) \\ \vdots \\
f_p(\mathbf{ z})\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
f_j(\mathbf{ z}) =
\mathbf{ w}_{j, :}^\top \mathbf{ z},
\]</span></p>
<p><span class="math display">\[
\epsilon_j \sim \mathcal{N}\left(0,\sigma^2_j\right).
\]</span></p>
</section>
<section id="data-representation" class="slide level2">
<h2>Data Representation</h2>
<p><span class="math display">\[
\mathbf{Y}
= \begin{bmatrix} \mathbf{ y}_{1, :}^\top \\ \mathbf{ y}_{2, :}^\top \\
\vdots \\
\mathbf{ y}_{n, :}^\top\end{bmatrix},
\]</span></p>
<p><span class="math display">\[
\mathbf{F} = \mathbf{Z}\mathbf{W}^\top,
\]</span></p>
</section>
<section id="latent-variables-vs-linear-regression"
class="slide level2">
<h2>Latent Variables vs Linear Regression</h2>
<p><span class="math display">\[
x_{i,j} \sim
\mathcal{N}\left(0,1\right),
\]</span> and we can write the density governing the latent variable
associated with a single point as, <span class="math display">\[
\mathbf{ z}_{i, :} \sim \mathcal{N}\left(\mathbf{0},\mathbf{I}\right).
\]</span></p>
<p><span class="math display">\[
\mathbf{f}_{i, :} =
\mathbf{f}(\mathbf{ z}_{i, :}) = \mathbf{W}\mathbf{ z}_{i, :}
\]</span></p>
<p><span class="math display">\[
\mathbf{f}_{i, :} \sim
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top\right)
\]</span></p>
</section>
<section id="data-distribution" class="slide level2">
<h2>Data Distribution</h2>
<p><span class="math display">\[
\mathbf{ y}_{i, :} = \sim
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top +
\boldsymbol{\Sigma}\right)
\]</span></p>
<p><span class="math display">\[
\boldsymbol{\Sigma} = \begin{bmatrix}\sigma^2_{1} &amp; 0 &amp; 0 &amp;
0\\
0 &amp; \sigma^2_{2} &amp; 0 &amp; 0\\
                                     0 &amp; 0 &amp; \ddots &amp;
0\\
                                     0 &amp; 0 &amp; 0 &amp;
\sigma^2_p\end{bmatrix}.
\]</span></p>
</section>
<section id="mean-vector" class="slide level2">
<h2>Mean Vector</h2>
<p><span class="math display">\[
\mathbf{ y}_{i, :} = \mathbf{W}\mathbf{ z}_{i, :} +
\boldsymbol{ \mu}+ \boldsymbol{ \epsilon}_{i, :}
\]</span></p>
<p><span class="math display">\[
\boldsymbol{ \mu}= \frac{1}{n} \sum_{i=1}^n
\mathbf{ y}_{i, :},
\]</span> <span class="math inline">\(\mathbf{C}=
\mathbf{W}\mathbf{W}^\top + \boldsymbol{\Sigma}\)</span></p>
<!-- SECTION Principal Component Analysis -->
</section>
<section id="principal-component-analysis" class="slide level2">
<h2>Principal Component Analysis</h2>
<p><span class="citation" data-cites="Hotelling:analysis33">Hotelling
(1933)</span> took <span class="math inline">\(\sigma^2_i \rightarrow
0\)</span> so <span class="math display">\[
\mathbf{ y}_{i, :} \sim \lim_{\sigma^2 \rightarrow 0}
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2
\mathbf{I}\right).
\]</span></p>
</section>
<section id="degenerate-covariance" class="slide level2">
<h2>Degenerate Covariance</h2>
<p><small> <span class="math display">\[
p(\mathbf{ y}_{i, :}|\mathbf{W}) =
\lim_{\sigma^2 \rightarrow 0} \frac{1}{(2\pi)^\frac{p}{2}
|\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}|^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}\mathbf{ y}_{i, :}\left[\mathbf{W}\mathbf{W}^\top+
\sigma^2
\mathbf{I}\right]^{-1}\mathbf{ y}_{i, :}\right),
\]</span></small></p>
</section>
<section id="computation-of-the-marginal-likelihood"
class="slide level2">
<h2>Computation of the Marginal Likelihood</h2>
<p><span class="math display">\[
\mathbf{ y}_{i,:}=\mathbf{W}\mathbf{ z}_{i,:}+\boldsymbol{
\epsilon}_{i,:},\quad \mathbf{ z}_{i,:} \sim
\mathcal{N}\left(\mathbf{0},\mathbf{I}\right), \quad \boldsymbol{
\epsilon}_{i,:} \sim
\mathcal{N}\left(\mathbf{0},\sigma^{2}\mathbf{I}\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{W}\mathbf{ z}_{i,:} \sim
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{W}\mathbf{ z}_{i, :} + \boldsymbol{ \epsilon}_{i, :} \sim
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2
\mathbf{I}\right)
\]</span></p>
</section>
<section id="linear-latent-variable-model-ii" class="slide level2">
<h2>Linear Latent Variable Model II</h2>
<p><strong>Probabilistic PCA Max. Likelihood Soln</strong> (<span
class="citation" data-cites="Tipping:probpca99">Tipping and Bishop
(1999)</span>)</p>
<div class="figure">
<div id="ppca-graph-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//dimred/ppca_graph.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Graphical model representing probabilistic PCA.
</aside>
<p><span
class="math display">\[p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{
y}_{i,
:}|\mathbf{0},\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\right)\]</span></p>
</section>
<section id="linear-latent-variable-model-ii-1" class="slide level2">
<h2>Linear Latent Variable Model II</h2>
<p><strong>Probabilistic PCA Max. Likelihood Soln</strong> (<span
class="citation" data-cites="Tipping:probpca99">Tipping and Bishop
(1999)</span>) <span class="math display">\[
  p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{
y}_{i,:}|\mathbf{0},\mathbf{C}\right),\quad
\mathbf{C}=\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}
  \]</span> <span class="math display">\[
  \log
p\left(\mathbf{Y}|\mathbf{W}\right)=-\frac{n}{2}\log\left|\mathbf{C}\right|-\frac{1}{2}\text{tr}\left(\mathbf{C}^{-1}\mathbf{Y}^{\top}\mathbf{Y}\right)+\text{const.}
  \]</span> If <span class="math inline">\(\mathbf{U}_{q}\)</span> are
first <span class="math inline">\(q\)</span> principal eigenvectors of
<span class="math inline">\(n^{-1}\mathbf{Y}^{\top}\mathbf{Y}\)</span>
and the corresponding eigenvalues are <span
class="math inline">\(\boldsymbol{\Lambda}_{q}\)</span>, <span
class="math display">\[
  \mathbf{W}=\mathbf{U}_{q}\mathbf{L}\mathbf{R}^{\top},\quad\mathbf{L}=\left(\boldsymbol{\Lambda}_{q}-\sigma^{2}\mathbf{I}\right)^{\frac{1}{2}}
  \]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an
arbitrary rotation matrix.</p>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a
href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></p></li>
<li><p>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></p></li>
<li><p>blog posts:</p>
<p><a
href="http://inverseprobability.com/2014/07/01/open-data-science">Open
Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Hotelling:analysis33" class="csl-entry" role="listitem">
Hotelling, H., 1933. Analysis of a complex of statistical variables into
principal components. Journal of Educational Psychology 24, 417–441.
</div>
<div id="ref-Tipping:probpca99" class="csl-entry" role="listitem">
Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component
analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a
href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a>
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // Turns fragments on and off globally
        fragments: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
