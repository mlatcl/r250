<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2021-01-21">
  <title>R250: Unsupervised Learning with Gaussian Processes II</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">R250: Unsupervised Learning with Gaussian Processes II</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2021-01-21</time></p>
  <p class="venue" style="text-align:center">Virtual (Zoom)</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
</section>
<section id="principal-component-analysis" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>PCA (<span class="citation" data-cites="Hotelling:analysis33">Hotelling (1933)</span>) is a linear embedding.</li>
<li>Today its presented as:
<ul>
<li>Rotate to find ‘directions’ in data with maximal variance.</li>
<li>How do we find these directions?</li>
</ul></li>
<li>Algorithmically we do this by diagonalizing the sample covariance matrix <span class="math display">\[
\mathbf{S}=\frac{1}{n}\sum_{i=1}^n\left(\mathbf{ y}_{i, :}-\boldsymbol{ \mu}\right)\left(\mathbf{ y}_{i, :} - \boldsymbol{ \mu}\right)^\top
\]</span></li>
</ul>
</section>
<section id="principal-component-analysis-1" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>Find directions in the data, <span class="math inline">\(\mathbf{ z}= \mathbf{U}\mathbf{ y}\)</span>, for which variance is maximized.</li>
</ul>
</section>
<section id="lagrangian" class="slide level2">
<h2>Lagrangian</h2>
<ul>
<li><p>Solution is found via constrained optimisation (which uses <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>): <span class="math display">\[
L\left(\mathbf{u}_{1},\lambda_{1}\right)=\mathbf{u}_{1}^{\top}\mathbf{S}\mathbf{u}_{1}+\lambda_{1}\left(1-\mathbf{u}_{1}^{\top}\mathbf{u}_{1}\right)
\]</span></p></li>
<li><p>Gradient with respect to <span class="math inline">\(\mathbf{u}_{1}\)</span> <span class="math display">\[\frac{\text{d}L\left(\mathbf{u}_{1},\lambda_{1}\right)}{\text{d}\mathbf{u}_{1}}=2\mathbf{S}\mathbf{u}_{1}-2\lambda_{1}\mathbf{u}_{1}\]</span> rearrange to form <span class="math display">\[\mathbf{S}\mathbf{u}_{1}=\lambda_{1}\mathbf{u}_{1}.\]</span> Which is known as an <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"><em>eigenvalue problem</em></a>.</p></li>
<li><p>Further directions that are <em>orthogonal</em> to the first can also be shown to be eigenvectors of the covariance.</p></li>
</ul>
</section>
<section id="linear-dimensionality-reduction" class="slide level2">
<h2>Linear Dimensionality Reduction</h2>
<ul>
<li>Represent data, <span class="math inline">\(\mathbf{Y}\)</span>, with a lower dimensional set of latent variables <span class="math inline">\(\mathbf{Z}\)</span>.</li>
<li>Assume a linear relationship of the form <span class="math display">\[
\mathbf{ y}_{i,:}=\mathbf{W}\mathbf{ z}_{i,:}+\boldsymbol{ \epsilon}_{i,:},
\]</span> where <span class="math display">\[
\boldsymbol{ \epsilon}_{i,:} \sim \mathcal{N}\left(\mathbf{0},\sigma^2\mathbf{I}\right)
\]</span></li>
</ul>
</section>
<section id="linear-latent-variable-model" class="slide level2">
<h2>Linear Latent Variable Model</h2>
<p><strong>Probabilistic PCA</strong></p>
<table>
<tr>
<td width>
<ul>
<li>Define <em>linear-Gaussian relationship</em> between latent variables and data.</li>
<li><strong>Standard</strong> Latent variable approach:
<ul>
<li>Define Gaussian prior over <em>latent space</em>, <span class="math inline">\(\mathbf{Z}\)</span>.</li>
</ul></li>
<li>Integrate out <em>latent variables</em>.
</td>
<td width="">
<div class="figure">
<div id="ppca-graph-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/ppca_graph.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Graphical model representing probabilistic PCA.
</aside>
<small> <span class="math display">\[
p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^2\mathbf{I}\right)
\]</span></li>
</ul>
<p><span class="math display">\[
p\left(\mathbf{Z}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ z}_{i,:}|\mathbf{0},\mathbf{I}\right)
\]</span></p>
<span class="math display">\[
p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{0},\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\right)
\]</span></small>
</td>
</tr>
</table>
</section>
<section id="robot-navigation-example" class="slide level2">
<h2>Robot Navigation Example</h2>
<ul>
<li>Example involving 215 observations of 30 access points.</li>
<li>Infer location of ‘robot’ and accesspoints.</li>
<li>This is known as SLAM (simulataneous localization and mapping).</li>
</ul>
<!-- SECTION Interpretations of Principal Component Analysis -->
</section>
<section id="interpretations-of-principal-component-analysis" class="slide level2">
<h2>Interpretations of Principal Component Analysis</h2>
</section>
<section id="relationship-to-matrix-factorization" class="slide level2">
<h2>Relationship to Matrix Factorization</h2>
<ul>
<li><p>PCA is closely related to matrix factorisation.</p></li>
<li><p>Instead of <span class="math inline">\(\mathbf{Z}\)</span>, <span class="math inline">\(\mathbf{W}\)</span></p></li>
<li><p>Define Users <span class="math inline">\(\mathbf{U}\)</span> and items <span class="math inline">\(\mathbf{V}\)</span></p></li>
<li><p>Matrix factorisation: <span class="math display">\[
f_{i, j} =
\mathbf{u}_{i, :}^\top \mathbf{v}_{j, :} 
\]</span> PCA: <span class="math display">\[
f_{i, j} = \mathbf{ z}_{i, :}^\top \mathbf{ w}_{j, :} 
\]</span></p></li>
</ul>
</section>
<section id="other-interpretations-of-pca-separating-model-and-algorithm" class="slide level2">
<h2>Other Interpretations of PCA: Separating Model and Algorithm</h2>
<ul>
<li>PCA introduced as latent variable model (a model).</li>
<li>Solution is through an eigenvalue problem (an algorithm).</li>
<li>This causes some confusion about what PCA is.</li>
</ul>
<p><span class="math display">\[
\mathbf{Y}= \mathbf{V} \boldsymbol{\Lambda} \mathbf{U}^\top
\]</span></p>
<p><span class="math display">\[
\mathbf{Y}^\top\mathbf{Y}=
\mathbf{U}\boldsymbol{\Lambda}\mathbf{V}^\top\mathbf{V} \boldsymbol{\Lambda}
\mathbf{U}^\top = \mathbf{U}\boldsymbol{\Lambda}^2 \mathbf{U}^\top
\]</span></p>
</section>
<section id="separating-model-and-algorithm" class="slide level2">
<h2>Separating Model and Algorithm</h2>
<ul>
<li>Separation between <em>model</em> and <em>algorithm</em> is helpful conceptually.</li>
<li>Even if in practice they conflate (e.g. deep neural networks).</li>
<li>Sometimes difficult to pull apart.</li>
<li>Helpful to revisit algorithms with modelling perspective in mind.
<ul>
<li>Probabilistic numerics</li>
</ul></li>
</ul>
<!-- SECTION PPCA Marginal Likelihood -->
</section>
<section id="ppca-marginal-likelihood" class="slide level2">
<h2>PPCA Marginal Likelihood</h2>
<p>We have developed the posterior density over the latent variables given the data and the parameters, and due to symmetries in the underlying prediction function, it has a very similar form to its sister density, the posterior of the weights given the data from Bayesian regression. Two key differences are as follows. If we were to do a Bayesian multiple output regression we would find that the marginal likelihood of the data is independent across the features and correlated across the data, <span class="math display">\[
p(\mathbf{Y}|\mathbf{Z})
= \prod_{j=1}^p \mathcal{N}\left(\mathbf{ y}_{:, j}|\mathbf{0},
\alpha\mathbf{Z}\mathbf{Z}^\top + \sigma^2 \mathbf{I}\right)
\]</span> where <span class="math inline">\(\mathbf{ y}_{:, j}\)</span> is a column of the data matrix and the independence is across the <em>features</em>, in probabilistic PCA the marginal likelihood has the form, <span class="math display">\[
p(\mathbf{Y}|\mathbf{W}) = \prod_{i=1}^n\mathcal{N}\left(\mathbf{ y}_{i,
:}|\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right)
\]</span> where <span class="math inline">\(\mathbf{ y}_{i, :}\)</span> is a row of the data matrix <span class="math inline">\(\mathbf{Y}\)</span> and the independence is across the data points.</p>
<!-- SECTION Computation of the Log Likelihood -->
</section>
<section id="computation-of-the-log-likelihood" class="slide level2">
<h2>Computation of the Log Likelihood</h2>
<p>The quality of the model can be assessed using the log likelihood of this Gaussian form. <span class="math display">\[
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2} \log \left|
\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right| -\frac{1}{2}
\sum_{i=1}^n\mathbf{ y}_{i, :}^\top \left(\mathbf{W}\mathbf{W}^\top + \sigma^2
\mathbf{I}\right)^{-1} \mathbf{ y}_{i, :} +\text{const}
\]</span> but this can be computed more rapidly by exploiting the low rank form of the covariance covariance, <span class="math inline">\(\mathbf{C}= \mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\)</span> and the fact that <span class="math inline">\(\mathbf{W}= \mathbf{U}\mathbf{L}\mathbf{R}^\top\)</span>. Specifically, we first use the decomposition of <span class="math inline">\(\mathbf{W}\)</span> to write: <span class="math display">\[
-\frac{n}{2} \log \left| \mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right|
= -\frac{n}{2} \sum_{i=1}^q \log (\ell_i^2 + \sigma^2) - \frac{n(p-q)}{2}\log
\sigma^2,
\]</span> where <span class="math inline">\(\ell_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\mathbf{L}\)</span>. Next, we use the <a href="http://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> which allows us to write the inverse as a quantity which contains another inverse in a smaller matrix: <span class="math display">\[
(\sigma^2 \mathbf{I}+ \mathbf{W}\mathbf{W}^\top)^{-1} =
\sigma^{-2}\mathbf{I}-\sigma^{-4}\mathbf{W}{\underbrace{(\mathbf{I}+\sigma^{-2}\mathbf{W}^\top\mathbf{W})}_{\mathbf{C}_x}}^{-1}\mathbf{W}^\top
\]</span> So, it turns out that the original inversion of the <span class="math inline">\(p \times p\)</span> matrix can be done by forming a quantity which contains the inversion of a <span class="math inline">\(q \times q\)</span> matrix which, moreover, turns out to be the quantity <span class="math inline">\(\mathbf{C}_x\)</span> of the posterior.</p>
<p>Now, we put everything together to obtain: <span class="math display">\[
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2} \sum_{i=1}^q
\log (\ell_i^2 + \sigma^2)
- \frac{n(p-q)}{2}\log \sigma^2 - \frac{1}{2} \text{tr}\left(\mathbf{Y}^\top \left(
\sigma^{-2}\mathbf{I}-\sigma^{-4}\mathbf{W}\mathbf{C}_x
\mathbf{W}^\top \right) \mathbf{Y}\right) + \text{const},
\]</span> where we used the fact that a scalar sum can be written as <span class="math inline">\(\sum_{i=1}^n\mathbf{ y}_{i,:}^\top \mathbf{K}\mathbf{ y}_{i,:} = \text{tr}\left(\mathbf{Y}^\top \mathbf{K}\mathbf{Y}\right)\)</span>, for any matrix <span class="math inline">\(\mathbf{K}\)</span> of appropriate dimensions. We now use the properties of the trace <span class="math inline">\(\text{tr}\left(\mathbf{A}+\mathbf{B}\right)=\text{tr}\left(\mathbf{A}\right)+\text{tr}\left(\mathbf{B}\right)\)</span> and <span class="math inline">\(\text{tr}\left(c \mathbf{A}\right) = c \text{tr}\left(\mathbf{A}\right)\)</span>, where <span class="math inline">\(c\)</span> is a scalar and <span class="math inline">\(\mathbf{A},\mathbf{B}\)</span> matrices of compatible sizes. Therefore, the final log likelihood takes the form: <span class="math display">\[
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2}
\sum_{i=1}^q \log (\ell_i^2 + \sigma^2) - \frac{n(p-q)}{2}\log \sigma^2 -
\frac{\sigma^{-2}}{2} \text{tr}\left(\mathbf{Y}^\top \mathbf{Y}\right)
+\frac{\sigma^{-4}}{2} \text{tr}\left(\mathbf{B}\mathbf{C}_x\mathbf{B}^\top\right) +
\text{const}
\]</span> where we also defined <span class="math inline">\(\mathbf{B}=\mathbf{Y}^\top\mathbf{W}\)</span>. Finally, notice that <span class="math inline">\(\text{tr}\left(\mathbf{Y}\mathbf{Y}^\top\right)=\text{tr}\left(\mathbf{Y}^\top\mathbf{Y}\right)\)</span> can be computed faster as the sum of all the elements of <span class="math inline">\(\mathbf{Y}\circ\mathbf{Y}\)</span>, where <span class="math inline">\(\circ\)</span> denotes the element-wise (or <a href="http://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard</a>) product.</p>
<!-- SECTION Non Linear Latent Variable Models -->
</section>
<section id="non-linear-latent-variable-models" class="slide level2">
<h2>Non Linear Latent Variable Models</h2>
</section>
<section id="difficulty-for-probabilistic-approaches" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A two dimensional grid mapped into three dimensions to form a two dimensional manifold.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-1" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A one dimensional line mapped into two dimensions by two separate independent functions. Each point can be mapped exactly through the mappings.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-2" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Gaussian density over the input of a non linear function leads to a very non Gaussian output. Here the output is multimodal.
</aside>
<!-- SECTION Dual Probabilistic PCA and GP-LVM -->
</section>
<section id="dual-probabilistic-pca-and-gp-lvm" class="slide level2">
<h2>Dual Probabilistic PCA and GP-LVM</h2>
</section>
<section id="dual-probabilistic-pca" class="slide level2">
<h2>Dual Probabilistic PCA</h2>
<p><strong>Probabilistic PCA</strong></p>
<ul>
<li>We have seen that PCA has a probabilistic interpretation <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>.</li>
<li>It is difficult to `non-linearise’ directly.</li>
<li>GTM and Density Networks are an attempt to do so.</li>
</ul>
<p><strong>Dual Probabilistic PCA</strong></p>
<ul>
<li>There is an alternative probabilistic interpretation of PCA <span class="citation" data-cites="Lawrence:pnpca05">(Lawrence, 2005)</span>.</li>
<li>This interpretation can be made non-linear.</li>
<li>The result is non-linear probabilistic PCA.</li>
</ul>
</section>
<section id="linear-latent-variable-model-iii" class="slide level2">
<h2>Linear Latent Variable Model III</h2>
<table>
<tr>
<td width="45%">
<p><strong>Dual Probabilistic PCA</strong></p>
<ul>
<li>Define <em>linear-Gaussian relationship</em> between latent variables and data.</li>
<li><strong>Novel</strong> Latent variable approach:</li>
<li>Define Gaussian prior over , <span class="math inline">\(\mathbf{W}\)</span>.</li>
<li>Integrate out <em>parameters</em>.
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/gplvm_graph.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<span class="math display">\[
p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^{2}\mathbf{I}\right)
\]</span> <span class="math display">\[
 p\left(\mathbf{W}\right)=\prod_{i=1}^{p}\mathcal{N}\left(\mathbf{ w}_{i,:}|\mathbf{0},\mathbf{I}\right)\]</span> <span class="math display">\[
 p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{p}\mathcal{N}\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\right)\]</span>
</td>
</tr>
</table></li>
</ul>
</section>
<section id="equivalence-of-formulations" class="slide level2">
<h2>Equivalence of Formulations</h2>
<p><strong>The Eigenvalue Problems are equivalent</strong></p>
<ul>
<li><p>Solution for Probabilistic PCA (solves for the mapping) <span class="math display">\[
\mathbf{Y}^{\top}\mathbf{Y}\mathbf{U}_{q}=\mathbf{U}_{q}\Lambda_{q}\quad\quad\quad\mathbf{W}=\mathbf{U}_{q}\mathbf{L}\mathbf{V}^{\top}
    \]</span></p></li>
<li><p>Solution for Dual Probabilistic PCA (solves for the latent positions) <span class="math display">\[
\mathbf{Y}\mathbf{Y}^{\top}\mathbf{U}_{q}^{\prime}=\mathbf{U}_{q}^{\prime}\Lambda_{q}\quad\quad\quad\mathbf{Z}=\mathbf{U}_{q}^{\prime}\mathbf{L}\mathbf{V}^{\top}
  \]</span></p></li>
<li><p>Equivalence is from <span class="math display">\[
\mathbf{U}_{q}=\mathbf{Y}^{\top}\mathbf{U}_{q}^{\prime}\Lambda_{q}^{-\frac{1}{2}}
  \]</span></p></li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Here we’re showing 20 samples taken from the prior over functions defined by our covarariance
</aside>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
We can sample many such functions, in this slide there are now 1000 in total. This is a sample from our prior over functions.
</aside>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Now we observe data. Here there are three data points. Conceptually in Bayesian inference we discard all samples that are distant from the data.
</aside>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Throwing away such samples we are left with our posterior. This is the collection of samples from the prior that are consistent with the data.
</aside>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
The elegance of the Gaussian process is that this result can be computed analytically using linear algebra.
</aside>
</section>
<section id="gaussian-process-gp" class="slide level2">
<h2>Gaussian Process (GP)</h2>
<p><strong>Prior for Functions</strong></p>
<ul>
<li><p>Probability Distribution over Functions</p></li>
<li><p>Functions are infinite dimensional.</p>
<ul>
<li>Prior distribution over <em>instantiations</em> of the function: finite dimensional objects.</li>
<li>Can prove by induction that GP is ‘consistent’.</li>
</ul></li>
</ul>
</section>
<section id="gaussian-process-gp-ii" class="slide level2">
<h2>Gaussian Process (GP) II</h2>
<ul>
<li><p>Mean and Covariance Functions</p></li>
<li><p>Instead of mean and covariance matrix, GP is defined by mean function and covariance function.</p>
<ul>
<li>Mean function often taken to be zero or constant.</li>
<li>Covariance function must be <em>positive definite</em>.</li>
<li>Class of valid covariance functions is the same as the class of <em>Mercer kernels</em>.</li>
</ul></li>
</ul>
</section>
<section id="gaussian-processes-iii" class="slide level2">
<h2>Gaussian Processes III</h2>
<p><strong>Zero mean Gaussian Process</strong></p>
<ul>
<li><p>A (zero mean) Gaussian process likelihood is of the form<span class="math display">\[
  p\left(\mathbf{ y}|\mathbf{Z}\right)=N\left(\mathbf{ y}|\mathbf{0},\mathbf{K}\right),\]</span> where <span class="math inline">\(\mathbf{K}\)</span> is the covariance function or .</p></li>
<li><p>The  with noise has the form<span class="math display">\[
  \mathbf{K}=\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\]</span></p></li>
<li><p>Priors over non-linear functions are also possible.</p>
<ul>
<li>To see what functions look like, we can sample from the prior process.</li>
</ul></li>
</ul>
</section>
<section id="gaussian-process-regression" class="slide level2">
<h2>Gaussian Process Regression</h2>
<p><strong>Posterior Distribution over Functions</strong></p>
<ul>
<li>Gaussian processes are often used for regression.</li>
<li>We are given a known inputs <span class="math inline">\(\mathbf{Z}\)</span> and targets <span class="math inline">\(\mathbf{Y}\)</span>.</li>
<li>We assume a prior distribution over functions by selecting a kernel.</li>
<li>Combine the prior with data to get a  distribution over functions.</li>
</ul>
</section>
<section id="exponentiated-quadratic-covariance" class="slide level2">
<h2>Exponentiated Quadratic Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha \exp\left(-\frac{\left\Vert \mathbf{ x}-\mathbf{ x}^\prime \right\Vert_2^2}{2\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The exponentiated quadratic covariance function.
</aside>
</section>
<section id="learning-covariance-parameters" class="slide level2">
<h2>Learning Covariance Parameters</h2>
<p>Can we determine covariance parameters from the data?</p>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\mathcal{N}\left(\mathbf{ y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}{\det{\mathbf{K}}^{\frac{1}{2}}}}{\exp\left(-\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\]</span></p>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{N}\left(\mathbf{ y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}\color{yellow}{\det{\mathbf{K}}^{\frac{1}{2}}}}\color{cyan}{\exp\left(-\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\end{aligned}
\]</span></p>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \log \mathcal{N}\left(\mathbf{ y}|\mathbf{0},\mathbf{K}\right)=&amp;\color{yellow}{-\frac{1}{2}\log\det{\mathbf{K}}}\color{cyan}{-\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}} \\ &amp;-\frac{n}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
E(\boldsymbol{ \theta}) = \color{yellow}{\frac{1}{2}\log\det{\mathbf{K}}} + \color{cyan}{\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}
\]</span></p>
</section>
<section id="capacity-control-through-the-determinant" class="slide level2">
<h2>Capacity Control through the Determinant</h2>
<p>The parameters are <em>inside</em> the covariance function (matrix).  <span class="math display">\[k_{i, j} = k(\mathbf{ x}_i, \mathbf{ x}_j; \boldsymbol{ \theta})\]</span></p>
</section>
<section id="eigendecomposition-of-covariance" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<p><span> <span class="math display">\[\mathbf{K}= \mathbf{R}\boldsymbol{ \Lambda}^2 \mathbf{R}^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\boldsymbol{ \Lambda}\)</span> represents distance on axes. <span class="math inline">\(\mathbf{R}\)</span> gives rotation.
</td>
</tr>
</table>
</section>
<section id="eigendecomposition-of-covariance-1" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<ul>
<li><span class="math inline">\(\boldsymbol{ \Lambda}\)</span> is <em>diagonal</em>, <span class="math inline">\(\mathbf{R}^\top\mathbf{R}= \mathbf{I}\)</span>.</li>
<li>Useful representation since <span class="math inline">\(\det{\mathbf{K}} = \det{\boldsymbol{ \Lambda}^2} = \det{\boldsymbol{ \Lambda}}^2\)</span>.</li>
</ul>
</section>
<section id="capacity-control-coloryellowlog-detmathbfk" class="slide level2">
<h2>Capacity control: <span class="math inline">\(\color{yellow}{\log \det{\mathbf{K}}}\)</span></h2>
<script>
showDivs(0, 'gp-optimise-determinant');
</script>
<p><small></small> <input id="range-gp-optimise-determinant" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise-determinant')" oninput="setDivs('gp-optimise-determinant')"> <button onclick="plusDivs(-1, 'gp-optimise-determinant')">❮</button> <button onclick="plusDivs(1, 'gp-optimise-determinant')">❯</button></p>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant000.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant001.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant002.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant003.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant004.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant005.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant006.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant007.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant008.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant009.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="data-fit-colorcyanfracmathbf-ytopmathbfk-1mathbf-y2" class="slide level2">
<h2>Data Fit: <span class="math inline">\(\color{cyan}{\frac{\mathbf{ y}^\top\mathbf{K}^{-1}\mathbf{ y}}{2}}\)</span></h2>
<script>
showDivs(0, 'gp-optimise-quadratic');
</script>
<p><small></small> <input id="range-gp-optimise-quadratic" type="range" min="0" max="2" value="0" onchange="setDivs('gp-optimise-quadratic')" oninput="setDivs('gp-optimise-quadratic')"> <button onclick="plusDivs(-1, 'gp-optimise-quadratic')">❮</button> <button onclick="plusDivs(1, 'gp-optimise-quadratic')">❯</button></p>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-quadratic000.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-quadratic001.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
<div class="figure">
<div id="gp-optimise-quadratic-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The data fit term of the Gaussian process is a quadratic loss centered around zero. This has eliptical contours, the principal axes of which are given by the covariance matrix.
</aside>
</section>
<section id="eboldsymbol-theta-coloryellowfrac12logdetmathbfkcolorcyanfracmathbf-ytopmathbfk-1mathbf-y2" class="slide level2">
<h2><span class="math display">\[E(\boldsymbol{ \theta}) = \color{yellow}{\frac{1}{2}\log\det{\mathbf{K}}}+\color{cyan}{\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}\]</span></h2>
</section>
<section id="quadratic-data-fit" class="slide level2">
<h2>Quadratic Data Fit</h2>
</section>
<section id="data-fit-term" class="slide level2">
<h2>Data Fit Term</h2>
<script>
showDivs(0, 'gp-optimise');
</script>
<p><small></small> <input id="range-gp-optimise" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise')" oninput="setDivs('gp-optimise')"> <button onclick="plusDivs(-1, 'gp-optimise')">❮</button> <button onclick="plusDivs(1, 'gp-optimise')">❯</button></p>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise000.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise001.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise002.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise003.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise004.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise005.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise007.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise008.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise009.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise011.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise012.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise013.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise014.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise015.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise017.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise018.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise019.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise020.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="non-linear-latent-variable-model" class="slide level2">
<h2>Non-Linear Latent Variable Model</h2>
<table>
<tr>
<td width="45%">
<p><small><strong>Dual Probabilistic PCA</strong> * Define <em>linear-Gaussian relationship</em> between latent variables and data. * <strong>Novel</strong> Latent variable approach: * Define Gaussian prior over <em>parameteters</em>, <span class="math inline">\(\mathbf{W}\)</span>. * Integrate out <em>parameters</em>.</p>
<ul>
<li>Inspection of the marginal likelihood shows …
<ul>
<li>The covariance matrix is a covariance function.</li>
<li>We recognise it as the ‘linear kernel’. </small>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/gplvm_graph.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>
<div class="fragment" data-fragment-index="1" style="">
<span class="math display">\[
 p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}N\left(\mathbf{ y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^{2}\mathbf{I}\right)\]</span> <span class="math display">\[
 p\left(\mathbf{W}\right)=\prod_{i=1}^{d}N\left(\mathbf{ w}_{i,:}|\mathbf{0},\mathbf{I}\right)\]</span>
</div>
<span class="math display">\[
 p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{d}N\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\right)\]</span> <span class="math display">\[
 p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{d}N\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{K}\right)\]</span> <span class="math display">\[
\mathbf{K}=\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\]</span> This is a product of Gaussian processes with linear kernels. <span class="math display">\[
\mathbf{K}=?
\]</span> Replace linear kernel with non-linear kernel for non-linear model.</small>
</td>
</tr>
</table></li>
</ul></li>
</ul>
</section>
<section id="non-linear-latent-variable-model-1" class="slide level2">
<h2>Non-Linear Latent Variable Model</h2>
<p><strong>EQ Kernel</strong></p>
<ul>
<li><p>The RBF kernel has the form <span class="math inline">\(k_{i,j}=k\left(\mathbf{ z}_{i,:},\mathbf{ z}_{j,:}\right),\)</span> where <span class="math display">\[
k\left(\mathbf{ z}_{i,:},\mathbf{ z}_{j,:}\right)=\alpha\exp\left(-\frac{\left(\mathbf{ z}_{i,:}-\mathbf{ z}_{j,:}\right)^{\top}\left(\mathbf{ z}_{i,:}-\mathbf{ z}_{j,:}\right)}{2\ell^{2}}\right).
\]</span></p></li>
<li><p>No longer possible to optimise wrt <span class="math inline">\(\mathbf{Z}\)</span> via an eigenvalue problem.</p></li>
<li><p>Instead find gradients with respect to <span class="math inline">\(\mathbf{Z},\alpha,\ell\)</span> and <span class="math inline">\(\sigma^{2}\)</span> and optimise using gradient methods.</p></li>
</ul>
</section>
<section id="oil-data" class="slide level2">
<h2>Oil Data</h2>
</section>
<section id="stick-man-data" class="slide level2">
<h2>Stick Man Data</h2>
</section>
<section id="applications" class="slide level2">
<h2>Applications</h2>
<ul>
<li>Style based inverse kinematics <span class="citation" data-cites="Grochow:styleik04">(Grochow et al., 2004)</span>.</li>
<li>Prior distributions for tracking <span class="citation" data-cites="Urtasun:3dpeople06">(Urtasun et al., 2006, p. Urtasun:priors05)</span>.</li>
<li>Assisted drawing <span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo, 2006)</span>.</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>GPLVM based on a dual probabilistic interpretation of PCA.</li>
<li>Straightforward to non-linearise it using Gaussian processes.</li>
<li>Result is a non-linear probabilistic PCA.</li>
<li><em>Optimise latent variables</em> rather than integrate them out.</li>
</ul>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="getting-started-and-downloading-data" class="slide level2">
<h2>Getting Started and Downloading Data</h2>
</section>
<section id="principal-component-analysis-2" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>What is the right shape <span class="math inline">\(n\times p\)</span> to use?</li>
</ul>
</section>
<section id="gaussian-process-latent-variable-model" class="slide level2">
<h2>Gaussian Process Latent Variable Model</h2>

</section>
<section id="cmu-mocap-database" class="slide level2">
<h2>CMU Mocap Database</h2>
</section>
<section id="example-latent-doodle-space" class="slide level2">
<h2>Example: Latent Doodle Space</h2>
<div class="figure">
<div id="latent-doodle-space-figure" class="figure-frame">
<iframe width height src="https://player.vimeo.com/video/3235882#t=" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
The latent doodle space idea of <span class="citation" data-cites="Baxter:doodle06">Baxter and Anjyo (2006)</span> manages to build a smooth mapping across very sparse data.
</aside>
</section>
<section id="example-latent-doodle-space-1" class="slide level2">
<h2>Example: Latent Doodle Space</h2>
<p><strong>Generalization with much less Data than Dimensions</strong></p>
<ul>
<li><p>Powerful uncertainly handling of GPs leads to surprising properties.</p></li>
<li><p>Non-linear models can be used where there are fewer data points than dimensions <em>without overfitting</em>.</p></li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo, 2006)</span></span></p>
</section>
<section id="example-continuous-character-control" class="slide level2">
<h2>Example: Continuous Character Control</h2>
<ul>
<li>Graph diffusion prior for enforcing connectivity between motions. <span class="math display">\[\log p(\mathbf{X}) = w_c \sum_{i,j} \log K_{ij}^d\]</span> with the graph diffusion kernel <span class="math inline">\(\mathbf{K}^d\)</span> obtain from <span class="math display">\[K_{ij}^d = \exp(\beta \mathbf{H})
\qquad \text{with} \qquad \mathbf{H} = -\mathbf{T}^{-1/2} \mathbf{L} \mathbf{T}^{-1/2}\]</span> the graph Laplacian, and <span class="math inline">\(\mathbf{T}\)</span> is a diagonal matrix with <span class="math inline">\(T_{ii} = \sum_j w(\mathbf{ x}_i, \mathbf{ x}_j)\)</span>, <span class="math display">\[L_{ij} = \begin{cases} \sum_k w(\mathbf{ x}_i,\mathbf{ x}_k) &amp; \text{if $i=j$}
\\
-w(\mathbf{ x}_i,\mathbf{ x}_j) &amp;\text{otherwise.}
\end{cases}\]</span> and <span class="math inline">\(w(\mathbf{ x}_i,\mathbf{ x}_j) = || \mathbf{ x}_i - \mathbf{ x}_j||^{-p}\)</span> measures similarity.</li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span></span></p>
</section>
<section id="character-control-results" class="slide level2">
<h2>Character Control: Results</h2>
<div class="figure">
<div id="charcter-control-gplvm-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/hr3pdDl5IAg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Character control in the latent space described the the GP-LVM <span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span>.
</aside>
</section>
<section id="data-for-blastocyst-development-in-mice-single-cell-taqman-arrays" class="slide level2">
<h2>Data for Blastocyst Development in Mice: Single Cell TaqMan Arrays</h2>
</section>
<section id="principal-component-analysis-3" class="slide level2">
<h2>Principal Component Analysis</h2>
</section>
<section id="pca-result" class="slide level2">
<h2>PCA Result</h2>
<div class="figure">
<div id="singlecell-data-pca-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/singlecell-data-pca.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
First two principal compoents of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data.
</aside>
</section>
<section id="gp-lvm-on-the-data" class="slide level2">
<h2>GP-LVM on the Data</h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Max Zwiessele
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/max-zwiessele.jpg" clip-path="url(#clip0)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Oliver Stegle
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/oliver-stegle.jpg" clip-path="url(#clip1)"/>
</svg>
</div>
<div class="figure">
<div id="singlecell-gplvm-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/singlecell-gplvm.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with the GP-LVM.
</aside>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="figure">
<div id="singlecell-gplvm-ard-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/singlecell-gplvm-ard.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The ARD parameters of the GP-LVM for the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data.
</aside>
</section>
<section id="blastocyst-data-isomap" class="slide level2">
<h2>Blastocyst Data: Isomap</h2>
<div class="figure">
<div id="singlecell-isomap-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/singlecell-isomap.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with Isomap.
</aside>
</section>
<section id="blastocyst-data-locally-linear-embedding" class="slide level2">
<h2>Blastocyst Data: Locally Linear Embedding</h2>
<div class="figure">
<div id="singlecell-lle-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/singlecell-lle.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with a locally linear embedding.
</aside>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Baxter:doodle06">
<p>Baxter, W.V., Anjyo, K.-I., 2006. Latent doodle space, in: EUROGRAPHICS. Vienna, Austria, pp. 477–485. <a href="https://doi.org/10.1111/j.1467-8659.2006.00967.x">https://doi.org/10.1111/j.1467-8659.2006.00967.x</a></p>
</div>
<div id="ref-Grochow:styleik04">
<p>Grochow, K., Martin, S.L., Hertzmann, A., Popovic, Z., 2004. Style-based inverse kinematics, in: ACM Transactions on Graphics (Siggraph 2004). pp. 522–531. <a href="https://doi.org/10.1145/1186562.1015755">https://doi.org/10.1145/1186562.1015755</a></p>
</div>
<div id="ref-Guo:fate10">
<p>Guo, G., Huss, M., Tong, G.Q., Wang, C., Sun, L.L., Clarke, N.D., Robsonemail, P., 2010. Resolution of cell fate decisions revealed by single-cell gene expression analysis from zygote to blastocyst. Developmental Cell 18, 675–685. <a href="https://doi.org/10.1016/j.devcel.2010.02.012">https://doi.org/10.1016/j.devcel.2010.02.012</a></p>
</div>
<div id="ref-Hotelling:analysis33">
<p>Hotelling, H., 1933. Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology 24, 417–441.</p>
</div>
<div id="ref-Lawrence:pnpca05">
<p>Lawrence, N.D., 2005. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of Machine Learning Research 6, 1783–1816.</p>
</div>
<div id="ref-Levine:control12">
<p>Levine, S., Wang, J.M., Haraux, A., Popović, Z., Koltun, V., 2012. Continuous character control with low-dimensional embeddings. ACM Transactions on Graphics (SIGGRAPH 2012) 31.</p>
</div>
<div id="ref-Tipping:probpca99">
<p>Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a></p>
</div>
<div id="ref-Urtasun:3dpeople06">
<p>Urtasun, R., Fleet, D.J., Fua, P., 2006. 3D people tracking with Gaussian process dynamical models, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Press, New York, U.S.A., pp. 238–245. <a href="https://doi.org/10.1109/CVPR.2006.15">https://doi.org/10.1109/CVPR.2006.15</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
