<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2021-01-21">
  <title>R250: Unsupervised Learning with Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">R250: Unsupervised Learning with Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2021-01-21</time></p>
  <p class="venue" style="text-align:center">Virtual (Zoom)</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
</section>
<section id="high-dimensional-data" class="slide level2">
<h2>High Dimensional Data</h2>
<ul>
<li>USPS Data Set Handwritten Digit</li>
<li>3648 dimensions (64 rows, 57 columns)</li>
<li>Space contains much more than just this digit.</li>
</ul>
</section>
<section id="usps-samples" class="slide level2">
<h2>USPS Samples</h2>
<script>
showDivs(0, 'dem-six-sample');
</script>
<p><small></small> <input id="range-dem-six-sample" type="range" min="0" max="4" value="0" onchange="setDivs('dem-six-sample')" oninput="setDivs('dem-six-sample')"> <button onclick="plusDivs(-1, 'dem-six-sample')">❮</button> <button onclick="plusDivs(1, 'dem-six-sample')">❯</button></p>
<div class="dem-six-sample" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six000.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-sample" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six001.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-sample" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six002.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-sample" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six003.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<ul>
<li>Even if we sample every nanonsecond from now until end of universe you won’t see original six!</li>
</ul>
</section>
<section id="simple-model-of-digit" class="slide level2">
<h2>Simple Model of Digit</h2>
<ul>
<li>Rotate a prototype</li>
</ul>
<script>
showDivs(1, 'dem-six-rotate');
</script>
<p><small></small> <input id="range-dem-six-rotate" type="range" min="1" max="6" value="1" onchange="setDivs('dem-six-rotate')" oninput="setDivs('dem-six-rotate')"> <button onclick="plusDivs(-1, 'dem-six-rotate')">❮</button> <button onclick="plusDivs(1, 'dem-six-rotate')">❯</button></p>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six_rotate001.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six_rotate002.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six_rotate003.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six_rotate004.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six_rotate005.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div class="dem-six-rotate" style="text-align:center;">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_six_rotate006.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="dem-six-mainfold-print-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_manifold_print001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/dem_manifold_print002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The rotated sixes projected onto the first two principal components of the ‘rotated data set’. The data lives on a one dimensional manifold in the 3,648 dimensional space.
</aside>
</section>
<section id="low-dimensional-manifolds" class="slide level2">
<h2>Low Dimensional Manifolds</h2>
<ul>
<li>Pure rotation is too simple
<ul>
<li>In practice data may undergo several distortions.</li>
</ul></li>
<li>For high dimensional data with <em>structure</em>:
<ul>
<li>We expect fewer distortions than dimensions;</li>
<li>Therefore we expect the data to live on a lower dimensional manifold.</li>
<li>Conclusion: Deal with high dimensional data by looking for a lower dimensional non-linear embedding.</li>
</ul></li>
</ul>
</section>
<section id="dimensionality-reduction" class="slide level2">
<h2>Dimensionality Reduction</h2>
<ul>
<li>Compress the data by replacing the original data with reduced number of continuous variables.</li>
</ul>
<div class="figure">
<div id="marionette-figure" class="figure-frame">
<object class data="../slides/diagrams/ml/marionette.svg" width="40%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Thinking of dimensionality reduction as a marionette. We observe the high dimensional pose of the puppet, <span class="math inline">\(\mathbf{ x}\)</span>, but the movement of the puppeteer’s hand, <span class="math inline">\(\mathbf{ z}\)</span> remains hidden to us. Dimensionality reduction aims to recover those hidden movements which generated the observations.
</aside>
</section>
<section id="dimensionality-reduction-1" class="slide level2">
<h2>Dimensionality Reduction</h2>
<ul>
<li>Position of each body part of a marionette could be thought of as our data, <span class="math inline">\(\mathbf{ x}_i\)</span>.</li>
<li>Each data point is the 3-D co-ordinates of all the different body parts</li>
<li>Movement of parts determined by puppeteer via strings.</li>
<li>For a simple puppet with one stick can move the stick up and down, left and right and twist.</li>
</ul>
</section>
<section id="dimensionality-reduction-2" class="slide level2">
<h2>Dimensionality Reduction</h2>
<ul>
<li>This gives three parameters in the puppeteers control.</li>
<li>Implies that the puppet we see moving is controlled by only 3 variables.</li>
<li>These 3 variables are often called the hidden or <em>latent variables</em>.</li>
<li>Assume similar for real world data, observations are derived from lower dimensional underlying process</li>
</ul>
</section>
<section id="examples-in-social-sciences" class="slide level2">
<h2>Examples in Social Sciences</h2>
<ul>
<li>Underpins <em>psychological scoring</em> such as <em>IQ</em> or <em>personality tests</em></li>
<li>Myers-Briggs assumes personality is four dimensional.</li>
<li>Political belief (left/right wing).</li>
<li>Also language modelling has taken similar approaches: <a href="https://arxiv.org/abs/1301.3781">word2vec</a></li>
</ul>
</section>
<section id="gaussian-variables-and-linear-dimensionality-reduction" class="slide level2">
<h2>Gaussian Variables and Linear Dimensionality Reduction</h2>
<ul>
<li>Return to non-linear shortly.</li>
<li>Now: Linear dimensionality reduction.</li>
<li>First: Review Gaussian density properties.</li>
</ul>
</section>
<section id="two-important-gaussian-properties" class="slide level2">
<h2>Two Important Gaussian Properties</h2>
</section>
<section id="sum-of-gaussians" class="slide level2">
<h2>Sum of Gaussians</h2>
<div class="fragment">
<p><span style="text-align:left">Sum of Gaussian variables is also Gaussian.</span></p>
<p><span class="math display">\[y_i \sim \mathcal{N}\left(\mu_i,\sigma_i^2\right)\]</span></p>
</div>
<div class="fragment">
<p><span style="text-align:left">And the sum is distributed as</span></p>
<p><span class="math display">\[
\sum_{i=1}^{n} y_i \sim \mathcal{N}\left(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2\right)
\]</span></p>
</div>
<div class="fragment">
<p><small>(<em>Aside</em>: As sum increases, sum of non-Gaussian, finite variance variables is also Gaussian because of <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>.)</small></p>
</div>
</section>
<section id="scaling-a-gaussian" class="slide level2">
<h2>Scaling a Gaussian</h2>
<div class="fragment">
<p><span style="text-align:left">Scaling a Gaussian leads to a Gaussian.</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[y\sim \mathcal{N}\left(\mu,\sigma^2\right)\]</span></p>
</div>
<div class="fragment">
<p><span style="text-align:left">And the scaled variable is distributed as</span></p>
<p><span class="math display">\[wy\sim \mathcal{N}\left(w\mu,w^2 \sigma^2\right).\]</span></p>
</div>
</section>
<section id="multivariate-gaussian-properties" class="slide level2">
<h2>Multivariate Gaussian Properties</h2>
<ul>
<li><p>If <span class="math display">\[
\mathbf{ y}= \mathbf{W}\mathbf{ x}+ \boldsymbol{ \epsilon},
\]</span></p></li>
<li><p>Assume <span class="math display">\[
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{ \mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim \mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
\]</span></p></li>
<li><p>Then <span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{ \mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{ \Sigma}\right).
\]</span> If <span class="math inline">\(\boldsymbol{ \Sigma}=\sigma^2\mathbf{I}\)</span>, this is Probabilistic PCA <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>.</p></li>
</ul>
<!-- SECTION Latent Variables -->
</section>
<section id="latent-variables" class="slide level2">
<h2>Latent Variables</h2>
<!-- SECTION Your Personality -->
</section>
<section id="your-personality" class="slide level2">
<h2>Your Personality</h2>
</section>
<section id="factor-analysis-model" class="slide level2">
<h2>Factor Analysis Model</h2>
<p><span class="math display">\[
\mathbf{ y}= \mathbf{f}(\mathbf{ z}) + \boldsymbol{ \epsilon},
\]</span></p>
<p><span class="math display">\[
\mathbf{f}(\mathbf{ z}) = \mathbf{W}\mathbf{ z}
\]</span></p>
</section>
<section id="closely-related-to-linear-regression" class="slide level2">
<h2>Closely Related to Linear Regression</h2>
<p><span class="math display">\[
\mathbf{f}(\mathbf{ z}) =
\begin{bmatrix} f_1(\mathbf{ z}) \\ f_2(\mathbf{ z}) \\ \vdots \\
f_p(\mathbf{ z})\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
f_j(\mathbf{ z}) =
\mathbf{ w}_{j, :}^\top \mathbf{ z},
\]</span></p>
<p><span class="math display">\[
\epsilon_j \sim \mathcal{N}\left(0,\sigma^2_j\right).
\]</span></p>
</section>
<section id="data-representation" class="slide level2">
<h2>Data Representation</h2>
<p><span class="math display">\[
\mathbf{Y}
= \begin{bmatrix} \mathbf{ y}_{1, :}^\top \\ \mathbf{ y}_{2, :}^\top \\ \vdots \\
\mathbf{ y}_{n, :}^\top\end{bmatrix},
\]</span></p>
<p><span class="math display">\[
\mathbf{F} = \mathbf{Z}\mathbf{W}^\top,
\]</span></p>
</section>
<section id="exercise-1" class="slide level2">
<h2>Exercise 1</h2>
<p>Show that, given all the definitions above, if, <span class="math display">\[
\mathbf{F} = \mathbf{Z}\mathbf{W}^\top
\]</span> and the elements of the vector valued function <span class="math inline">\(\mathbf{F}\)</span> are given by <span class="math display">\[
f_{i, j} = f_j(\mathbf{ z}_{i, :}),
\]</span> where <span class="math inline">\(\mathbf{ z}_{i, :}\)</span> is the <span class="math inline">\(i\)</span>th row of the latent variables, <span class="math inline">\(\mathbf{Z}\)</span>, then show that <span class="math display">\[
f_j(\mathbf{ z}_{i, :}) = \mathbf{ w}_{j, :}^\top
\mathbf{ z}_{i, :}
\]</span></p>
</section>
<section id="latent-variables-vs-linear-regression" class="slide level2">
<h2>Latent Variables vs Linear Regression</h2>
<p><span class="math display">\[
x_{i,j} \sim
\mathcal{N}\left(0,1\right),
\]</span> and we can write the density governing the latent variable associated with a single point as, <span class="math display">\[
\mathbf{ z}_{i, :} \sim \mathcal{N}\left(\mathbf{0},\mathbf{I}\right).
\]</span></p>
<p><span class="math display">\[
\mathbf{f}_{i, :} =
\mathbf{f}(\mathbf{ z}_{i, :}) = \mathbf{W}\mathbf{ z}_{i, :} 
\]</span></p>
<p><span class="math display">\[
\mathbf{f}_{i, :} \sim \mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top\right)
\]</span></p>
</section>
<section id="data-distribution" class="slide level2">
<h2>Data Distribution</h2>
<p><span class="math display">\[
\mathbf{ y}_{i, :} = \sim \mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top + \boldsymbol{\Sigma}\right)
\]</span></p>
<p><span class="math display">\[
\boldsymbol{\Sigma} = \begin{bmatrix}\sigma^2_{1} &amp; 0 &amp; 0 &amp; 0\\
0 &amp; \sigma^2_{2} &amp; 0 &amp; 0\\
                                     0 &amp; 0 &amp; \ddots &amp;
0\\
                                     0 &amp; 0 &amp; 0 &amp; \sigma^2_p\end{bmatrix}.
\]</span></p>
</section>
<section id="mean-vector" class="slide level2">
<h2>Mean Vector</h2>
<p><span class="math display">\[
\mathbf{ y}_{i, :} = \mathbf{W}\mathbf{ z}_{i, :} +
\boldsymbol{ \mu}+ \boldsymbol{ \epsilon}_{i, :}
\]</span></p>
<p><span class="math display">\[
\boldsymbol{ \mu}= \frac{1}{n} \sum_{i=1}^n
\mathbf{ y}_{i, :},
\]</span> <span class="math inline">\(\mathbf{C}= \mathbf{W}\mathbf{W}^\top + \boldsymbol{\Sigma}\)</span></p>
<!-- SECTION Principal Component Analysis -->
</section>
<section id="principal-component-analysis" class="slide level2">
<h2>Principal Component Analysis</h2>
<p><span class="citation" data-cites="Hotelling:analysis33">Hotelling (1933)</span> took <span class="math inline">\(\sigma^2_i \rightarrow 0\)</span> so <span class="math display">\[
\mathbf{ y}_{i, :} \sim \lim_{\sigma^2 \rightarrow 0} \mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right).
\]</span></p>
</section>
<section id="degenerate-covariance" class="slide level2">
<h2>Degenerate Covariance</h2>
<p><small> <span class="math display">\[
p(\mathbf{ y}_{i, :}|\mathbf{W}) =
\lim_{\sigma^2 \rightarrow 0} \frac{1}{(2\pi)^\frac{p}{2}
|\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}|^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}\mathbf{ y}_{i, :}\left[\mathbf{W}\mathbf{W}^\top+ \sigma^2
\mathbf{I}\right]^{-1}\mathbf{ y}_{i, :}\right),
\]</span></small></p>
</section>
<section id="computation-of-the-marginal-likelihood" class="slide level2">
<h2>Computation of the Marginal Likelihood</h2>
<p><span class="math display">\[
\mathbf{ y}_{i,:}=\mathbf{W}\mathbf{ z}_{i,:}+\boldsymbol{ \epsilon}_{i,:},\quad \mathbf{ z}_{i,:} \sim \mathcal{N}\left(\mathbf{0},\mathbf{I}\right), \quad \boldsymbol{ \epsilon}_{i,:} \sim \mathcal{N}\left(\mathbf{0},\sigma^{2}\mathbf{I}\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{W}\mathbf{ z}_{i,:} \sim \mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{W}\mathbf{ z}_{i, :} + \boldsymbol{ \epsilon}_{i, :} \sim \mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right)
\]</span></p>
</section>
<section id="linear-latent-variable-model-ii" class="slide level2">
<h2>Linear Latent Variable Model II</h2>
<p><strong>Probabilistic PCA Max. Likelihood Soln</strong> (<span class="citation" data-cites="Tipping:probpca99">Tipping and Bishop (1999)</span>)</p>
<div class="figure">
<div id="ppca-graph-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/ppca_graph.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Graphical model representing probabilistic PCA.
</aside>
<p><span class="math display">\[p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i, :}|\mathbf{0},\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\right)\]</span></p>
</section>
<section id="linear-latent-variable-model-ii-1" class="slide level2">
<h2>Linear Latent Variable Model II</h2>
<p><strong>Probabilistic PCA Max. Likelihood Soln</strong> (<span class="citation" data-cites="Tipping:probpca99">Tipping and Bishop (1999)</span>) <span class="math display">\[
  p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{0},\mathbf{C}\right),\quad \mathbf{C}=\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}
  \]</span> <span class="math display">\[
  \log p\left(\mathbf{Y}|\mathbf{W}\right)=-\frac{n}{2}\log\left|\mathbf{C}\right|-\frac{1}{2}\text{tr}\left(\mathbf{C}^{-1}\mathbf{Y}^{\top}\mathbf{Y}\right)+\text{const.}
  \]</span> If <span class="math inline">\(\mathbf{U}_{q}\)</span> are first <span class="math inline">\(q\)</span> principal eigenvectors of <span class="math inline">\(n^{-1}\mathbf{Y}^{\top}\mathbf{Y}\)</span> and the corresponding eigenvalues are <span class="math inline">\(\boldsymbol{\Lambda}_{q}\)</span>, <span class="math display">\[
  \mathbf{W}=\mathbf{U}_{q}\mathbf{L}\mathbf{R}^{\top},\quad\mathbf{L}=\left(\boldsymbol{\Lambda}_{q}-\sigma^{2}\mathbf{I}\right)^{\frac{1}{2}}
  \]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an arbitrary rotation matrix.</p>
</section>
<section id="principal-component-analysis-1" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>PCA (<span class="citation" data-cites="Hotelling:analysis33">Hotelling (1933)</span>) is a linear embedding.</li>
<li>Today its presented as:
<ul>
<li>Rotate to find ‘directions’ in data with maximal variance.</li>
<li>How do we find these directions?</li>
</ul></li>
<li>Algorithmically we do this by diagonalizing the sample covariance matrix <span class="math display">\[
\mathbf{S}=\frac{1}{n}\sum_{i=1}^n\left(\mathbf{ y}_{i, :}-\boldsymbol{ \mu}\right)\left(\mathbf{ y}_{i, :} - \boldsymbol{ \mu}\right)^\top
\]</span></li>
</ul>
</section>
<section id="principal-component-analysis-2" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>Find directions in the data, <span class="math inline">\(\mathbf{ z}= \mathbf{U}\mathbf{ y}\)</span>, for which variance is maximized.</li>
</ul>
</section>
<section id="lagrangian" class="slide level2">
<h2>Lagrangian</h2>
<ul>
<li><p>Solution is found via constrained optimisation (which uses <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>): <span class="math display">\[
L\left(\mathbf{u}_{1},\lambda_{1}\right)=\mathbf{u}_{1}^{\top}\mathbf{S}\mathbf{u}_{1}+\lambda_{1}\left(1-\mathbf{u}_{1}^{\top}\mathbf{u}_{1}\right)
\]</span></p></li>
<li><p>Gradient with respect to <span class="math inline">\(\mathbf{u}_{1}\)</span> <span class="math display">\[\frac{\text{d}L\left(\mathbf{u}_{1},\lambda_{1}\right)}{\text{d}\mathbf{u}_{1}}=2\mathbf{S}\mathbf{u}_{1}-2\lambda_{1}\mathbf{u}_{1}\]</span> rearrange to form <span class="math display">\[\mathbf{S}\mathbf{u}_{1}=\lambda_{1}\mathbf{u}_{1}.\]</span> Which is known as an <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"><em>eigenvalue problem</em></a>.</p></li>
<li><p>Further directions that are <em>orthogonal</em> to the first can also be shown to be eigenvectors of the covariance.</p></li>
</ul>
</section>
<section id="linear-dimensionality-reduction" class="slide level2">
<h2>Linear Dimensionality Reduction</h2>
<ul>
<li>Represent data, <span class="math inline">\(\mathbf{Y}\)</span>, with a lower dimensional set of latent variables <span class="math inline">\(\mathbf{Z}\)</span>.</li>
<li>Assume a linear relationship of the form <span class="math display">\[
\mathbf{ y}_{i,:}=\mathbf{W}\mathbf{ z}_{i,:}+\boldsymbol{ \epsilon}_{i,:},
\]</span> where <span class="math display">\[
\boldsymbol{ \epsilon}_{i,:} \sim \mathcal{N}\left(\mathbf{0},\sigma^2\mathbf{I}\right)
\]</span></li>
</ul>
</section>
<section id="linear-latent-variable-model" class="slide level2">
<h2>Linear Latent Variable Model</h2>
<p><strong>Probabilistic PCA</strong></p>
<table>
<tr>
<td width>
<ul>
<li>Define <em>linear-Gaussian relationship</em> between latent variables and data.</li>
<li><strong>Standard</strong> Latent variable approach:
<ul>
<li>Define Gaussian prior over <em>latent space</em>, <span class="math inline">\(\mathbf{Z}\)</span>.</li>
</ul></li>
<li>Integrate out <em>latent variables</em>.
</td>
<td width="">
<div class="figure">
<div id="ppca-graph-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/ppca_graph.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Graphical model representing probabilistic PCA.
</aside>
<small> <span class="math display">\[
p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^2\mathbf{I}\right)
\]</span></li>
</ul>
<p><span class="math display">\[
p\left(\mathbf{Z}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ z}_{i,:}|\mathbf{0},\mathbf{I}\right)
\]</span></p>
<span class="math display">\[
p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{0},\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\right)
\]</span></small>
</td>
</tr>
</table>
</section>
<section id="robot-navigation-example" class="slide level2">
<h2>Robot Navigation Example</h2>
<ul>
<li>Example involving 215 observations of 30 access points.</li>
<li>Infer location of ‘robot’ and accesspoints.</li>
<li>This is known as SLAM (simulataneous localization and mapping).</li>
</ul>
<!-- SECTION Interpretations of Principal Component Analysis -->
</section>
<section id="interpretations-of-principal-component-analysis" class="slide level2">
<h2>Interpretations of Principal Component Analysis</h2>
</section>
<section id="relationship-to-matrix-factorization" class="slide level2">
<h2>Relationship to Matrix Factorization</h2>
<ul>
<li><p>PCA is closely related to matrix factorisation.</p></li>
<li><p>Instead of <span class="math inline">\(\mathbf{Z}\)</span>, <span class="math inline">\(\mathbf{W}\)</span></p></li>
<li><p>Define Users <span class="math inline">\(\mathbf{U}\)</span> and items <span class="math inline">\(\mathbf{V}\)</span></p></li>
<li><p>Matrix factorisation: <span class="math display">\[
f_{i, j} =
\mathbf{u}_{i, :}^\top \mathbf{v}_{j, :} 
\]</span> PCA: <span class="math display">\[
f_{i, j} = \mathbf{ z}_{i, :}^\top \mathbf{ w}_{j, :} 
\]</span></p></li>
</ul>
</section>
<section id="other-interpretations-of-pca-separating-model-and-algorithm" class="slide level2">
<h2>Other Interpretations of PCA: Separating Model and Algorithm</h2>
<ul>
<li>PCA introduced as latent variable model (a model).</li>
<li>Solution is through an eigenvalue problem (an algorithm).</li>
<li>This causes some confusion about what PCA is.</li>
</ul>
<p><span class="math display">\[
\mathbf{Y}= \mathbf{V} \boldsymbol{\Lambda} \mathbf{U}^\top
\]</span></p>
<p><span class="math display">\[
\mathbf{Y}^\top\mathbf{Y}=
\mathbf{U}\boldsymbol{\Lambda}\mathbf{V}^\top\mathbf{V} \boldsymbol{\Lambda}
\mathbf{U}^\top = \mathbf{U}\boldsymbol{\Lambda}^2 \mathbf{U}^\top
\]</span></p>
</section>
<section id="separating-model-and-algorithm" class="slide level2">
<h2>Separating Model and Algorithm</h2>
<ul>
<li>Separation between <em>model</em> and <em>algorithm</em> is helpful conceptually.</li>
<li>Even if in practice they conflate (e.g. deep neural networks).</li>
<li>Sometimes difficult to pull apart.</li>
<li>Helpful to revisit algorithms with modelling perspective in mind.
<ul>
<li>Probabilistic numerics</li>
</ul></li>
</ul>
<!--include{_dimred/includes/ppca-marginal-likelihood.md}-->
</section>
<section id="difficulty-for-probabilistic-approaches" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A two dimensional grid mapped into three dimensions to form a two dimensional manifold.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-1" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A one dimensional line mapped into two dimensions by two separate independent functions. Each point can be mapped exactly through the mappings.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-2" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Gaussian density over the input of a non linear function leads to a very non Gaussian output. Here the output is multimodal.
</aside>
<!-- SECTION Dual Probabilistic PCA and GP-LVM -->
</section>
<section id="dual-probabilistic-pca-and-gp-lvm" class="slide level2">
<h2>Dual Probabilistic PCA and GP-LVM</h2>
</section>
<section id="dual-probabilistic-pca" class="slide level2">
<h2>Dual Probabilistic PCA</h2>
<p><strong>Probabilistic PCA</strong></p>
<ul>
<li>We have seen that PCA has a probabilistic interpretation <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>.</li>
<li>It is difficult to `non-linearise’ directly.</li>
<li>GTM and Density Networks are an attempt to do so.</li>
</ul>
<p><strong>Dual Probabilistic PCA</strong></p>
<ul>
<li>There is an alternative probabilistic interpretation of PCA <span class="citation" data-cites="Lawrence:pnpca05">(Lawrence, 2005)</span>.</li>
<li>This interpretation can be made non-linear.</li>
<li>The result is non-linear probabilistic PCA.</li>
</ul>
</section>
<section id="linear-latent-variable-model-iii" class="slide level2">
<h2>Linear Latent Variable Model III</h2>
<table>
<tr>
<td width="45%">
<p><strong>Dual Probabilistic PCA</strong></p>
<ul>
<li>Define <em>linear-Gaussian relationship</em> between latent variables and data.</li>
<li><strong>Novel</strong> Latent variable approach:</li>
<li>Define Gaussian prior over , <span class="math inline">\(\mathbf{W}\)</span>.</li>
<li>Integrate out <em>parameters</em>.
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/gplvm_graph.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<span class="math display">\[
p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^{2}\mathbf{I}\right)
\]</span> <span class="math display">\[
 p\left(\mathbf{W}\right)=\prod_{i=1}^{p}\mathcal{N}\left(\mathbf{ w}_{i,:}|\mathbf{0},\mathbf{I}\right)\]</span> <span class="math display">\[
 p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{p}\mathcal{N}\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\right)\]</span>
</td>
</tr>
</table></li>
</ul>
<p>{</p>
</section>
<section id="linear-latent-variable-model-iv" class="slide level2">
<h2>Linear Latent Variable Model IV</h2>
<div class="fragment" data-fragment-index="1" style="">
<strong>Dual</strong>
</div>
Probabilistic PCA Max. Likelihood Soln
<div class="fragment" data-fragment-index="1" style="">
<span class="citation" data-cites="Lawrence:gplvm03">(Lawrence, n.d., p. @Lawrence:pnpca05)</span>
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/gplvm_graph.png" width="25%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>
<div class="fragment" data-fragment-index="1" style="">
<p><span class="math display">\[
p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{p}\mathcal{N}\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\right)
\]</span></p>
</div>
<div class="fragment" data-fragment-index="2" style="">
<span class="math display">\[
p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{p}\mathcal{N}\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{K}\right),\quad\quad\mathbf{K}=\mathbf{Z}\mathbf{\mathbf{Z}}^{\top}+\sigma^{2}\mathbf{I}
\]</span> <span class="math display">\[
\log p\left(\mathbf{Y}|\mathbf{Z}\right)=-\frac{p}{2}\log\left|\mathbf{K}\right|-\frac{1}{2}\text{tr}\left(\mathbf{K}^{-1}\mathbf{Y}\mathbf{Y}^{\top}\right)+\mbox{const.}
\]</span> If <span class="math inline">\(\mathbf{U}_{q}^{\prime}\)</span> are first <span class="math inline">\(q\)</span> principal eigenvectors of <span class="math inline">\(p^{-1}\mathbf{Y}\mathbf{Y}^{\top}\)</span> and the corresponding eigenvalues are <span class="math inline">\(\Lambda_{q}\)</span>, <span class="math display">\[
\mathbf{Z}=\mathbf{U^{\prime}}_{q}\mathbf{L}\mathbf{R}^{\top},\quad\quad\mathbf{L}=\left(\Lambda_{q}-\sigma^{2}\mathbf{I}\right)^{\frac{1}{2}}
\]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an arbitrary rotation matrix.
</div>
<div class="fragment" data-fragment-index="3" style="">
<span class="math display">\[
p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{ y}_{i,:}|\mathbf{0},\mathbf{C}\right),\quad\quad\mathbf{C}=\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\]</span> <span class="math display">\[
\log p\left(\mathbf{Y}|\mathbf{W}\right)=-\frac{n}{2}\log\left|\mathbf{C}\right|-\frac{1}{2}\text{tr}\left(\mathbf{C}^{-1}\mathbf{Y}^{\top}\mathbf{Y}\right)+\mbox{const.}
\]</span> If <span class="math inline">\(\mathbf{U}_{q}\)</span> are first <span class="math inline">\(q\)</span> principal eigenvectors of <span class="math inline">\(n^{-1}\mathbf{Y}^{\top}\mathbf{Y}\)</span> and the corresponding eigenvalues are <span class="math inline">\(\Lambda_{q}\)</span>, <span class="math display">\[
\mathbf{W}=\mathbf{U}_{q}\mathbf{L}\mathbf{R}^{\top},\quad\quad\mathbf{L}=\left(\Lambda_{q}-\sigma^{2}\mathbf{I}\right)^{\frac{1}{2}}
\]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an arbitrary rotation matrix.
</div>
<pre><code> &lt;/small&gt;</code></pre>
</section>
<section id="equivalence-of-formulations" class="slide level2">
<h2>Equivalence of Formulations</h2>
<p><strong>The Eigenvalue Problems are equivalent</strong></p>
<ul>
<li><p>Solution for Probabilistic PCA (solves for the mapping) <span class="math display">\[
\mathbf{Y}^{\top}\mathbf{Y}\mathbf{U}_{q}=\mathbf{U}_{q}\Lambda_{q}\quad\quad\quad\mathbf{W}=\mathbf{U}_{q}\mathbf{L}\mathbf{V}^{\top}
    \]</span></p></li>
<li><p>Solution for Dual Probabilistic PCA (solves for the latent positions) <span class="math display">\[
\mathbf{Y}\mathbf{Y}^{\top}\mathbf{U}_{q}^{\prime}=\mathbf{U}_{q}^{\prime}\Lambda_{q}\quad\quad\quad\mathbf{Z}=\mathbf{U}_{q}^{\prime}\mathbf{L}\mathbf{V}^{\top}
  \]</span></p></li>
<li><p>Equivalence is from <span class="math display">\[
\mathbf{U}_{q}=\mathbf{Y}^{\top}\mathbf{U}_{q}^{\prime}\Lambda_{q}^{-\frac{1}{2}}
  \]</span></p></li>
</ul>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Here we’re showing 20 samples taken from the prior over functions defined by our covarariance
</aside>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
We can sample many such functions, in this slide there are now 1000 in total. This is a sample from our prior over functions.
</aside>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Now we observe data. Here there are three data points. Conceptually in Bayesian inference we discard all samples that are distant from the data.
</aside>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Throwing away such samples we are left with our posterior. This is the collection of samples from the prior that are consistent with the data.
</aside>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
The elegance of the Gaussian process is that this result can be computed analytically using linear algebra.
</aside>
</section>
<section id="gaussian-process-gp" class="slide level2">
<h2>Gaussian Process (GP)</h2>
<p><strong>Prior for Functions</strong></p>
<ul>
<li><p>Probability Distribution over Functions</p></li>
<li><p>Functions are infinite dimensional.</p>
<ul>
<li>Prior distribution over <em>instantiations</em> of the function: finite dimensional objects.</li>
<li>Can prove by induction that GP is ‘consistent’.</li>
</ul></li>
</ul>
</section>
<section id="gaussian-process-gp-ii" class="slide level2">
<h2>Gaussian Process (GP) II</h2>
<ul>
<li><p>Mean and Covariance Functions</p></li>
<li><p>Instead of mean and covariance matrix, GP is defined by mean function and covariance function.</p>
<ul>
<li>Mean function often taken to be zero or constant.</li>
<li>Covariance function must be <em>positive definite</em>.</li>
<li>Class of valid covariance functions is the same as the class of <em>Mercer kernels</em>.</li>
</ul></li>
</ul>
</section>
<section id="gaussian-processes-iii" class="slide level2">
<h2>Gaussian Processes III</h2>
<p><strong>Zero mean Gaussian Process</strong></p>
<ul>
<li><p>A (zero mean) Gaussian process likelihood is of the form<span class="math display">\[
  p\left(\mathbf{ y}|\mathbf{Z}\right)=N\left(\mathbf{ y}|\mathbf{0},\mathbf{K}\right),\]</span> where <span class="math inline">\(\mathbf{K}\)</span> is the covariance function or .</p></li>
<li><p>The  with noise has the form<span class="math display">\[
  \mathbf{K}=\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\]</span></p></li>
<li><p>Priors over non-linear functions are also possible.</p>
<ul>
<li>To see what functions look like, we can sample from the prior process.</li>
</ul></li>
</ul>
</section>
<section id="gaussian-process-regression" class="slide level2">
<h2>Gaussian Process Regression</h2>
<p><strong>Posterior Distribution over Functions</strong></p>
<ul>
<li>Gaussian processes are often used for regression.</li>
<li>We are given a known inputs <span class="math inline">\(\mathbf{Z}\)</span> and targets <span class="math inline">\(\mathbf{Y}\)</span>.</li>
<li>We assume a prior distribution over functions by selecting a kernel.</li>
<li>Combine the prior with data to get a  distribution over functions.</li>
</ul>
</section>
<section id="exponentiated-quadratic-covariance" class="slide level2">
<h2>Exponentiated Quadratic Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha \exp\left(-\frac{\left\Vert \mathbf{ x}-\mathbf{ x}^\prime \right\Vert_2^2}{2\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The exponentiated quadratic covariance function.
</aside>
</section>
<section id="learning-covariance-parameters" class="slide level2">
<h2>Learning Covariance Parameters</h2>
<p>Can we determine covariance parameters from the data?</p>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\mathcal{N}\left(\mathbf{ y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}{\det{\mathbf{K}}^{\frac{1}{2}}}}{\exp\left(-\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\]</span></p>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{N}\left(\mathbf{ y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}\color{yellow}{\det{\mathbf{K}}^{\frac{1}{2}}}}\color{cyan}{\exp\left(-\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\end{aligned}
\]</span></p>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \log \mathcal{N}\left(\mathbf{ y}|\mathbf{0},\mathbf{K}\right)=&amp;\color{yellow}{-\frac{1}{2}\log\det{\mathbf{K}}}\color{cyan}{-\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}} \\ &amp;-\frac{n}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
E(\boldsymbol{ \theta}) = \color{yellow}{\frac{1}{2}\log\det{\mathbf{K}}} + \color{cyan}{\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}
\]</span></p>
</section>
<section id="capacity-control-through-the-determinant" class="slide level2">
<h2>Capacity Control through the Determinant</h2>
<p>The parameters are <em>inside</em> the covariance function (matrix).  <span class="math display">\[k_{i, j} = k(\mathbf{ x}_i, \mathbf{ x}_j; \boldsymbol{ \theta})\]</span></p>
</section>
<section id="eigendecomposition-of-covariance" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<p><span> <span class="math display">\[\mathbf{K}= \mathbf{R}\boldsymbol{ \Lambda}^2 \mathbf{R}^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\boldsymbol{ \Lambda}\)</span> represents distance on axes. <span class="math inline">\(\mathbf{R}\)</span> gives rotation.
</td>
</tr>
</table>
</section>
<section id="eigendecomposition-of-covariance-1" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<ul>
<li><span class="math inline">\(\boldsymbol{ \Lambda}\)</span> is <em>diagonal</em>, <span class="math inline">\(\mathbf{R}^\top\mathbf{R}= \mathbf{I}\)</span>.</li>
<li>Useful representation since <span class="math inline">\(\det{\mathbf{K}} = \det{\boldsymbol{ \Lambda}^2} = \det{\boldsymbol{ \Lambda}}^2\)</span>.</li>
</ul>
</section>
<section id="capacity-control-coloryellowlog-detmathbfk" class="slide level2">
<h2>Capacity control: <span class="math inline">\(\color{yellow}{\log \det{\mathbf{K}}}\)</span></h2>
<script>
showDivs(0, 'gp-optimise-determinant');
</script>
<p><small></small> <input id="range-gp-optimise-determinant" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise-determinant')" oninput="setDivs('gp-optimise-determinant')"> <button onclick="plusDivs(-1, 'gp-optimise-determinant')">❮</button> <button onclick="plusDivs(1, 'gp-optimise-determinant')">❯</button></p>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant000.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant001.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant002.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant003.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant004.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant005.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant006.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant007.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant008.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-determinant009.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="data-fit-colorcyanfracmathbf-ytopmathbfk-1mathbf-y2" class="slide level2">
<h2>Data Fit: <span class="math inline">\(\color{cyan}{\frac{\mathbf{ y}^\top\mathbf{K}^{-1}\mathbf{ y}}{2}}\)</span></h2>
<script>
showDivs(0, 'gp-optimise-quadratic');
</script>
<p><small></small> <input id="range-gp-optimise-quadratic" type="range" min="0" max="2" value="0" onchange="setDivs('gp-optimise-quadratic')" oninput="setDivs('gp-optimise-quadratic')"> <button onclick="plusDivs(-1, 'gp-optimise-quadratic')">❮</button> <button onclick="plusDivs(1, 'gp-optimise-quadratic')">❯</button></p>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-quadratic000.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-quadratic001.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
<div class="figure">
<div id="gp-optimise-quadratic-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The data fit term of the Gaussian process is a quadratic loss centered around zero. This has eliptical contours, the principal axes of which are given by the covariance matrix.
</aside>
</section>
<section id="eboldsymbol-theta-coloryellowfrac12logdetmathbfkcolorcyanfracmathbf-ytopmathbfk-1mathbf-y2" class="slide level2">
<h2><span class="math display">\[E(\boldsymbol{ \theta}) = \color{yellow}{\frac{1}{2}\log\det{\mathbf{K}}}+\color{cyan}{\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}\]</span></h2>
</section>
<section id="quadratic-data-fit" class="slide level2">
<h2>Quadratic Data Fit</h2>
</section>
<section id="data-fit-term" class="slide level2">
<h2>Data Fit Term</h2>
<script>
showDivs(0, 'gp-optimise');
</script>
<p><small></small> <input id="range-gp-optimise" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise')" oninput="setDivs('gp-optimise')"> <button onclick="plusDivs(-1, 'gp-optimise')">❮</button> <button onclick="plusDivs(1, 'gp-optimise')">❯</button></p>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise000.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise001.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise002.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise003.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise004.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise005.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise007.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise008.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise009.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise011.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise012.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise013.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise014.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise015.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise017.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise018.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise019.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise020.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="non-linear-latent-variable-model" class="slide level2">
<h2>Non-Linear Latent Variable Model</h2>
<table>
<tr>
<td width="45%">
<p><small><strong>Dual Probabilistic PCA</strong> * Define <em>linear-Gaussian relationship</em> between latent variables and data. * <strong>Novel</strong> Latent variable approach: * Define Gaussian prior over <em>parameteters</em>, <span class="math inline">\(\mathbf{W}\)</span>. * Integrate out <em>parameters</em>.</p>
<ul>
<li>Inspection of the marginal likelihood shows …
<ul>
<li>The covariance matrix is a covariance function.</li>
<li>We recognise it as the ‘linear kernel’. </small>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/dimred/gplvm_graph.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>
<div class="fragment" data-fragment-index="1" style="">
<span class="math display">\[
 p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}N\left(\mathbf{ y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^{2}\mathbf{I}\right)\]</span> <span class="math display">\[
 p\left(\mathbf{W}\right)=\prod_{i=1}^{d}N\left(\mathbf{ w}_{i,:}|\mathbf{0},\mathbf{I}\right)\]</span>
</div>
<span class="math display">\[
 p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{d}N\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\right)\]</span> <span class="math display">\[
 p\left(\mathbf{Y}|\mathbf{Z}\right)=\prod_{j=1}^{d}N\left(\mathbf{ y}_{:,j}|\mathbf{0},\mathbf{K}\right)\]</span> <span class="math display">\[
\mathbf{K}=\mathbf{Z}\mathbf{Z}^{\top}+\sigma^{2}\mathbf{I}\]</span> This is a product of Gaussian processes with linear kernels. <span class="math display">\[
\mathbf{K}=?
\]</span> Replace linear kernel with non-linear kernel for non-linear model.</small>
</td>
</tr>
</table></li>
</ul></li>
</ul>
</section>
<section id="non-linear-latent-variable-model-1" class="slide level2">
<h2>Non-Linear Latent Variable Model</h2>
<p><strong>EQ Kernel</strong></p>
<ul>
<li><p>The RBF kernel has the form <span class="math inline">\(k_{i,j}=k\left(\mathbf{ z}_{i,:},\mathbf{ z}_{j,:}\right),\)</span> where <span class="math display">\[
k\left(\mathbf{ z}_{i,:},\mathbf{ z}_{j,:}\right)=\alpha\exp\left(-\frac{\left(\mathbf{ z}_{i,:}-\mathbf{ z}_{j,:}\right)^{\top}\left(\mathbf{ z}_{i,:}-\mathbf{ z}_{j,:}\right)}{2\ell^{2}}\right).
\]</span></p></li>
<li><p>No longer possible to optimise wrt <span class="math inline">\(\mathbf{Z}\)</span> via an eigenvalue problem.</p></li>
<li><p>Instead find gradients with respect to <span class="math inline">\(\mathbf{Z},\alpha,\ell\)</span> and <span class="math inline">\(\sigma^{2}\)</span> and optimise using gradient methods.</p></li>
</ul>
</section>
<section id="oil-data" class="slide level2">
<h2>Oil Data</h2>
</section>
<section id="stick-man-data" class="slide level2">
<h2>Stick Man Data</h2>
</section>
<section id="applications" class="slide level2">
<h2>Applications</h2>
<ul>
<li>Style based inverse kinematics <span class="citation" data-cites="Grochow:styleik04">(Grochow et al., 2004)</span>.</li>
<li>Prior distributions for tracking <span class="citation" data-cites="Urtasun:3dpeople06">(Urtasun et al., 2006, p. Urtasun:priors05)</span>.</li>
<li>Assisted drawing <span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo, 2006)</span>.</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>GPLVM based on a dual probabilistic interpretation of PCA.</li>
<li>Straightforward to non-linearise it using Gaussian processes.</li>
<li>Result is a non-linear probabilistic PCA.</li>
<li><em>Optimise latent variables</em> rather than integrate them out.</li>
</ul>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="getting-started-and-downloading-data" class="slide level2">
<h2>Getting Started and Downloading Data</h2>
</section>
<section id="principal-component-analysis-3" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>What is the right shape <span class="math inline">\(n\times p\)</span> to use?</li>
</ul>
</section>
<section id="gaussian-process-latent-variable-model" class="slide level2">
<h2>Gaussian Process Latent Variable Model</h2>

</section>
<section id="cmu-mocap-database" class="slide level2">
<h2>CMU Mocap Database</h2>
</section>
<section id="example-latent-doodle-space" class="slide level2">
<h2>Example: Latent Doodle Space</h2>
<div class="figure">
<div id="latent-doodle-space-figure" class="figure-frame">
<iframe width height src="https://player.vimeo.com/video/3235882#t=" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
The latent doodle space idea of <span class="citation" data-cites="Baxter:doodle06">Baxter and Anjyo (2006)</span> manages to build a smooth mapping across very sparse data.
</aside>
</section>
<section id="example-latent-doodle-space-1" class="slide level2">
<h2>Example: Latent Doodle Space</h2>
<p><strong>Generalization with much less Data than Dimensions</strong></p>
<ul>
<li><p>Powerful uncertainly handling of GPs leads to surprising properties.</p></li>
<li><p>Non-linear models can be used where there are fewer data points than dimensions <em>without overfitting</em>.</p></li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Baxter:doodle06">(Baxter and Anjyo, 2006)</span></span></p>
</section>
<section id="example-continuous-character-control" class="slide level2">
<h2>Example: Continuous Character Control</h2>
<ul>
<li>Graph diffusion prior for enforcing connectivity between motions. <span class="math display">\[\log p(\mathbf{X}) = w_c \sum_{i,j} \log K_{ij}^d\]</span> with the graph diffusion kernel <span class="math inline">\(\mathbf{K}^d\)</span> obtain from <span class="math display">\[K_{ij}^d = \exp(\beta \mathbf{H})
\qquad \text{with} \qquad \mathbf{H} = -\mathbf{T}^{-1/2} \mathbf{L} \mathbf{T}^{-1/2}\]</span> the graph Laplacian, and <span class="math inline">\(\mathbf{T}\)</span> is a diagonal matrix with <span class="math inline">\(T_{ii} = \sum_j w(\mathbf{ x}_i, \mathbf{ x}_j)\)</span>, <span class="math display">\[L_{ij} = \begin{cases} \sum_k w(\mathbf{ x}_i,\mathbf{ x}_k) &amp; \text{if $i=j$}
\\
-w(\mathbf{ x}_i,\mathbf{ x}_j) &amp;\text{otherwise.}
\end{cases}\]</span> and <span class="math inline">\(w(\mathbf{ x}_i,\mathbf{ x}_j) = || \mathbf{ x}_i - \mathbf{ x}_j||^{-p}\)</span> measures similarity.</li>
</ul>
<p><span style="text-align:right"><span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span></span></p>
</section>
<section id="character-control-results" class="slide level2">
<h2>Character Control: Results</h2>
<div class="figure">
<div id="charcter-control-gplvm-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/hr3pdDl5IAg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Character control in the latent space described the the GP-LVM <span class="citation" data-cites="Levine:control12">Levine et al. (2012)</span>.
</aside>
</section>
<section id="data-for-blastocyst-development-in-mice-single-cell-taqman-arrays" class="slide level2">
<h2>Data for Blastocyst Development in Mice: Single Cell TaqMan Arrays</h2>
</section>
<section id="principal-component-analysis-4" class="slide level2">
<h2>Principal Component Analysis</h2>
</section>
<section id="pca-result" class="slide level2">
<h2>PCA Result</h2>
<div class="figure">
<div id="singlecell-data-pca-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/singlecell-data-pca.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
First two principal compoents of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data.
</aside>
</section>
<section id="gp-lvm-on-the-data" class="slide level2">
<h2>GP-LVM on the Data</h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Max Zwiessele
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/max-zwiessele.jpg" clip-path="url(#clip0)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Oliver Stegle
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/oliver-stegle.jpg" clip-path="url(#clip1)"/>
</svg>
</div>
<div class="figure">
<div id="singlecell-gplvm-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/singlecell-gplvm.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with the GP-LVM.
</aside>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="figure">
<div id="singlecell-gplvm-ard-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gplvm/singlecell-gplvm-ard.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The ARD parameters of the GP-LVM for the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data.
</aside>
</section>
<section id="blastocyst-data-isomap" class="slide level2">
<h2>Blastocyst Data: Isomap</h2>
<div class="figure">
<div id="singlecell-isomap-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/singlecell-isomap.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with Isomap.
</aside>
</section>
<section id="blastocyst-data-locally-linear-embedding" class="slide level2">
<h2>Blastocyst Data: Locally Linear Embedding</h2>
<div class="figure">
<div id="singlecell-lle-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/singlecell-lle.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the <span class="citation" data-cites="Guo:fate10">Guo et al. (2010)</span> blastocyst development data with a locally linear embedding.
</aside>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Baxter:doodle06">
<p>Baxter, W.V., Anjyo, K.-I., 2006. Latent doodle space, in: EUROGRAPHICS. Vienna, Austria, pp. 477–485. <a href="https://doi.org/10.1111/j.1467-8659.2006.00967.x">https://doi.org/10.1111/j.1467-8659.2006.00967.x</a></p>
</div>
<div id="ref-Grochow:styleik04">
<p>Grochow, K., Martin, S.L., Hertzmann, A., Popovic, Z., 2004. Style-based inverse kinematics, in: ACM Transactions on Graphics (Siggraph 2004). pp. 522–531. <a href="https://doi.org/10.1145/1186562.1015755">https://doi.org/10.1145/1186562.1015755</a></p>
</div>
<div id="ref-Guo:fate10">
<p>Guo, G., Huss, M., Tong, G.Q., Wang, C., Sun, L.L., Clarke, N.D., Robsonemail, P., 2010. Resolution of cell fate decisions revealed by single-cell gene expression analysis from zygote to blastocyst. Developmental Cell 18, 675–685. <a href="https://doi.org/10.1016/j.devcel.2010.02.012">https://doi.org/10.1016/j.devcel.2010.02.012</a></p>
</div>
<div id="ref-Hotelling:analysis33">
<p>Hotelling, H., 1933. Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology 24, 417–441.</p>
</div>
<div id="ref-Lawrence:gplvm03">
<p>Lawrence, N.D., n.d. Gaussian process models for visualisation of high dimensional data, in:. pp. 329–336.</p>
</div>
<div id="ref-Lawrence:pnpca05">
<p>Lawrence, N.D., 2005. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of Machine Learning Research 6, 1783–1816.</p>
</div>
<div id="ref-Levine:control12">
<p>Levine, S., Wang, J.M., Haraux, A., Popović, Z., Koltun, V., 2012. Continuous character control with low-dimensional embeddings. ACM Transactions on Graphics (SIGGRAPH 2012) 31.</p>
</div>
<div id="ref-Tipping:probpca99">
<p>Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a></p>
</div>
<div id="ref-Urtasun:3dpeople06">
<p>Urtasun, R., Fleet, D.J., Fua, P., 2006. 3D people tracking with Gaussian process dynamical models, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Press, New York, U.S.A., pp. 238–245. <a href="https://doi.org/10.1109/CVPR.2006.15">https://doi.org/10.1109/CVPR.2006.15</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
