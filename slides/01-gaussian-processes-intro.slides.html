<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2020-01-24">
  <title>R250: Gaussian Processes Introduction</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">R250: Gaussian Processes Introduction</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil
D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2020-01-24</time></p>
  <p class="venue" style="text-align:center">Computer Lab, University of
Cambridge</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pierre-simon-laplace-image-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/Pierre-Simon_Laplace.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Pierre-Simon Laplace 1749-1827.
</aside>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=1YQPAAAAQAAJ&amp;pg=PR17-IA2"><img
data-src="https://mlatcl.github.io/deepnn/./slides/diagrams//books/1YQPAAAAQAAJ-PR17-IA2.png" /></a>
</div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="figure">
<div id="laplaces-determinism-english-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/laplacesDeterminismEnglish.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Laplace’s determinsim in English translation.
</aside>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="figure">
<div id="laplaces-demon-cropped-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala_16_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
</aside>
<!--<object data="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala.pdf" type="application/pdf" width="80%" height="">
    <embed src="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala.pdf" type="application/pdf">
        <p>This browser does not support PDF viewing. Please download the PDF to view it: <a href="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala.pdf">Download PDF</a>.</p>
    </embed>
</object>-->
</section>
<section id="laplaces-gremlin" class="slide level2">
<h2>Laplace’s Gremlin</h2>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=1YQPAAAAQAAJ&amp;pg=PR17-IA4"><img
data-src="https://mlatcl.github.io/deepnn/./slides/diagrams//books/1YQPAAAAQAAJ-PR17-IA4.png" /></a>
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="figure">
<div id="probability-relative-in-part-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
To Laplace, determinism is a strawman. Ignorance of mechanism and data
leads to uncertainty which should be dealt with through probability.
</aside>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="figure">
<div id="probability-relative-in-part-cropped-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala_18_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
</aside>
<!--<object data="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala.pdf" type="application/pdf" width="80%" height="">
    <embed src="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala.pdf" type="application/pdf">
        <p>This browser does not support PDF viewing. Please download the PDF to view it: <a href="https://mlatcl.github.io/deepnn/./slides/diagrams//physics/philosophicaless00lapliala.pdf">Download PDF</a>.</p>
    </embed>
</object>
-->
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Here we’re showing 20 samples taken from the prior over functions
defined by our covarariance
</aside>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
We can sample many such functions, in this slide there are now 1000 in
total. This is a sample from our prior over functions.
</aside>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Now we observe data. Here there are three data points. Conceptually in
Bayesian inference we discard all samples that are distant from the
data.
</aside>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Throwing away such samples we are left with our posterior. This is the
collection of samples from the prior that are consistent with the data.
</aside>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
The elegance of the Gaussian process is that this result can be computed
analytically using linear algebra.
</aside>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level2">
<h2>What is Machine Learning?</h2>
</section>
<section id="what-is-machine-learning-1" class="slide level2">
<h2>What is Machine Learning?</h2>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively
acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience
(other data! transfer learning etc), or beliefs about the regularities
of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a
categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a
href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine
Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-2" class="slide level2">
<h2>What is Machine Learning?</h2>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<ul>
<li class="fragment">To combine data with a model need:</li>
<li class="fragment"><strong>a prediction function</strong> <span
class="math inline">\(f(\cdot)\)</span> includes our beliefs about the
regularities of the universe</li>
<li class="fragment"><strong>an objective function</strong> <span
class="math inline">\(E(\cdot)\)</span> defines the cost of
misprediction.</li>
</ul>
</section>
<section id="artificial-intelligence" class="slide level2">
<h2>Artificial Intelligence</h2>
<ul>
<li>Machine learning is a mainstay because of importance of
prediction.</li>
</ul>
</section>
<section id="uncertainty" class="slide level2">
<h2>Uncertainty</h2>
<ul>
<li>Uncertainty in prediction arises from:</li>
<li>scarcity of training data and</li>
<li>mismatch between the set of prediction functions we choose and all
possible prediction functions.</li>
<li>Also uncertainties in objective, leave those for another day.</li>
</ul>
</section>
<section id="neural-networks-and-prediction-functions"
class="slide level2">
<h2>Neural Networks and Prediction Functions</h2>
<ul>
<li>adaptive non-linear function models inspired by simple neuron models
<span class="citation" data-cites="McCulloch:neuron43">(McCulloch and
Pitts, 1943)</span></li>
<li>have become popular because of their ability to model data.</li>
<li>can be composed to form highly complex functions</li>
<li>start by focussing on one hidden layer</li>
</ul>
</section>
<section id="prediction-function-of-one-hidden-layer"
class="slide level2">
<h2>Prediction Function of One Hidden Layer</h2>
<p><span class="math display">\[
f(\mathbf{ x}) = \left.\mathbf{ w}^{(2)}\right.^\top \boldsymbol{
\phi}(\mathbf{W}_{1}, \mathbf{ x})
\]</span></p>
<p><span class="math inline">\(f(\cdot)\)</span> is a scalar function
with vector inputs,</p>
<p><span class="math inline">\(\boldsymbol{ \phi}(\cdot)\)</span> is a
vector function with vector inputs.</p>
<ul>
<li><p>dimensionality of the vector function is known as the number of
hidden units, or the number of neurons.</p></li>
<li><p>elements of <span class="math inline">\(\boldsymbol{
\phi}(\cdot)\)</span> are the <em>activation</em> function of the neural
network</p></li>
<li><p>elements of <span class="math inline">\(\mathbf{W}_{1}\)</span>
are the parameters of the activation functions.</p></li>
</ul>
</section>
<section id="relations-with-classical-statistics" class="slide level2">
<h2>Relations with Classical Statistics</h2>
<ul>
<li><p>In statistics activation functions are known as <em>basis
functions</em>.</p></li>
<li><p>would think of this as a <em>linear model</em>: not linear
predictions, linear in the parameters</p></li>
<li><p><span class="math inline">\(\mathbf{ w}_{1}\)</span> are
<em>static</em> parameters.</p></li>
</ul>
</section>
<section id="adaptive-basis-functions" class="slide level2">
<h2>Adaptive Basis Functions</h2>
<ul>
<li>In machine learning we optimize <span
class="math inline">\(\mathbf{W}_{1}\)</span> as well as <span
class="math inline">\(\mathbf{W}_{2}\)</span> (which would normally be
denoted in statistics by <span
class="math inline">\(\boldsymbol{\beta}\)</span>).</li>
</ul>
</section>
<section id="integrated-basis-functions" class="slide level2">
<h2>Integrated Basis Functions</h2>
<ul>
<li><p>Revisit that decision: follow the path of <span class="citation"
data-cites="Neal:bayesian94">Neal (1994)</span> and <span
class="citation" data-cites="MacKay:bayesian92">MacKay
(1992)</span>.</p></li>
<li><p>Consider the probabilistic approach.</p></li>
</ul>
</section>
<section id="probabilistic-modelling" class="slide level2">
<h2>Probabilistic Modelling</h2>
<ul>
<li>Probabilistically we want, <span class="math display">\[
p(y_*|\mathbf{ y}, \mathbf{X}, \mathbf{ x}_*),
\]</span> <span class="math inline">\(y_*\)</span> is a test output
<span class="math inline">\(\mathbf{ x}_*\)</span> is a test input <span
class="math inline">\(\mathbf{X}\)</span> is a training input matrix
<span class="math inline">\(\mathbf{ y}\)</span> is training
outputs</li>
</ul>
</section>
<section id="joint-model-of-world" class="slide level2">
<h2>Joint Model of World</h2>
<p><span class="math display">\[
p(y_*|\mathbf{ y}, \mathbf{X}, \mathbf{ x}_*) = \int p(y_*|\mathbf{
x}_*, \mathbf{W}) p(\mathbf{W}| \mathbf{ y}, \mathbf{X}) \text{d}
\mathbf{W}
\]</span></p>
<div class="fragment">
<p><span class="math inline">\(\mathbf{W}\)</span> contains <span
class="math inline">\(\mathbf{W}_1\)</span> and <span
class="math inline">\(\mathbf{W}_2\)</span></p>
<p><span class="math inline">\(p(\mathbf{W}| \mathbf{ y},
\mathbf{X})\)</span> is posterior density</p>
</div>
</section>
<section id="likelihood" class="slide level2">
<h2>Likelihood</h2>
<p><span class="math inline">\(p(y|\mathbf{ x}, \mathbf{W})\)</span> is
the <em>likelihood</em> of data point</p>
<div class="fragment">
<p>Normally assume independence: <span class="math display">\[
p(\mathbf{ y}|\mathbf{X}, \mathbf{W}) = \prod_{i=1}^np(y_i|\mathbf{
x}_i, \mathbf{W}),\]</span></p>
</div>
</section>
<section id="likelihood-and-prediction-function" class="slide level2">
<h2>Likelihood and Prediction Function</h2>
<p><span class="math display">\[
p(y_i | f(\mathbf{ x}_i)) = \frac{1}{\sqrt{2\pi \sigma^2}}
\exp\left(-\frac{\left(y_i - f(\mathbf{
x}_i)\right)^2}{2\sigma^2}\right)
\]</span></p>
</section>
<section id="unsupervised-learning" class="slide level2">
<h2>Unsupervised Learning</h2>
<ul>
<li><p>Can also consider priors over latents <span
class="math display">\[
p(\mathbf{ y}_*|\mathbf{ y}) = \int p(\mathbf{ y}_*|\mathbf{X}_*,
\mathbf{W}) p(\mathbf{W}| \mathbf{ y}, \mathbf{X}) p(\mathbf{X})
p(\mathbf{X}_*) \text{d} \mathbf{W}\text{d}
\mathbf{X}\text{d}\mathbf{X}_*
\]</span></p></li>
<li><p>This gives <em>unsupervised learning</em>.</p></li>
</ul>
</section>
<section id="probabilistic-inference" class="slide level2">
<h2>Probabilistic Inference</h2>
<ul>
<li><p>Data: <span class="math inline">\(\mathbf{ y}\)</span></p></li>
<li><p>Model: <span class="math inline">\(p(\mathbf{ y}, \mathbf{
y}^*)\)</span></p></li>
<li><p>Prediction: <span class="math inline">\(p(\mathbf{ y}^*| \mathbf{
y})\)</span></p></li>
</ul>
</section>
<section id="graphical-models" class="slide level2">
<h2>Graphical Models</h2>
<ul>
<li>Represent joint distribution through <em>conditional
dependencies</em>.</li>
<li>E.g. Markov chain</li>
</ul>
<p><span class="math display">\[p(\mathbf{ y}) = p(y_n| y_{n-1})
p(y_{n-1}|y_{n-2}) \dots p(y_{2} | y_{1})\]</span></p>
<div class="figure">
<div id="markov-chain-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/markov.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Markov chain is a simple form of probabilistic graphical model
providing a particular decomposition of the joint density.
</aside>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<p>Predict Perioperative Risk of Clostridium Difficile Infection
Following Colon Surgery <span class="citation"
data-cites="Steele:predictive12">(Steele et al., 2012)</span></p>
<div class="figure">
<div id="c-difficile-bayes-net-diagnosis-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/deepnn/./slides/diagrams//bayes-net-diagnosis.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A probabilistic directed graph used to predict the perioperative risk of
<em>C Difficile</em> infection following colon surgery. When these
models have good predictive performance they are often difficult to
interpret. This may be due to the limited representation capability of
the conditional densities in the model.
</aside>
</section>
<section id="performing-inference" class="slide level2">
<h2>Performing Inference</h2>
<ul>
<li><p>Easy to write in probabilities</p></li>
<li><p>But underlying this is a wealth of computational
challenges.</p></li>
<li><p>High dimensional integrals typically require
approximation.</p></li>
</ul>
</section>
<section id="linear-models" class="slide level2">
<h2>Linear Models</h2>
<ul>
<li><p>In statistics, focussed more on <em>linear</em> model implied by
<span class="math display">\[
f(\mathbf{ x}) = \left.\mathbf{ w}^{(2)}\right.^\top \boldsymbol{
\phi}(\mathbf{W}_1, \mathbf{ x})
\]</span></p></li>
<li><p>Hold <span class="math inline">\(\mathbf{W}_1\)</span> fixed for
given analysis.</p></li>
<li><p>Gaussian prior for <span
class="math inline">\(\mathbf{W}\)</span>, <span class="math display">\[
\mathbf{ w}^{(2)} \sim \mathcal{N}\left(\mathbf{0},\mathbf{C}\right).
\]</span> <span class="math display">\[
y_i = f(\mathbf{ x}_i) + \epsilon_i,
\]</span> where <span class="math display">\[
\epsilon_i \sim \mathcal{N}\left(0,\sigma^2\right)
\]</span></p></li>
</ul>
</section>
<section id="linear-gaussian-models" class="slide level2">
<h2>Linear Gaussian Models</h2>
<ul>
<li>Normally integrals are complex but for this Gaussian linear case
they are trivial.</li>
</ul>
</section>
<section id="multivariate-gaussian-properties" class="slide level2">
<h2>Multivariate Gaussian Properties</h2>
</section>
<section id="recall-univariate-gaussian-properties"
class="slide level2">
<h2>Recall Univariate Gaussian Properties</h2>
<div class="fragment">
<ol type="1">
<li>Sum of Gaussian variables is also Gaussian.</li>
</ol>
<p><span class="math display">\[y_i \sim
\mathcal{N}\left(\mu_i,\sigma_i^2\right)\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\sum_{i=1}^{n} y_i \sim
\mathcal{N}\left(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2\right)\]</span></p>
</div>
</section>
<section id="recall-univariate-gaussian-properties-1"
class="slide level2">
<h2>Recall Univariate Gaussian Properties</h2>
<ol start="2" type="1">
<li>Scaling a Gaussian leads to a Gaussian.</li>
</ol>
<div class="fragment">
<p><span class="math display">\[y\sim
\mathcal{N}\left(\mu,\sigma^2\right)\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[wy\sim \mathcal{N}\left(w\mu,w^2
\sigma^2\right)\]</span></p>
</div>
</section>
<section id="multivariate-consequence" class="slide level2">
<h2>Multivariate Consequence</h2>
<div style="text-align:left">
If
</div>
<p><span class="math display">\[\mathbf{ x}\sim
\mathcal{N}\left(\boldsymbol{ \mu},\mathbf{C}\right)\]</span></p>
<div class="fragment">
<div style="text-align:left">
And
</div>
<p><span class="math display">\[\mathbf{ y}= \mathbf{W}\mathbf{
x}\]</span></p>
</div>
<div class="fragment">
<div style="text-align:left">
Then
</div>
<p><span class="math display">\[\mathbf{ y}\sim
\mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top\right)\]</span></p>
</div>
</section>
<section id="linear-gaussian-models-1" class="slide level2">
<h2>Linear Gaussian Models</h2>
<ol type="1">
<li>linear Gaussian models are easier to deal with</li>
<li>Even the parameters <em>within</em> the process can be handled, by
considering a particular limit.</li>
</ol>
</section>
<section id="multivariate-gaussian-properties-1" class="slide level2">
<h2>Multivariate Gaussian Properties</h2>
<ul>
<li><p>If <span class="math display">\[
\mathbf{ y}= \mathbf{W}\mathbf{ x}+ \boldsymbol{ \epsilon},
\]</span></p></li>
<li><p>Assume <span class="math display">\[
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{
\mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim
\mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
\]</span></p></li>
<li><p>Then <span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{ \Sigma}\right).
\]</span> If <span class="math inline">\(\boldsymbol{
\Sigma}=\sigma^2\mathbf{I}\)</span>, this is Probabilistic PCA <span
class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop,
1999)</span>.</p></li>
</ul>
</section>
<section id="linear-model-overview" class="slide level2">
<h2>Linear Model Overview</h2>
<ul>
<li>Set each activation function computed at each data point to be</li>
</ul>
<p><span class="math display">\[
\phi_{i,j} = \phi(\mathbf{ w}^{(1)}_{j}, \mathbf{ x}_{i})
\]</span> Define <em>design matrix</em> <span class="math display">\[
\boldsymbol{ \Phi}=
\begin{bmatrix}
\phi_{1, 1} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, h} \\
\phi_{1, 2} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\phi_{n, 1} &amp; \phi_{n, 2} &amp; \dots &amp; \phi_{n, h}
\end{bmatrix}.
\]</span></p>
</section>
<section id="matrix-representation-of-a-neural-network"
class="slide level2">
<h2>Matrix Representation of a Neural Network</h2>
<p><span class="math display">\[y\left(\mathbf{ x}\right) = \boldsymbol{
\phi}\left(\mathbf{ x}\right)^\top \mathbf{ w}+ \epsilon\]</span></p>
<div class="fragment">
<p><span class="math display">\[\mathbf{ y}= \boldsymbol{ \Phi}\mathbf{
w}+ \boldsymbol{ \epsilon}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\boldsymbol{ \epsilon}\sim
\mathcal{N}\left(\mathbf{0},\sigma^2\mathbf{I}\right)\]</span></p>
</div>
</section>
<section id="multivariate-gaussian-properties-2" class="slide level2">
<h2>Multivariate Gaussian Properties</h2>
<ul>
<li><p>If <span class="math display">\[
\mathbf{ y}= \mathbf{W}\mathbf{ x}+ \boldsymbol{ \epsilon},
\]</span></p></li>
<li><p>Assume <span class="math display">\[
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{
\mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim
\mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
\]</span></p></li>
<li><p>Then <span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{ \Sigma}\right).
\]</span> If <span class="math inline">\(\boldsymbol{
\Sigma}=\sigma^2\mathbf{I}\)</span>, this is Probabilistic PCA <span
class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop,
1999)</span>.</p></li>
</ul>
</section>
<section id="prior-density" class="slide level2">
<h2>Prior Density</h2>
<ul>
<li>Define <span class="math display">\[
\mathbf{ w}\sim \mathcal{N}\left(\mathbf{0},\alpha\mathbf{I}\right),
\]</span></li>
<li>Rules of multivariate Gaussians to see that, <span
class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{0},\alpha \boldsymbol{
\Phi}\boldsymbol{ \Phi}^\top + \sigma^2 \mathbf{I}\right).
\]</span></li>
</ul>
<p><span class="math display">\[
\mathbf{K}= \alpha \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top + \sigma^2
\mathbf{I}.
\]</span></p>
</section>
<section id="joint-gaussian-density" class="slide level2">
<h2>Joint Gaussian Density</h2>
<ul>
<li>Elements are a function <span class="math inline">\(k_{i,j} =
k\left(\mathbf{ x}_i, \mathbf{ x}_j\right)\)</span></li>
</ul>
<p><span class="math display">\[
\mathbf{K}= \alpha \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top + \sigma^2
\mathbf{I}.
\]</span></p>
</section>
<section id="covariance-function" class="slide level2">
<h2>Covariance Function</h2>
<p><span class="math display">\[
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) = \alpha \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_i\right)^\top \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_j\right)
\]</span></p>
<ul>
<li>formed by inner products of the rows of the <em>design
matrix</em>.</li>
</ul>
</section>
<section id="gaussian-process" class="slide level2">
<h2>Gaussian Process</h2>
<ul>
<li><p>Instead of making assumptions about our density over each data
point, <span class="math inline">\(y_i\)</span> as i.i.d.</p></li>
<li><p>make a joint Gaussian assumption over our data.</p></li>
<li><p>covariance matrix is now a function of both the parameters of the
activation function, <span class="math inline">\(\mathbf{W}_1\)</span>,
and the input variables, <span
class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li><p>Arises from integrating out <span class="math inline">\(\mathbf{
w}^{(2)}\)</span>.</p></li>
</ul>
</section>
<section id="basis-functions" class="slide level2">
<h2>Basis Functions</h2>
<ul>
<li>Can be very complex, such as deep kernels, <span class="citation"
data-cites="Cho:deep09">(Cho and Saul, 2009)</span> or could even put a
convolutional neural network inside.</li>
<li>Viewing a neural network in this way is also what allows us to
beform sensible <em>batch</em> normalizations <span class="citation"
data-cites="Ioffe:batch15">(Ioffe and Szegedy, 2015)</span>.</li>
</ul>
</section>
<section id="non-degenerate-gaussian-processes" class="slide level2">
<h2>Non-degenerate Gaussian Processes</h2>
<ul>
<li>This process is <em>degenerate</em>.</li>
<li>Covariance function is of rank at most <span
class="math inline">\(h\)</span>.</li>
<li>As <span class="math inline">\(n\rightarrow \infty\)</span>,
covariance matrix is not full rank.</li>
<li>Leading to <span class="math inline">\(\det{\mathbf{K}} =
0\)</span></li>
</ul>
</section>
<section id="infinite-networks" class="slide level2">
<h2>Infinite Networks</h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Radford Neal
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/deepnn/./slides/diagrams//people/radford-neal.jpg" clip-path="url(#clip0)"/>
</svg>
</div>
<ul>
<li>In ML Radford Neal <span class="citation"
data-cites="Neal:bayesian94">(Neal, 1994)</span> asked “what would
happen if you took <span class="math inline">\(h\rightarrow
\infty\)</span>?”</li>
</ul>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Page 37 of <a
href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s
1994 thesis</a>
</aside>
</section>
<section id="roughly-speaking" class="slide level2">
<h2>Roughly Speaking</h2>
<ul>
<li>Instead of <span class="math display">\[
\begin{align*}
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) &amp; = \alpha \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_i\right)^\top \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_j\right)\\
&amp; = \alpha \sum_k \phi\left(\mathbf{ w}^{(1)}_k, \mathbf{
x}_i\right) \phi\left(\mathbf{ w}^{(1)}_k, \mathbf{ x}_j\right)
\end{align*}
\]</span></li>
<li>Sample infinitely many from a prior density, <span
class="math inline">\(p(\mathbf{ w}^{(1)})\)</span>, <span
class="math display">\[
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) = \alpha \int
\phi\left(\mathbf{ w}^{(1)}, \mathbf{ x}_i\right) \phi\left(\mathbf{
w}^{(1)}, \mathbf{ x}_j\right) p(\mathbf{ w}^{(1)}) \text{d}\mathbf{
w}^{(1)}
\]</span></li>
<li>Also applies for non-Gaussian <span class="math inline">\(p(\mathbf{
w}^{(1)})\)</span> because of the <em>central limit theorem</em>.</li>
</ul>
</section>
<section id="simple-probabilistic-program" class="slide level2">
<h2>Simple Probabilistic Program</h2>
<ul>
<li><p>If <span class="math display">\[
\begin{align*}
\mathbf{ w}^{(1)} &amp; \sim p(\cdot)\\ \phi_i &amp; =
\phi\left(\mathbf{ w}^{(1)}, \mathbf{ x}_i\right),
\end{align*}
\]</span> has finite variance.</p></li>
<li><p>Then taking number of hidden units to infinity, is also a
Gaussian process.</p></li>
</ul>
</section>
<section id="further-reading" class="slide level2">
<h2>Further Reading</h2>
<ul>
<li><p>Chapter 2 of Neal’s thesis <span class="citation"
data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>Rest of Neal’s thesis. <span class="citation"
data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>David MacKay’s PhD thesis <span class="citation"
data-cites="MacKay:bayesian92">(MacKay, 1992)</span></p></li>
</ul>
<!-- ### Two Dimensional Gaussian Distribution -->
<!-- include{_ml/includes/two-d-gaussian.md} -->
</section>
<section id="distributions-over-functions" class="slide level2">
<h2>Distributions over Functions</h2>
</section>
<section id="sampling-a-function" class="slide level2">
<h2>Sampling a Function</h2>
<p><strong>Multi-variate Gaussians</strong></p>
<ul>
<li>We will consider a Gaussian with a particular structure of
covariance matrix.</li>
<li>Generate a single sample from this 25 dimensional Gaussian density,
<span class="math display">\[
\mathbf{ f}=\left[f_{1},f_{2}\dots f_{25}\right].
\]</span></li>
<li>We will plot these points against their index.</li>
</ul>
</section>
<section id="gaussian-distribution-sample" class="slide level2">
<h2>Gaussian Distribution Sample</h2>
<script>
showDivs(0, 'two_point_sample');
</script>
<p><small></small>
<input id="range-two_point_sample" type="range" min="0" max="8" value="0" onchange="setDivs('two_point_sample')" oninput="setDivs('two_point_sample')">
<button onclick="plusDivs(-1, 'two_point_sample')">❮</button>
<button onclick="plusDivs(1, 'two_point_sample')">❯</button></p>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample000.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample001.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample002.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample003.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample004.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample005.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample006.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample007.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample008.svg" width style=" ">
</object>
</div>
</section>
<section id="sampling-a-function-from-a-gaussian" class="slide level2">
<h2>Sampling a Function from a Gaussian</h2>
<script>
showDivs(9, 'two-point-sample');
</script>
<p><small></small>
<input id="range-two-point-sample" type="range" min="9" max="12" value="9" onchange="setDivs('two-point-sample')" oninput="setDivs('two-point-sample')">
<button onclick="plusDivs(-1, 'two-point-sample')">❮</button>
<button onclick="plusDivs(1, 'two-point-sample')">❯</button></p>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample000.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample001.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample002.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample003.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample004.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample005.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample006.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample007.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample008.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="joint-density-of-f_1-and-f_2" class="slide level2">
<h2>Joint Density of <span class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_2\)</span></h2>
</section>
<section id="prediction-of-f_2-from-f_1" class="slide level2">
<h2>Prediction of <span class="math inline">\(f_{2}\)</span> from <span
class="math inline">\(f_{1}\)</span></h2>
<script>
showDivs(9, 'two_point_sample2');
</script>
<p><small></small>
<input id="range-two_point_sample2" type="range" min="9" max="12" value="9" onchange="setDivs('two_point_sample2')" oninput="setDivs('two_point_sample2')">
<button onclick="plusDivs(-1, 'two_point_sample2')">❮</button>
<button onclick="plusDivs(1, 'two_point_sample2')">❯</button></p>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample009.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample010.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample011.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample012.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="uluru" class="slide level2">
<h2>Uluru</h2>
<div class="figure">
<div id="uluru-as-probability-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/799px-Uluru_Panorama.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Uluru, the sacred rock in Australia. If we think of it as a probability
density, viewing it from this side gives us one <em>marginal</em> from
the density. Figuratively speaking, slicing through the rock would give
a conditional density.
</aside>
</section>
<section id="prediction-with-correlated-gaussians" class="slide level2">
<h2>Prediction with Correlated Gaussians</h2>
<ul>
<li>Prediction of <span class="math inline">\(f_2\)</span> from <span
class="math inline">\(f_1\)</span> requires <em>conditional
density</em>.</li>
<li>Conditional density is <em>also</em> Gaussian. <span
class="math display">\[
p(f_2|f_1) = \mathcal{N}\left(f_2|\frac{k_{1, 2}}{k_{1, 1}}f_1, k_{2, 2}
- \frac{k_{1,2}^2}{k_{1,1}}\right)
\]</span> where covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} k_{1, 1} &amp; k_{1, 2}\\ k_{2, 1} &amp;
k_{2, 2}.\end{bmatrix}
\]</span></li>
</ul>
</section>
<section id="joint-density-of-f_1-and-f_8" class="slide level2">
<h2>Joint Density of <span class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_8\)</span></h2>
</section>
<section id="prediction-of-f_8-from-f_1" class="slide level2">
<h2>Prediction of <span class="math inline">\(f_{8}\)</span> from <span
class="math inline">\(f_{1}\)</span></h2>
<script>
showDivs(13, 'two_point_sample3');
</script>
<p><small></small>
<input id="range-two_point_sample3" type="range" min="13" max="17" value="13" onchange="setDivs('two_point_sample3')" oninput="setDivs('two_point_sample3')">
<button onclick="plusDivs(-1, 'two_point_sample3')">❮</button>
<button onclick="plusDivs(1, 'two_point_sample3')">❯</button></p>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample013.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample014.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample015.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample016.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/two_point_sample017.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="details" class="slide level2">
<h2>Details</h2>
<ul>
<li>The single contour of the Gaussian density represents the
<font color="yellow">joint distribution, <span
class="math inline">\(p(f_1, f_8)\)</span></font></li>
</ul>
<div class="fragment">
<ul>
<li>We observe a value for <font color="magenta"><span
class="math inline">\(f_1=-?\)</span></font></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Conditional density: <font color="cyan"><span
class="math inline">\(p(f_8|f_1=?)\)</span></font>.</li>
</ul>
</div>
</section>
<section id="prediction-with-correlated-gaussians-1"
class="slide level2">
<h2>Prediction with Correlated Gaussians</h2>
<ul>
<li><p>Prediction of <span class="math inline">\(\mathbf{ f}_*\)</span>
from <span class="math inline">\(\mathbf{ f}\)</span> requires
multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian.
<large> <span class="math display">\[
p(\mathbf{ f}_*|\mathbf{ f}) = {\mathcal{N}\left(\mathbf{
f}_*|\mathbf{K}_{*,\mathbf{ f}}\mathbf{K}_{\mathbf{ f},\mathbf{
f}}^{-1}\mathbf{ f},\mathbf{K}_{*,*}-\mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}_{\mathbf{ f},\mathbf{ f}}^{-1}\mathbf{K}_{\mathbf{
f},*}\right)}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} \mathbf{K}_{\mathbf{ f}, \mathbf{ f}} &amp;
\mathbf{K}_{*, \mathbf{ f}}\\ \mathbf{K}_{\mathbf{ f}, *} &amp;
\mathbf{K}_{*, *}\end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="prediction-with-correlated-gaussians-2"
class="slide level2">
<h2>Prediction with Correlated Gaussians</h2>
<ul>
<li><p>Prediction of <span class="math inline">\(\mathbf{ f}_*\)</span>
from <span class="math inline">\(\mathbf{ f}\)</span> requires
multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian.
<large> <span class="math display">\[
p(\mathbf{ f}_*|\mathbf{ f}) = {\mathcal{N}\left(\mathbf{
f}_*|\boldsymbol{ \mu},\boldsymbol{ \Sigma}\right)}
\]</span> <span class="math display">\[
\boldsymbol{ \mu}= \mathbf{K}_{*,\mathbf{ f}}\mathbf{K}_{\mathbf{
f},\mathbf{ f}}^{-1}\mathbf{ f}
\]</span> <span class="math display">\[
\boldsymbol{ \Sigma}= \mathbf{K}_{*,*}-\mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}_{\mathbf{ f},\mathbf{ f}}^{-1}\mathbf{K}_{\mathbf{ f},*}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} \mathbf{K}_{\mathbf{ f}, \mathbf{ f}} &amp;
\mathbf{K}_{*, \mathbf{ f}}\\ \mathbf{K}_{\mathbf{ f}, *} &amp;
\mathbf{K}_{*, *}\end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="key-object" class="slide level2">
<h2>Key Object</h2>
<ul>
<li>Covariance function, <span
class="math inline">\(\mathbf{K}\)</span></li>
<li>Determines properties of samples.</li>
<li>Function of <span class="math inline">\(\mathbf{X}\)</span>, <span
class="math display">\[k_{i,j} = k(\mathbf{ x}_i, \mathbf{
x}_j)\]</span></li>
</ul>
</section>
<section id="linear-algebra" class="slide level2">
<h2>Linear Algebra</h2>
<ul>
<li><p>Posterior mean <span class="math display">\[f_D(\mathbf{ x}_*) =
\mathbf{ k}(\mathbf{ x}_*, \mathbf{X}) \mathbf{K}^{-1}
\mathbf{ y}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* =
\mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{ f}, *}\]</span></p></li>
</ul>
</section>
<section id="linear-algebra-1" class="slide level2">
<h2>Linear Algebra</h2>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D(\mathbf{ x}_*) = \mathbf{
k}(\mathbf{ x}_*, \mathbf{X}) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* =
\mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{ f}, *}\]</span></p></li>
</ul>
</section>
<section id="exponentiated-quadratic-covariance" class="slide level2">
<h2>Exponentiated Quadratic Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\exp\left(-\frac{\left\Vert \mathbf{ x}-\mathbf{ x}^\prime
\right\Vert_2^2}{2\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The exponentiated quadratic covariance function.
</aside>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1896.
</aside>
</section>
<section id="alan-turing" class="slide level2">
<h2>Alan Turing</h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Alan Turing, in 1946 he was only 11 minutes slower than the winner of
the 1948 games. Would he have won a hypothetical games held in 1946?
Source:
<a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank">Alan
Turing Internet Scrapbook</a>.
</aside>
</section>
<section id="probability-winning-olympics" class="slide level2">
<h2>Probability Winning Olympics?</h2>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had
been held in 1946?</li>
</ul>
</section>
<section id="gaussian-process-fit" class="slide level2">
<h2>Gaussian Process Fit</h2>
</section>
<section id="olympic-marathon-data-gp" class="slide level2">
<h2>Olympic Marathon Data GP</h2>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/olympic-marathon-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Marathon data. The error bars are
too large, perhaps due to the outlier from 1904.
</aside>
</section>
<section id="learning-covariance-parameters" class="slide level2">
<h2>Learning Covariance Parameters</h2>
<p>Can we determine covariance parameters from the data?</p>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}{\det{\mathbf{K}}^{\frac{1}{2}}}}{\exp\left(-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\]</span></p>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}\color{yellow}{\det{\mathbf{K}}^{\frac{1}{2}}}}\color{cyan}{\exp\left(-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\end{aligned}
\]</span></p>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \log \mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=&amp;\color{yellow}{-\frac{1}{2}\log\det{\mathbf{K}}}\color{cyan}{-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}} \\ &amp;-\frac{n}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
E(\boldsymbol{ \theta}) =
\color{yellow}{\frac{1}{2}\log\det{\mathbf{K}}} +
\color{cyan}{\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}
\]</span></p>
</section>
<section id="capacity-control-through-the-determinant"
class="slide level2">
<h2>Capacity Control through the Determinant</h2>
<p>The parameters are <em>inside</em> the covariance function (matrix).
<span class="math display">\[k_{i, j} = k(\mathbf{ x}_i, \mathbf{ x}_j;
\boldsymbol{ \theta})\]</span></p>
</section>
<section id="eigendecomposition-of-covariance" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<p><span> <span class="math display">\[\mathbf{K}=
\mathbf{R}\boldsymbol{ \Lambda}^2 \mathbf{R}^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\boldsymbol{ \Lambda}\)</span> represents
distance on axes. <span class="math inline">\(\mathbf{R}\)</span> gives
rotation.
</td>
</tr>
</table>
</section>
<section id="eigendecomposition-of-covariance-1" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<ul>
<li><span class="math inline">\(\boldsymbol{ \Lambda}\)</span> is
<em>diagonal</em>, <span
class="math inline">\(\mathbf{R}^\top\mathbf{R}=
\mathbf{I}\)</span>.</li>
<li>Useful representation since <span
class="math inline">\(\det{\mathbf{K}} = \det{\boldsymbol{ \Lambda}^2} =
\det{\boldsymbol{ \Lambda}}^2\)</span>.</li>
</ul>
</section>
<section id="capacity-control-coloryellowlog-detmathbfk"
class="slide level2">
<h2>Capacity control: <span class="math inline">\(\color{yellow}{\log
\det{\mathbf{K}}}\)</span></h2>
<script>
showDivs(0, 'gp-optimise-determinant');
</script>
<p><small></small>
<input id="range-gp-optimise-determinant" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise-determinant')" oninput="setDivs('gp-optimise-determinant')">
<button onclick="plusDivs(-1, 'gp-optimise-determinant')">❮</button>
<button onclick="plusDivs(1, 'gp-optimise-determinant')">❯</button></p>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant000.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant001.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant002.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant003.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant004.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant005.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant006.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant007.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant008.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-determinant009.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="quadratic-data-fit" class="slide level2">
<h2>Quadratic Data Fit</h2>
</section>
<section id="data-fit-colorcyanfracmathbf-ytopmathbfk-1mathbf-y2"
class="slide level2">
<h2>Data Fit: <span class="math inline">\(\color{cyan}{\frac{\mathbf{
y}^\top\mathbf{K}^{-1}\mathbf{ y}}{2}}\)</span></h2>
<script>
showDivs(0, 'gp-optimise-quadratic');
</script>
<p><small></small>
<input id="range-gp-optimise-quadratic" type="range" min="0" max="2" value="0" onchange="setDivs('gp-optimise-quadratic')" oninput="setDivs('gp-optimise-quadratic')">
<button onclick="plusDivs(-1, 'gp-optimise-quadratic')">❮</button>
<button onclick="plusDivs(1, 'gp-optimise-quadratic')">❯</button></p>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-quadratic000.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-quadratic001.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
<div class="figure">
<div id="gp-optimise-quadratic-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The data fit term of the Gaussian process is a quadratic loss centered
around zero. This has eliptical contours, the principal axes of which
are given by the covariance matrix.
</aside>
</section>
<section
id="eboldsymbol-theta-coloryellowfrac12logdetmathbfkcolorcyanfracmathbf-ytopmathbfk-1mathbf-y2"
class="slide level2">
<h2><span class="math display">\[E(\boldsymbol{ \theta}) =
\color{yellow}{\frac{1}{2}\log\det{\mathbf{K}}}+\color{cyan}{\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}\]</span></h2>
</section>
<section id="data-fit-term" class="slide level2">
<h2>Data Fit Term</h2>
<script>
showDivs(0, 'gp-optimise');
</script>
<p><small></small>
<input id="range-gp-optimise" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise')" oninput="setDivs('gp-optimise')">
<button onclick="plusDivs(-1, 'gp-optimise')">❮</button>
<button onclick="plusDivs(1, 'gp-optimise')">❯</button></p>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise000.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise001.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise002.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise003.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise004.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise005.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise006.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise007.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise008.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise009.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise010.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise011.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise012.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise013.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise014.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise015.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise016.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise017.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise018.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise019.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise020.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/gp-optimise021.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="della-gatta-gene-data" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<ul>
<li>Given given expression levels in the form of a time series from
<span class="citation" data-cites="DellaGatta:direct08">Della Gatta et
al. (2008)</span>.</li>
</ul>
</section>
<section id="della-gatta-gene-data-1" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gene expression levels over time for a gene from data provided by <span
class="citation" data-cites="DellaGatta:direct08">Della Gatta et al.
(2008)</span>. We would like to understand whether there is signal in
the data, or we are only observing noise.
</aside>
</section>
<section id="gene-expression-example" class="slide level2">
<h2>Gene Expression Example</h2>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene
<span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and
Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Freddie Kalaitzis
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/deepnn/./slides/diagrams//people/freddie-kalaitzis.jpg" clip-path="url(#clip1)"/>
</svg>
</div>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The example is taken from the paper “A Simple Approach to Ranking
Differentially Expressed Gene Expression Time Courses through Gaussian
Process Regression.” <span class="citation"
data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.
</aside>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180"
class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
</section>
<section id="tp53-gene-data-gp" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale
parameter initialized to 50 minutes.
</aside>
</section>
<section id="tp53-gene-data-gp-1" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale
parameter initialized to 2000 minutes.
</aside>
</section>
<section id="tp53-gene-data-gp-2" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the noise
initialized low (standard deviation 0.1) and the time scale parameter
initialized to 20 minutes.
</aside>
</section>
<section id="multiple-optima" class="slide level2">
<h2>Multiple Optima</h2>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<!--
## Multiple Optima  {}



<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//gp/multiple-optima001.svg" width="" style=" "></object>-->
</section>
<section id="example-prediction-of-malaria-incidence-in-uganda"
class="slide level2">
<h2>Example: Prediction of Malaria Incidence in Uganda</h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Martin Mubangizi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/deepnn/./slides/diagrams//people/martin-mubangizi.png" clip-path="url(#clip2)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip3">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ricardo Andrade Pacecho
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/deepnn/./slides/diagrams//people/ricardo-andrade-pacheco.png" clip-path="url(#clip3)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip4">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
John Quinn
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/deepnn/./slides/diagrams//people/john-quinn.jpg" clip-path="url(#clip4)"/>
</svg>
</div>
<ul>
<li>Work with Ricardo Andrade Pacheco, John Quinn and Martin Mubangizi
(Makerere University, Uganda)</li>
<li>See <a href="http://air.ug/research.html">AI-DEV Group</a>.</li>
<li>See <a href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">UN
Global Pulse Disease Outbreaks Site</a></li>
</ul>
</section>
<section id="malaria-prediction-in-uganda" class="slide level2">
<h2>Malaria Prediction in Uganda</h2>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Ugandan districts. Data SRTM/NASA from <a
href="https://dds.cr.usgs.gov/srtm/version2_1"
class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.
</aside>
<div style="text-align:right">
<span class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>
</div>
</section>
<section id="kapchorwa-district" class="slide level2">
<h2>Kapchorwa District</h2>
<div class="figure">
<div id="kapchorwa-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/deepnn/./slides/diagrams//health/Kapchorwa_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Kapchorwa District, home district of Stephen Kiprotich.
</aside>
</section>
<section id="tororo-district" class="slide level2">
<h2>Tororo District</h2>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/deepnn/./slides/diagrams//health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Tororo district, where the sentinel site, Nagongera, is located.
</aside>
</section>
<section id="malaria-prediction-in-nagongera-sentinel-site"
class="slide level2">
<h2>Malaria Prediction in Nagongera (Sentinel Site)</h2>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/deepnn/./slides/diagrams//health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Sentinel and HMIS data along with rainfall and temperature for the
Nagongera sentinel station in the Tororo district.
</aside>
</section>
<section id="mubende-district" class="slide level2">
<h2>Mubende District</h2>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/deepnn/./slides/diagrams//health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Mubende District.
</aside>
</section>
<section id="malaria-prediction-in-uganda-1" class="slide level2">
<h2>Malaria Prediction in Uganda</h2>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Prediction of malaria incidence in Mubende.
</aside>
</section>
<section id="gp-school-at-makerere" class="slide level2">
<h2>GP School at Makerere</h2>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The project arose out of the Gaussian process summer school held at
Makerere in Kampala in 2013. The school led, in turn, to the Data
Science Africa initiative.
</aside>
</section>
<section id="kabarole-district" class="slide level2">
<h2>Kabarole District</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/deepnn/./slides/diagrams//health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Kabarole district in Uganda.
</aside>
</section>
<section id="early-warning-system" class="slide level2">
<h2>Early Warning System</h2>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Estimate of the current disease situation in the Kabarole district over
time. Estimate is constructed with a Gaussian process with an additive
covariance funciton.
</aside>
</section>
<section id="early-warning-systems" class="slide level2">
<h2>Early Warning Systems</h2>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The map of Ugandan districts with an overview of the Malaria situation
in each district.
</aside>
</section>
<section id="additive-covariance" class="slide level2">
<h2>Additive Covariance</h2>
<center>
<span class="math display">\[k_f(\mathbf{ x}, \mathbf{ x}^\prime) =
k_g(\mathbf{ x}, \mathbf{ x}^\prime) + k_h(\mathbf{ x}, \mathbf{
x}^\prime)\]</span>
</center>
<div class="figure">
<div id="add-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/add_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
An additive covariance function formed by combining a linear and an
exponentiated quadratic covariance functions.
</aside>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip5">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Aki Vehtari
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/deepnn/./slides/diagrams//people/aki-vehtari.jpg" clip-path="url(#clip5)"/>
</svg>
</div>
<div class="figure">
<div id="bialik-friday-the-13th-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
This is a retrospective analysis of US births by Aki Vehtari. The
challenges of forecasting. Even with seasonal and weekly effects removed
there are significant effects on holidays, weekends, etc.
</aside>
</section>
<section id="gelman-book" class="slide level2">
<h2>Gelman Book</h2>
<div class="figure">
<div id="bayesian-data-analysis-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Two different editions of Bayesian Data Analysis <span class="citation"
data-cites="Gelman:bayesian13">(Gelman et al., 2013)</span>.
</aside>
<div style="text-align:right">
<span class="citation" data-cites="Gelman:bayesian13">Gelman et al.
(2013)</span>
</div>
</section>
<section id="basis-function-covariance" class="slide level2">
<h2>Basis Function Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\boldsymbol{ \phi}(\mathbf{ x})^\top \boldsymbol{ \phi}(\mathbf{
x}^\prime)\]</span>
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/basis_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
A covariance function based on a non-linear basis given by <span
class="math inline">\(\boldsymbol{ \phi}(\mathbf{ x})\)</span>.
</aside>
</section>
<section id="brownian-covariance" class="slide level2">
<h2>Brownian Covariance</h2>
<center>
<span class="math display">\[k(t, t^\prime)=\alpha \min(t,
t^\prime)\]</span>
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/brownian_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Brownian motion covariance function.
</aside>
</section>
<section id="mlp-covariance" class="slide level2">
<h2>MLP Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\arcsin\left(\frac{w \mathbf{ x}^\top \mathbf{ x}^\prime +
b}{\sqrt{\left(w \mathbf{ x}^\top \mathbf{ x}+ b + 1\right)\left(w
\left.\mathbf{ x}^\prime\right.^\top \mathbf{ x}^\prime + b +
1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/mlp_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The multi-layer perceptron covariance function. This is derived by
considering the infinite limit of a neural network with probit
activation functions.
</aside>
</section>
<section id="relu-covariance" class="slide level2">
<h2>RELU Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha \arcsin\left(\frac{w \mathbf{ x}^\top \mathbf{ x}^\prime + b}
{\sqrt{\left(w \mathbf{ x}^\top \mathbf{ x}+ b + 1\right)
\left(w \left.\mathbf{ x}^\prime\right.^\top \mathbf{ x}^\prime + b +
1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="relu-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/relu_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/relu_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Rectified linear unit covariance function.
</aside>
</section>
<section id="sinc-covariance" class="slide level2">
<h2>Sinc Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\text{sinc}\left(\pi w r\right)\]</span>
</center>
<div class="figure">
<div id="sinc-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/sinc_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/sinc_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Sinc covariance function.
</aside>
</section>
<section id="polynomial-covariance" class="slide level2">
<h2>Polynomial Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha(w \mathbf{ x}^\top\mathbf{ x}^\prime + b)^d\]</span>
</center>
<div class="figure">
<div id="polynomial-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/polynomial_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/polynomial_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Polynomial covariance function.
</aside>
</section>
<section id="periodic-covariance" class="slide level2">
<h2>Periodic Covariance</h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha\exp\left(\frac{-2\sin(\pi rw)^2}{\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="periodic-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/periodic_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/periodic_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Periodic covariance function.
</aside>
</section>
<section id="linear-model-of-coregionalization-covariance"
class="slide level2">
<h2>Linear Model of Coregionalization Covariance</h2>
<center>
<span class="math display">\[k(i, j, \mathbf{ x}, \mathbf{ x}^\prime) =
b_{i,j} k(\mathbf{ x}, \mathbf{ x}^\prime)\]</span>
</center>
<div class="figure">
<div id="lmc-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/lmc_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/lmc_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Linear model of coregionalization covariance function.
</aside>
</section>
<section id="intrinsic-coregionalization-model-covariance"
class="slide level2">
<h2>Intrinsic Coregionalization Model Covariance</h2>
<center>
<span class="math display">\[k(i, j, \mathbf{ x}, \mathbf{ x}^\prime) =
b_{i,j} k(\mathbf{ x}, \mathbf{ x}^\prime)\]</span>
</center>
<div class="figure">
<div id="icm-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/icm_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/icm_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Intrinsic coregionalization model covariance function.
</aside>
</section>
<section id="extensions" class="slide level2">
<h2>Extensions</h2>
<ul>
<li>Approximate inference <span class="citation"
data-cites="Nickisch:approximations08">(e.g. Nickisch and Rasmussen,
2008)</span></li>
<li>Large Data <span class="citation"
data-cites="Thang:unifying17 Hensman:bigdata13">(e.g. Bui et al., 2017;
Hensman et al., n.d.)</span></li>
<li>Multiple outputs <span class="citation"
data-cites="Alvarez:vector12">(e.g. Álvarez et al., 2012)</span></li>
<li>Bayesian optimisation <span class="citation"
data-cites="Snoek:practical12">(e.g. Snoek et al., 2012)</span></li>
<li>Deep GPs <span class="citation" data-cites="Damianou:deepgp13">(e.g.
Damianou and Lawrence, 2013)</span></li>
</ul>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a
href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></p></li>
<li><p>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></p></li>
<li><p>blog posts:</p>
<p><a
href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What
is Machine Learning?</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Alvarez:vector12" class="csl-entry" role="listitem">
Álvarez, M.A., Rosasco, L., Lawrence, N.D., 2012. Kernels for
vector-valued functions: A review. Foundations and Trends in Machine
Learning 4, 195–266. <a
href="https://doi.org/10.1561/2200000036">https://doi.org/10.1561/2200000036</a>
</div>
<div id="ref-Andrade:consistent14" class="csl-entry" role="listitem">
Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.
Consistent mapping of government malaria records across a changing
territory delimitation. Malaria Journal 13. <a
href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a>
</div>
<div id="ref-Thang:unifying17" class="csl-entry" role="listitem">
Bui, T.D., Yan, J., Turner, R.E., 2017. <a
href="http://jmlr.org/papers/v18/16-603.html">A unifying framework for
<span>G</span>aussian process pseudo-point approximations using power
expectation propagation</a>. Journal of Machine Learning Research 18,
1–72.
</div>
<div id="ref-Cho:deep09" class="csl-entry" role="listitem">
Cho, Y., Saul, L.K., 2009. <a
href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel
methods for deep learning</a>, in: Bengio, Y., Schuurmans, D., Lafferty,
J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural
Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.
</div>
<div id="ref-Damianou:deepgp13" class="csl-entry" role="listitem">
Damianou, A., Lawrence, N.D., 2013. Deep <span>G</span>aussian
processes. pp. 207–215.
</div>
<div id="ref-DellaGatta:direct08" class="csl-entry" role="listitem">
Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D.,
Missero, C., Bernardo, D. di, 2008. Direct targets of the TRP63
transcription factor revealed by a combination of gene expression
profiling and reverse engineering. Genome Research 18, 939–948. <a
href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a>
</div>
<div id="ref-Gelman:bayesian13" class="csl-entry" role="listitem">
Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin,
D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.
</div>
<div id="ref-Hensman:bigdata13" class="csl-entry" role="listitem">
Hensman, J., Fusi, N., Lawrence, N.D., n.d. <span>G</span>aussian
processes for big data.
</div>
<div id="ref-Ioffe:batch15" class="csl-entry" role="listitem">
Ioffe, S., Szegedy, C., 2015. <a
href="http://proceedings.mlr.press/v37/ioffe15.html">Batch
normalization: Accelerating deep network training by reducing internal
covariate shift</a>, in: Bach, F., Blei, D. (Eds.), Proceedings of the
32nd International Conference on Machine Learning, Proceedings of
Machine Learning Research. PMLR, Lille, France, pp. 448–456.
</div>
<div id="ref-Kalaitzis:simple11" class="csl-entry" role="listitem">
Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking
differentially expressed gene expression time courses through
<span>Gaussian</span> process regression. BMC Bioinformatics 12. <a
href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a>
</div>
<div id="ref-MacKay:bayesian92" class="csl-entry" role="listitem">
MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis).
California Institute of Technology.
</div>
<div id="ref-McCulloch:neuron43" class="csl-entry" role="listitem">
McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas
immanent in nervous activity. Bulletin of Mathematical Biophysics 5,
115–133. <a
href="https://doi.org/10.1007/BF02478259">https://doi.org/10.1007/BF02478259</a>
</div>
<div id="ref-Mubangizi:malaria14" class="csl-entry" role="listitem">
Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,
N.D., 2014. Malaria surveillance with multiple data sources using
<span>Gaussian</span> process models, in: 1st International Conference
on the Use of Mobile <span>ICT</span> in Africa.
</div>
<div id="ref-Neal:bayesian94" class="csl-entry" role="listitem">
Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis).
Dept. of Computer Science, University of Toronto.
</div>
<div id="ref-Nickisch:approximations08" class="csl-entry"
role="listitem">
Nickisch, H., Rasmussen, C.E., 2008. Approximations for binary
<span>G</span>aussian process classification. Journal of Machine
Learning Research 6, 2035–2078.
</div>
<div id="ref-Snoek:practical12" class="csl-entry" role="listitem">
Snoek, J., Larochelle, H., Adams, R.P., 2012. <a
href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf">Practical
<span>B</span>ayesian optimization of machine learning algorithms</a>,
in: Pereira, F., Burges, C.J.C., Bottou, L., Weinberger, K.Q. (Eds.),
Advances in Neural Information Processing Systems 25. Curran Associates,
Inc., pp. 2951–2959.
</div>
<div id="ref-Steele:predictive12" class="csl-entry" role="listitem">
Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson,
E., Avital, I., Stojadinovic, A., 2012. Using machine-learned
<span>B</span>ayesian belief networks to predict perioperative risk of
clostridium difficile infection following colon surgery. Interact J Med
Res 1, e6. <a
href="https://doi.org/10.2196/ijmr.2131">https://doi.org/10.2196/ijmr.2131</a>
</div>
<div id="ref-Tipping:probpca99" class="csl-entry" role="listitem">
Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component
analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a
href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a>
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // Turns fragments on and off globally
        fragments: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
