{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R250: Gaussian Processes Introduction\n",
    "=====================================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge\n",
    "\n",
    "### 2020-01-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In this talk we give an introduction to Gaussian processes\n",
    "for students who are interested in working with GPs for the the R250\n",
    "module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pierre-Simon Laplace\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/Pierre-Simon_Laplace.png\" style=\"width:30%\">\n",
    "\n",
    "Figure: <i>Pierre-Simon Laplace 1749-1827.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_google_book(id='1YQPAAAAQAAJ', page='PR17-IA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Famously, Laplace considered the idea of a deterministic Universe, one\n",
    "in which the model is *known*, or as the below translation refers to it,\n",
    "“an intelligence which could comprehend all the forces by which nature\n",
    "is animated”. He speculates on an “intelligence” that can submit this\n",
    "vast data to analysis and propsoses that such an entity would be able to\n",
    "predict the future.\n",
    "\n",
    "> Given for one instant an intelligence which could comprehend all the\n",
    "> forces by which nature is animated and the respective situation of the\n",
    "> beings who compose it—an intelligence sufficiently vast to submit\n",
    "> these data to analysis—it would embrace in the same formulate the\n",
    "> movements of the greatest bodies of the universe and those of the\n",
    "> lightest atom; for it, nothing would be uncertain and the future, as\n",
    "> the past, would be present in its eyes.\n",
    "\n",
    "This notion is known as *Laplace’s demon* or *Laplace’s superman*.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/laplacesDeterminismEnglish.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Laplace’s determinsim in English translation.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace’s Gremlin\n",
    "-----------------\n",
    "\n",
    "Unfortunately, most analyses of his ideas stop at that point, whereas\n",
    "his real point is that such a notion is unreachable. Not so much\n",
    "*superman* as *strawman*. Just three pages later in the “Philosophical\n",
    "Essay on Probabilities” (Laplace, 1814), Laplace goes on to observe:\n",
    "\n",
    "> The curve described by a simple molecule of air or vapor is regulated\n",
    "> in a manner just as certain as the planetary orbits; the only\n",
    "> difference between them is that which comes from our ignorance.\n",
    ">\n",
    "> Probability is relative, in part to this ignorance, in part to our\n",
    "> knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_google_book(id='1YQPAAAAQAAJ', page='PR17-IA4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/philosophicaless00lapliala.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>To Laplace, determinism is a strawman. Ignorance of mechanism\n",
    "and data leads to uncertainty which should be dealt with through\n",
    "probability.</i>\n",
    "\n",
    "In other words, we can never make use of the idealistic deterministic\n",
    "Universe due to our ignorance about the world, Laplace’s suggestion, and\n",
    "focus in this essay is that we turn to probability to deal with this\n",
    "uncertainty. This is also our inspiration for using probability in\n",
    "machine learning. This is the true message of Laplace’s essay, not\n",
    "determinism, but the gremlin of uncertainty that emerges from our\n",
    "ignorance.\n",
    "\n",
    "The “forces by which nature is animated” is our *model*, the “situation\n",
    "of beings that compose it” is our *data* and the “intelligence\n",
    "sufficiently vast enough to submit these data to analysis” is our\n",
    "compute. The fly in the ointment is our *ignorance* about these aspects.\n",
    "And *probability* is the tool we use to incorporate this ignorance\n",
    "leading to uncertainty or *doubt* in our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Inference by Rejection Sampling\n",
    "----------------------------------------\n",
    "\n",
    "One view of Bayesian inference is to assume we are given a mechanism for\n",
    "generating samples, where we assume that mechanism is representing on\n",
    "accurate view on the way we believe the world works.\n",
    "\n",
    "This mechanism is known as our *prior* belief.\n",
    "\n",
    "We combine our prior belief with our observations of the real world by\n",
    "discarding all those samples that are inconsistent with our prior. The\n",
    "*likelihood* defines mathematically what we mean by inconsistent with\n",
    "the prior. The higher the noise level in the likelihood, the looser the\n",
    "notion of consistent.\n",
    "\n",
    "The samples that remain are considered to be samples from the\n",
    "*posterior*.\n",
    "\n",
    "This approach to Bayesian inference is closely related to two sampling\n",
    "techniques known as *rejection sampling* and *importance sampling*. It\n",
    "is realized in practice in an approach known as *approximate Bayesian\n",
    "computation* (ABC) or likelihood-free inference.\n",
    "\n",
    "In practice, the algorithm is often too slow to be practical, because\n",
    "most samples will be inconsistent with the data and as a result the\n",
    "mechanism has to be operated many times to obtain a few posterior\n",
    "samples.\n",
    "\n",
    "However, in the Gaussian process case, when the likelihood also assumes\n",
    "Gaussian noise, we can operate this mechanism mathematically, and obtain\n",
    "the posterior density *analytically*. This is the benefit of Gaussian\n",
    "processes.\n",
    "\n",
    "First we will load in two python functions for computing the covariance\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov,\n",
    "                     name='Exponentiated Quadratic',\n",
    "                     shortname='eq',                     \n",
    "                     lengthscale=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sample from a multivariate normal density (a multivariate\n",
    "Gaussian), using the covariance function as the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rejection_samples(kernel=kernel, \n",
    "    diagrams='./gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('gp_rejection_sample{sample:0>3}.png', \n",
    "                            directory='./gp', \n",
    "                            sample=IntSlider(1,1,5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample003.png\" style=\"width:100%\">\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample004.png\" style=\"width:100%\">\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample005.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>One view of Bayesian inference is we have a machine for\n",
    "generating samples (the *prior*), and we discard all samples\n",
    "inconsistent with our data, leaving the samples of interest (the\n",
    "*posterior*). This is a rejection sampling view of Bayesian inference.\n",
    "The Gaussian process allows us to do this analytically by multiplying\n",
    "the *prior* by the *likelihood*.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Machine Learning?\n",
    "=========================\n",
    "\n",
    "What is machine learning? At its most basic level machine learning is a\n",
    "combination of\n",
    "\n",
    "$$\\text{data} + \\text{model} \\stackrel{\\text{compute}}{\\rightarrow} \\text{prediction}$$\n",
    "\n",
    "where *data* is our observations. They can be actively or passively\n",
    "acquired (meta-data). The *model* contains our assumptions, based on\n",
    "previous experience. That experience can be other data, it can come from\n",
    "transfer learning, or it can merely be our beliefs about the\n",
    "regularities of the universe. In humans our models include our inductive\n",
    "biases. The *prediction* is an action to be taken or a categorization or\n",
    "a quality score. The reason that machine learning has become a mainstay\n",
    "of artificial intelligence is the importance of predictions in\n",
    "artificial intelligence. The data and the model are combined through\n",
    "computation.\n",
    "\n",
    "In practice we normally perform machine learning using two functions. To\n",
    "combine data with a model we typically make use of:\n",
    "\n",
    "**a prediction function** a function which is used to make the\n",
    "predictions. It includes our beliefs about the regularities of the\n",
    "universe, our assumptions about how the world works, e.g. smoothness,\n",
    "spatial similarities, temporal similarities.\n",
    "\n",
    "**an objective function** a function which defines the cost of\n",
    "misprediction. Typically it includes knowledge about the world’s\n",
    "generating processes (probabilistic objectives) or the costs we pay for\n",
    "mispredictions (empiricial risk minimization).\n",
    "\n",
    "The combination of data and model through the prediction function and\n",
    "the objective function leads to a *learning algorithm*. The class of\n",
    "prediction functions and objective functions we can make use of is\n",
    "restricted by the algorithms they lead to. If the prediction function or\n",
    "the objective function are too complex, then it can be difficult to find\n",
    "an appropriate learning algorithm. Much of the acdemic field of machine\n",
    "learning is the quest for new learning algorithms that allow us to bring\n",
    "different types of models and data together.\n",
    "\n",
    "A useful reference for state of the art in machine learning is the UK\n",
    "Royal Society Report, [Machine Learning: Power and Promise of Computers\n",
    "that Learn by\n",
    "Example](https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf).\n",
    "\n",
    "You can also check my post blog post on [What is Machine\n",
    "Learning?](http://inverseprobability.com/2017/07/17/what-is-machine-learning)..\n",
    "\n",
    "In practice, we normally also have uncertainty associated with these\n",
    "functions. Uncertainty in the prediction function arises from\n",
    "\n",
    "1.  scarcity of training data and\n",
    "2.  mismatch between the set of prediction functions we choose and all\n",
    "    possible prediction functions.\n",
    "\n",
    "There are also challenges around specification of the objective\n",
    "function, but for we will save those for another day. For the moment,\n",
    "let us focus on the prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks and Prediction Functions\n",
    "----------------------------------------\n",
    "\n",
    "Neural networks are adaptive non-linear function models. Originally,\n",
    "they were studied (by McCulloch and Pitts (McCulloch and Pitts, 1943))\n",
    "as simple models for neurons, but over the last decade they have become\n",
    "popular because they are a flexible approach to modelling complex data.\n",
    "A particular characteristic of neural network models is that they can be\n",
    "composed to form highly complex functions which encode many of our\n",
    "expectations of the real world. They allow us to encode our assumptions\n",
    "about how the world works.\n",
    "\n",
    "We will return to composition later, but for the moment, let’s focus on\n",
    "a one hidden layer neural network. We are interested in the prediction\n",
    "function, so we’ll ignore the objective function (which is often called\n",
    "an error function) for the moment, and just describe the mathematical\n",
    "object of interest\n",
    "\n",
    "$$\n",
    "f(\\mathbf{ x}) = \\mathbf{W}^\\top \\boldsymbol{ \\phi}(\\mathbf{V}, \\mathbf{ x})\n",
    "$$\n",
    "\n",
    "Where in this case $f(\\cdot)$ is a scalar function with vector inputs,\n",
    "and $\\boldsymbol{ \\phi}(\\cdot)$ is a vector function with vector inputs.\n",
    "The dimensionality of the vector function is known as the number of\n",
    "hidden units, or the number of neurons. The elements of this vector\n",
    "function are known as the *activation* function of the neural network\n",
    "and $\\mathbf{V}$ are the parameters of the activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relations with Classical Statistics\n",
    "-----------------------------------\n",
    "\n",
    "In statistics activation functions are traditionally known as *basis\n",
    "functions*. And we would think of this as a *linear model*. It’s doesn’t\n",
    "make linear predictions, but it’s linear because in statistics\n",
    "estimation focuses on the parameters, $\\mathbf{W}$, not the parameters,\n",
    "$\\mathbf{V}$. The linear model terminology refers to the fact that the\n",
    "model is *linear in the parameters*, but it is *not* linear in the data\n",
    "unless the activation functions are chosen to be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Basis Functions\n",
    "------------------------\n",
    "\n",
    "The first difference in the (early) neural network literature to the\n",
    "classical statistical literature is the decision to optimize these\n",
    "parameters, $\\mathbf{V}$, as well as the parameters, $\\mathbf{W}$ (which\n",
    "would normally be denoted in statistics by $\\boldsymbol{\\beta}$)[1].\n",
    "\n",
    "[1] In classical statistics we often interpret these parameters,\n",
    "$\\beta$, whereas in machine learning we are normally more interested in\n",
    "the result of the prediction, and less in the prediction. Although this\n",
    "is changing with more need for accountability. In honour of this I\n",
    "normally use $\\boldsymbol{\\beta}$ when I care about the value of these\n",
    "parameters, and $\\mathbf{ w}$ when I care more about the quality of the\n",
    "prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Basis Functions\n",
    "--------------------------\n",
    "\n",
    "We’re going to go revisit that decision, and follow the path of Radford\n",
    "Neal (Neal, 1994) who, inspired by work of David MacKay (MacKay, 1992)\n",
    "and others did his PhD thesis on Bayesian Neural Networks. If we take a\n",
    "Bayesian approach to parameter inference (note I am using inference here\n",
    "in the classical sense, not in the sense of prediction of test data,\n",
    "which seems to be a newer usage), then we don’t wish to fit parameters\n",
    "at all, rather we wish to integrate them away and understand the family\n",
    "of functions that the model describes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic Modelling\n",
    "-----------------------\n",
    "\n",
    "This Bayesian approach is designed to deal with uncertainty arising from\n",
    "fitting our prediction function to the data we have, a reduced data set.\n",
    "\n",
    "The Bayesian approach can be derived from a broader understanding of\n",
    "what our objective is. If we accept that we can jointly represent all\n",
    "things that happen in the world with a probability distribution, then we\n",
    "can interogate that probability to make predictions. So, if we are\n",
    "interested in predictions, $y_*$ at future points input locations of\n",
    "interest, $\\mathbf{ x}_*$ given previously training data, $\\mathbf{ y}$\n",
    "and corresponding inputs, $\\mathbf{X}$, then we are really interogating\n",
    "the following probability density, $$\n",
    "p(y_*|\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}_*),\n",
    "$$ there is nothing controversial here, as long as you accept that you\n",
    "have a good joint model of the world around you that relates test data\n",
    "to training data, $p(y_*, \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}_*)$ then\n",
    "this conditional distribution can be recovered through standard rules of\n",
    "probability\n",
    "($\\text{data} + \\text{model} \\rightarrow \\text{prediction}$).\n",
    "\n",
    "We can construct this joint density through the use of the following\n",
    "decomposition: $$\n",
    "p(y_*|\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}_*) = \\int p(y_*|\\mathbf{ x}_*, \\mathbf{W}) p(\\mathbf{W}| \\mathbf{ y}, \\mathbf{X}) \\text{d} \\mathbf{W}\n",
    "$$\n",
    "\n",
    "where, for convenience, we are assuming *all* the parameters of the\n",
    "model are now represented by $\\boldsymbol{ \\theta}$ (which contains\n",
    "$\\mathbf{W}$ and $\\mathbf{V}$) and\n",
    "$p(\\boldsymbol{ \\theta}| \\mathbf{ y}, \\mathbf{X})$ is recognised as the\n",
    "posterior density of the parameters given data and\n",
    "$p(y_*|\\mathbf{ x}_*, \\boldsymbol{ \\theta})$ is the *likelihood* of an\n",
    "individual test data point given the parameters.\n",
    "\n",
    "The likelihood of the data is normally assumed to be independent across\n",
    "the parameters, $$\n",
    "p(\\mathbf{ y}|\\mathbf{X}, \\mathbf{W}) = \\prod_{i=1}^np(y_i|\\mathbf{ x}_i, \\mathbf{W}),$$\n",
    "\n",
    "and if that is so, it is easy to extend our predictions across all\n",
    "future, potential, locations, $$\n",
    "p(\\mathbf{ y}_*|\\mathbf{ y}, \\mathbf{X}, \\mathbf{X}_*) = \\int p(\\mathbf{ y}_*|\\mathbf{X}_*, \\boldsymbol{ \\theta}) p(\\boldsymbol{ \\theta}| \\mathbf{ y}, \\mathbf{X}) \\text{d} \\boldsymbol{ \\theta}.\n",
    "$$\n",
    "\n",
    "The likelihood is also where the *prediction function* is incorporated.\n",
    "For example in the regression case, we consider an objective based\n",
    "around the Gaussian density, $$\n",
    "p(y_i | f(\\mathbf{ x}_i)) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{\\left(y_i - f(\\mathbf{ x}_i)\\right)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "In short, that is the classical approach to probabilistic inference, and\n",
    "all approaches to Bayesian neural networks fall within this path. For a\n",
    "deep probabilistic model, we can simply take this one stage further and\n",
    "place a probability distribution over the input locations, $$\n",
    "p(\\mathbf{ y}_*|\\mathbf{ y}) = \\int p(\\mathbf{ y}_*|\\mathbf{X}_*, \\boldsymbol{ \\theta}) p(\\boldsymbol{ \\theta}| \\mathbf{ y}, \\mathbf{X}) p(\\mathbf{X}) p(\\mathbf{X}_*) \\text{d} \\boldsymbol{ \\theta}\\text{d} \\mathbf{X}\\text{d}\\mathbf{X}_*\n",
    "$$ and we have *unsupervised learning* (from where we can get deep\n",
    "generative models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical Models\n",
    "----------------\n",
    "\n",
    "One way of representing a joint distribution is to consider conditional\n",
    "dependencies between data. Conditional dependencies allow us to\n",
    "factorize the distribution. For example, a Markov chain is a\n",
    "factorization of a distribution into components that represent the\n",
    "conditional relationships between points that are neighboring, often in\n",
    "time or space. It can be decomposed in the following form.\n",
    "$$p(\\mathbf{ y}) = p(y_n| y_{n-1}) p(y_{n-1}|y_{n-2}) \\dots p(y_{2} | y_{1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica']}, size=30)\n",
    "rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = daft.PGM(shape=[3, 1],\n",
    "               origin=[0, 0], \n",
    "               grid_unit=5, \n",
    "               node_unit=1.9, \n",
    "               observed_style='shaded',\n",
    "              line_width=3)\n",
    "\n",
    "\n",
    "pgm.add_node(daft.Node(\"y_1\", r\"$y_1$\", 0.5, 0.5, fixed=False))\n",
    "pgm.add_node(daft.Node(\"y_2\", r\"$y_2$\", 1.5, 0.5, fixed=False))\n",
    "pgm.add_node(daft.Node(\"y_3\", r\"$y_3$\", 2.5, 0.5, fixed=False))\n",
    "pgm.add_edge(\"y_1\", \"y_2\")\n",
    "pgm.add_edge(\"y_2\", \"y_3\")\n",
    "\n",
    "pgm.render().figure.savefig(\"./ml/markov.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/markov.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Markov chain is a simple form of probabilistic graphical\n",
    "model providing a particular decomposition of the joint density.</i>\n",
    "\n",
    "By specifying conditional independencies we can reduce the\n",
    "parameterization required for our data, instead of directly specifying\n",
    "the parameters of the joint distribution, we can specify each set of\n",
    "parameters of the conditonal independently. This can also give an\n",
    "advantage in terms of interpretability. Understanding a conditional\n",
    "independence structure gives a structured understanding of data. If\n",
    "developed correctly, according to causal methodology, it can even inform\n",
    "how we should intervene in the system to drive a desired result (Pearl,\n",
    "1995).\n",
    "\n",
    "However, a challenge arises when the data becomes more complex. Consider\n",
    "the graphical model shown below, used to predict the perioperative risk\n",
    "of *C Difficile* infection following colon surgery (Steele et al.,\n",
    "2012).\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/bayes-net-diagnosis.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>A probabilistic directed graph used to predict the\n",
    "perioperative risk of *C Difficile* infection following colon surgery.\n",
    "When these models have good predictive performance they are often\n",
    "difficult to interpret. This may be due to the limited representation\n",
    "capability of the conditional densities in the model.</i>\n",
    "\n",
    "To capture the complexity in the interelationship between the data, the\n",
    "graph itself becomes more complex, and less interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Inference\n",
    "--------------------\n",
    "\n",
    "As far as combining our data and our model to form our prediction, the\n",
    "devil is in the detail. While everything is easy to write in terms of\n",
    "probability densities, as we move from $\\text{data}$ and $\\text{model}$\n",
    "to $\\text{prediction}$ there is that simple\n",
    "$\\stackrel{\\text{compute}}{\\rightarrow}$ sign, which is now burying a\n",
    "wealth of difficulties. Each integral sign above is a high dimensional\n",
    "integral which will typically need approximation. Approximations also\n",
    "come with computational demands. As we consider more complex classes of\n",
    "functions, the challenges around the integrals become harder and\n",
    "prediction of future test data given our model and the data becomes so\n",
    "involved as to be impractical or impossible.\n",
    "\n",
    "Statisticians realized these challenges early on, indeed, so early that\n",
    "they were actually physicists, both Laplace and Gauss worked on models\n",
    "such as this, in Gauss’s case he made his career on prediction of the\n",
    "location of the lost planet (later reclassified as a asteroid, then\n",
    "dwarf planet), Ceres. Gauss and Laplace made use of maximum a posteriori\n",
    "estimates for simplifying their computations and Laplace developed\n",
    "Laplace’s method (and invented the Gaussian density) to expand around\n",
    "that mode. But classical statistics needs better guarantees around model\n",
    "performance and interpretation, and as a result has focussed more on the\n",
    "*linear* model implied by $$\n",
    "  f(\\mathbf{ x}) = \\left.\\mathbf{ w}^{(2)}\\right.^\\top \\boldsymbol{ \\phi}(\\mathbf{W}_1, \\mathbf{ x})\n",
    "  $$\n",
    "\n",
    "$$\n",
    "  \\mathbf{ w}^{(2)} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{C}\\right).\n",
    "  $$\n",
    "\n",
    "The Gaussian likelihood given above implies that the data observation is\n",
    "related to the function by noise corruption so we have, $$\n",
    "  y_i = f(\\mathbf{ x}_i) + \\epsilon_i,\n",
    "  $$ where $$\n",
    "  \\epsilon_i \\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\n",
    "  $$\n",
    "\n",
    "and while normally integrating over high dimensional parameter vectors\n",
    "is highly complex, here it is *trivial*. That is because of a property\n",
    "of the multivariate Gaussian.\n",
    "\n",
    "Gaussian processes are initially of interest because\n",
    "\n",
    "1.  linear Gaussian models are easier to deal with\n",
    "2.  Even the parameters *within* the process can be handled, by\n",
    "    considering a particular limit.\n",
    "\n",
    "Let’s first of all review the properties of the multivariate Gaussian\n",
    "distribution that make linear Gaussian models easier to deal with. We’ll\n",
    "return to the, perhaps surprising, result on the parameters within the\n",
    "nonlinearity, $\\boldsymbol{ \\theta}$, shortly.\n",
    "\n",
    "To work with linear Gaussian models, to find the marginal likelihood all\n",
    "you need to know is the following rules. If $$\n",
    "\\mathbf{ y}= \\mathbf{W}\\mathbf{ x}+ \\boldsymbol{ \\epsilon},\n",
    "$$ where $\\mathbf{ y}$, $\\mathbf{ x}$ and $\\boldsymbol{ \\epsilon}$ are\n",
    "vectors and we assume that $\\mathbf{ x}$ and $\\boldsymbol{ \\epsilon}$\n",
    "are drawn from multivariate Gaussians, $$\n",
    "\\begin{align}\n",
    "\\mathbf{ x}& \\sim \\mathcal{N}\\left(\\boldsymbol{ \\mu},\\mathbf{C}\\right)\\\\\n",
    "\\boldsymbol{ \\epsilon}& \\sim \\mathcal{N}\\left(\\mathbf{0},\\boldsymbol{ \\Sigma}\\right)\n",
    "\\end{align}\n",
    "$$ then we know that $\\mathbf{ y}$ is also drawn from a multivariate\n",
    "Gaussian with, $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{W}\\boldsymbol{ \\mu},\\mathbf{W}\\mathbf{C}\\mathbf{W}^\\top + \\boldsymbol{ \\Sigma}\\right).\n",
    "$$\n",
    "\n",
    "With appropriately defined covariance, $\\boldsymbol{ \\Sigma}$, this is\n",
    "actually the marginal likelihood for Factor Analysis, or Probabilistic\n",
    "Principal Component Analysis (Tipping and Bishop, 1999), because we\n",
    "integrated out the inputs (or *latent* variables they would be called in\n",
    "that case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model Overview\n",
    "---------------------\n",
    "\n",
    "However, we are focussing on what happens in models which are non-linear\n",
    "in the inputs, whereas the above would be *linear* in the inputs. To\n",
    "consider these, we introduce a matrix, called the design matrix. We set\n",
    "each activation function computed at each data point to be $$\n",
    "\\phi_{i,j} = \\phi(\\mathbf{ w}^{(1)}_{j}, \\mathbf{ x}_{i})\n",
    "$$ and define the matrix of activations (known as the *design matrix* in\n",
    "statistics) to be, $$\n",
    "\\boldsymbol{ \\Phi}= \n",
    "\\begin{bmatrix}\n",
    "\\phi_{1, 1} & \\phi_{1, 2} & \\dots & \\phi_{1, h} \\\\\n",
    "\\phi_{1, 2} & \\phi_{1, 2} & \\dots & \\phi_{1, n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_{n, 1} & \\phi_{n, 2} & \\dots & \\phi_{n, h}\n",
    "\\end{bmatrix}.\n",
    "$$ By convention this matrix always has $n$ rows and $h$ columns, now if\n",
    "we define the vector of all noise corruptions,\n",
    "$\\boldsymbol{ \\epsilon}= \\left[\\epsilon_1, \\dots \\epsilon_n\\right]^\\top$.\n",
    "\n",
    "If we define the prior distribution over the vector $\\mathbf{ w}$ to be\n",
    "Gaussian, $$\n",
    "\\mathbf{ w}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha\\mathbf{I}\\right),\n",
    "$$ then we can use rules of multivariate Gaussians to see that, $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top + \\sigma^2 \\mathbf{I}\\right).\n",
    "$$\n",
    "\n",
    "In other words, our training data is distributed as a multivariate\n",
    "Gaussian, with zero mean and a covariance given by $$\n",
    "\\mathbf{K}= \\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top + \\sigma^2 \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "This is an $n\\times n$ size matrix. Its elements are in the form of a\n",
    "function. The maths shows that any element, index by $i$ and $j$, is a\n",
    "function *only* of inputs associated with data points $i$ and $j$,\n",
    "$\\mathbf{ y}_i$, $\\mathbf{ y}_j$.\n",
    "$k_{i,j} = k\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right)$\n",
    "\n",
    "If we look at the portion of this function associated only with\n",
    "$f(\\cdot)$, i.e. we remove the noise, then we can write down the\n",
    "covariance associated with our neural network, $$\n",
    "k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) = \\alpha \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_i\\right)^\\top \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_j\\right)\n",
    "$$ so the elements of the covariance or *kernel* matrix are formed by\n",
    "inner products of the rows of the *design matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process\n",
    "----------------\n",
    "\n",
    "This is the essence of a Gaussian process. Instead of making assumptions\n",
    "about our density over each data point, $y_i$ as i.i.d. we make a joint\n",
    "Gaussian assumption over our data. The covariance matrix is now a\n",
    "function of both the parameters of the activation function,\n",
    "$\\mathbf{V}$, and the input variables, $\\mathbf{X}$. This comes about\n",
    "through integrating out the parameters of the model, $\\mathbf{ w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "---------------\n",
    "\n",
    "We can basically put anything inside the basis functions, and many\n",
    "people do. These can be deep kernels (Cho and Saul, 2009) or we can\n",
    "learn the parameters of a convolutional neural network inside there.\n",
    "\n",
    "Viewing a neural network in this way is also what allows us to beform\n",
    "sensible *batch* normalizations (Ioffe and Szegedy, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-degenerate Gaussian Processes\n",
    "---------------------------------\n",
    "\n",
    "The process described above is degenerate. The covariance function is of\n",
    "rank at most $h$ and since the theoretical amount of data could always\n",
    "increase $n\\rightarrow \\infty$, the covariance function is not full\n",
    "rank. This means as we increase the amount of data to infinity, there\n",
    "will come a point where we can’t normalize the process because the\n",
    "multivariate Gaussian has the form, $$\n",
    "\\mathcal{N}\\left(\\mathbf{ f}|\\mathbf{0},\\mathbf{K}\\right) = \\frac{1}{\\left(2\\pi\\right)^{\\frac{n}{2}}\\det{\\mathbf{K}}^\\frac{1}{2}} \\exp\\left(-\\frac{\\mathbf{ f}^\\top\\mathbf{K}\\mathbf{ f}}{2}\\right)\n",
    "$$ and a non-degenerate kernel matrix leads to $\\det{\\mathbf{K}} = 0$\n",
    "defeating the normalization (it’s equivalent to finding a projection in\n",
    "the high dimensional Gaussian where the variance of the the resulting\n",
    "univariate Gaussian is zero, i.e. there is a null space on the\n",
    "covariance, or alternatively you can imagine there are one or more\n",
    "directions where the Gaussian has become the delta function).\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip0\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Radford Neal\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/radford-neal.jpg\" clip-path=\"url(#clip0)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "In the machine learning field, it was Radford Neal (Neal, 1994) that\n",
    "realized the potential of the next step. In his 1994 thesis, he was\n",
    "considering Bayesian neural networks, of the type we described above,\n",
    "and in considered what would happen if you took the number of hidden\n",
    "nodes, or neurons, to infinity, i.e. $h\\rightarrow \\infty$.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/neal-infinite-priors.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Page 37 of [Radford Neal’s 1994\n",
    "thesis](http://www.cs.toronto.edu/~radford/ftp/thesis.pdf)</i>\n",
    "\n",
    "In loose terms, what Radford considers is what happens to the elements\n",
    "of the covariance function, $$\n",
    "  \\begin{align*}\n",
    "  k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) & = \\alpha \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_i\\right)^\\top \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_j\\right)\\\\\n",
    "  & = \\alpha \\sum_k \\phi\\left(\\mathbf{ w}^{(1)}_k, \\mathbf{ x}_i\\right) \\phi\\left(\\mathbf{ w}^{(1)}_k, \\mathbf{ x}_j\\right)\n",
    "  \\end{align*}\n",
    "  $$ if instead of considering a finite number you sample infinitely\n",
    "many of these activation functions, sampling parameters from a prior\n",
    "density, $p(\\mathbf{ v})$, for each one, $$\n",
    "k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) = \\alpha \\int \\phi\\left(\\mathbf{ w}^{(1)}, \\mathbf{ x}_i\\right) \\phi\\left(\\mathbf{ w}^{(1)}, \\mathbf{ x}_j\\right) p(\\mathbf{ w}^{(1)}) \\text{d}\\mathbf{ w}^{(1)}\n",
    "$$ And that’s not *only* for Gaussian $p(\\mathbf{ v})$. In fact this\n",
    "result holds for a range of activations, and a range of prior densities\n",
    "because of the *central limit theorem*.\n",
    "\n",
    "To write it in the form of a probabilistic program, as long as the\n",
    "distribution for $\\phi_i$ implied by this short probabilistic program,\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  \\mathbf{ v}& \\sim p(\\cdot)\\\\\n",
    "  \\phi_i & = \\phi\\left(\\mathbf{ v}, \\mathbf{ x}_i\\right), \n",
    "  \\end{align*}\n",
    "  $$ has finite variance, then the result of taking the number of hidden\n",
    "units to infinity, with appropriate scaling, is also a Gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading\n",
    "---------------\n",
    "\n",
    "To understand this argument in more detail, I highly recommend reading\n",
    "chapter 2 of Neal’s thesis (Neal, 1994), which remains easy to read and\n",
    "clear today. Indeed, for readers interested in Bayesian neural networks,\n",
    "both Raford Neal’s and David MacKay’s PhD thesis (MacKay, 1992) remain\n",
    "essential reading. Both theses embody a clarity of thought, and an\n",
    "ability to weave together threads from different fields that was the\n",
    "business of machine learning in the 1990s. Radford and David were also\n",
    "pioneers in making their software widely available and publishing\n",
    "material on the web.\n",
    "\n",
    "<!-- ### Two Dimensional Gaussian Distribution -->\n",
    "<!-- include{_ml/includes/two-d-gaussian.md} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(4949)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling a Function\n",
    "-------------------\n",
    "\n",
    "We will consider a Gaussian distribution with a particular structure of\n",
    "covariance matrix. We will generate *one* sample from a 25-dimensional\n",
    "Gaussian density. $$\n",
    "\\mathbf{ f}=\\left[f_{1},f_{2}\\dots f_{25}\\right].\n",
    "$$ in the figure below we plot these data on the $y$-axis against their\n",
    "*indices* on the $x$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s polynomial_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s exponentiated_quadratic mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "from mlai import Kernel, exponentiated_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel=Kernel(function=exponentiated_quadratic, lengthscale=0.5)\n",
    "plot.two_point_sample(kernel.K, diagrams='./gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', './gp', sample=IntSlider(0, 0, 8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample008.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A 25 dimensional correlated random variable (values ploted\n",
    "against index)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling a Function from a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(0, 0, 8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample001.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_2$ along with the\n",
    "conditional distribution of $f_2$ given $f_1$</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Density of $f_1$ and $f_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(9, 9, 12, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample012.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_2$ along with the\n",
    "conditional distribution of $f_2$ given $f_1$</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uluru\n",
    "-----\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/799px-Uluru_Panorama.jpg\" style=\"width:\">\n",
    "\n",
    "Figure: <i>Uluru, the sacred rock in Australia. If we think of it as a\n",
    "probability density, viewing it from this side gives us one *marginal*\n",
    "from the density. Figuratively speaking, slicing through the rock would\n",
    "give a conditional density.</i>\n",
    "\n",
    "When viewing these contour plots, I sometimes find it helpful to think\n",
    "of Uluru, the prominent rock formation in Australia. The rock rises\n",
    "above the surface of the plane, just like a probability density rising\n",
    "above the zero line. The rock is three dimensional, but when we view\n",
    "Uluru from the classical position, we are looking at one side of it.\n",
    "This is equivalent to viewing the marginal density.\n",
    "\n",
    "The joint density can be viewed from above, using contours. The\n",
    "conditional density is equivalent to *slicing* the rock. Uluru is a holy\n",
    "rock, so this has to be an imaginary slice. Imagine we cut down a\n",
    "vertical plane orthogonal to our view point (e.g. coming across our view\n",
    "point). This would give a profile of the rock, which when renormalized,\n",
    "would give us the conditional distribution, the value of conditioning\n",
    "would be the location of the slice in the direction we are facing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction with Correlated Gaussians\n",
    "------------------------------------\n",
    "\n",
    "Of course in practice, rather than manipulating mountains physically,\n",
    "the advantage of the Gaussian density is that we can perform these\n",
    "manipulations mathematically.\n",
    "\n",
    "Prediction of $f_2$ given $f_1$ requires the *conditional density*,\n",
    "$p(f_2|f_1)$.Another remarkable property of the Gaussian density is that\n",
    "this conditional distribution is *also* guaranteed to be a Gaussian\n",
    "density. It has the form, $$\n",
    "p(f_2|f_1) = \\mathcal{N}\\left(f_2|\\frac{k_{1, 2}}{k_{1, 1}}f_1, k_{2, 2} - \\frac{k_{1,2}^2}{k_{1,1}}\\right)\n",
    "$$where we have assumed that the covariance of the original joint\n",
    "density was given by $$\n",
    "\\mathbf{K}= \\begin{bmatrix} k_{1, 1} & k_{1, 2}\\\\ k_{2, 1} & k_{2, 2}.\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using these formulae we can determine the conditional density for any of\n",
    "the elements of our vector $\\mathbf{ f}$. For example, the variable\n",
    "$f_8$ is less correlated with $f_1$ than $f_2$. If we consider this\n",
    "variable we see the conditional density is more diffuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Density of $f_1$ and $f_8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(13, 13, 17, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample013.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sample from the joint Gaussian model, points indexed by 1 and\n",
    "8 highlighted.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of $f_{8}$ from $f_{1}$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/two_point_sample017.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_8$ along with the\n",
    "conditional distribution of $f_8$ given $f_1$</i>\n",
    "\n",
    "-   Covariance function, $\\mathbf{K}$\n",
    "\n",
    "-   Determines properties of samples.\n",
    "\n",
    "-   Function of $\\mathbf{X}$,\n",
    "    $$k_{i,j} = k(\\mathbf{ x}_i, \\mathbf{ x}_j)$$\n",
    "\n",
    "-   Posterior mean\n",
    "    $$f_D(\\mathbf{ x}_*) = \\mathbf{ k}(\\mathbf{ x}_*, \\mathbf{X}) \\mathbf{K}^{-1}\n",
    "    \\mathbf{ y}$$\n",
    "\n",
    "-   Posterior covariance\n",
    "    $$\\mathbf{C}_* = \\mathbf{K}_{*,*} - \\mathbf{K}_{*,\\mathbf{ f}}\n",
    "    \\mathbf{K}^{-1} \\mathbf{K}_{\\mathbf{ f}, *}$$\n",
    "\n",
    "-   Posterior mean\n",
    "\n",
    "    $$f_D(\\mathbf{ x}_*) = \\mathbf{ k}(\\mathbf{ x}_*, \\mathbf{X}) \\boldsymbol{\\alpha}$$\n",
    "\n",
    "-   Posterior covariance\n",
    "    $$\\mathbf{C}_* = \\mathbf{K}_{*,*} - \\mathbf{K}_{*,\\mathbf{ f}}\n",
    "    \\mathbf{K}^{-1} \\mathbf{K}_{\\mathbf{ f}, *}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiated Quadratic Covariance\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov,\n",
    "                     name='Exponentiated Quadratic',\n",
    "                     shortname='eq',                     \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\exp\\left(-\\frac{\\ltwoNorm{\\inputVector-\\inputVector^\\prime}^2}{2\\lengthScale^2}\\right)',\n",
    "                     lengthscale=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_func(kernel=kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponentiated quadratic covariance, also known as the Gaussian\n",
    "covariance or the RBF covariance and the squared exponential. Covariance\n",
    "between two points is related to the negative exponential of the squared\n",
    "distnace between those points. This covariance function can be derived\n",
    "in a few different ways: as the infinite limit of a radial basis\n",
    "function neural network, as diffusion in the heat equation, as a\n",
    "Gaussian filter in *Fourier space* or as the composition as a series of\n",
    "linear filters applied to a base function.\n",
    "\n",
    "The covariance takes the following form, $$\n",
    "k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right)\n",
    "$$ where $\\ell$ is the *length scale* or *time scale* of the process and\n",
    "$\\alpha$ represents the overall process variance.\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/eq_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/eq_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>The exponentiated quadratic covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olympic Marathon Data\n",
    "---------------------\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "-   Present results using pace per km.\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/Stephen_Kiprotich.jpg\" style=\"width:100%\">\n",
    "<small>Image from Wikimedia Commons\n",
    "<a href=\"http://bit.ly/16kMKHQ\" class=\"uri\">http://bit.ly/16kMKHQ</a></small>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The first thing we will do is load a standard data set for regression\n",
    "modelling. The data consists of the pace of Olympic Gold Medal Marathon\n",
    "winners for the Olympics from 1896 to present. First we load in the data\n",
    "and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/sods/ods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "yhat = (y-offset)/scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon.svg', \n",
    "                  directory='./datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/datasets/olympic-marathon.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic marathon pace times since 1892.</i>\n",
    "\n",
    "Things to notice about the data include the outlier in 1904, in this\n",
    "year, the olympics was in St Louis, USA. Organizational problems and\n",
    "challenges with dust kicked up by the cars following the race meant that\n",
    "participants got lost, and only very few participants completed.\n",
    "\n",
    "More recent years see more consistently quick marathons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alan Turing\n",
    "-----------\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/turing-times.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/turing-run.jpg\" style=\"width:50%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Alan Turing, in 1946 he was only 11 minutes slower than the\n",
    "winner of the 1948 games. Would he have won a hypothetical games held in\n",
    "1946? Source:\n",
    "<a href=\"http://www.turing.org.uk/scrapbook/run.html\" target=\"_blank\" >Alan\n",
    "Turing Internet Scrapbook</a>.</i>\n",
    "\n",
    "If we had to summarise the objectives of machine learning in one word, a\n",
    "very good candidate for that word would be *generalization*. What is\n",
    "generalization? From a human perspective it might be summarised as the\n",
    "ability to take lessons learned in one domain and apply them to another\n",
    "domain. If we accept the definition given in the first session for\n",
    "machine learning, $$\n",
    "\\text{data} + \\text{model} \\stackrel{\\text{compute}}{\\rightarrow} \\text{prediction}\n",
    "$$ then we see that without a model we can’t generalise: we only have\n",
    "data. Data is fine for answering very specific questions, like “Who won\n",
    "the Olympic Marathon in 2012?”, because we have that answer stored,\n",
    "however, we are not given the answer to many other questions. For\n",
    "example, Alan Turing was a formidable marathon runner, in 1946 he ran a\n",
    "time 2 hours 46 minutes (just under four minutes per kilometer, faster\n",
    "than I and most of the other [Endcliffe Park\n",
    "Run](http://www.parkrun.org.uk/sheffieldhallam/) runners can do 5 km).\n",
    "What is the probability he would have won an Olympics if one had been\n",
    "held in 1946?\n",
    "\n",
    "To answer this question we need to generalize, but before we formalize\n",
    "the concept of generalization let’s introduce some formal representation\n",
    "of what it means to generalize in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Fit\n",
    "--------------------\n",
    "\n",
    "Our first objective will be to perform a Gaussian process fit to the\n",
    "data, we’ll do this using the [GPy\n",
    "software](https://github.com/SheffieldML/GPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first command sets up the model, then `m_full.optimize()` optimizes\n",
    "the parameters of the covariance function and the noise level of the\n",
    "model. Once the fit is complete, we’ll try creating some test points,\n",
    "and computing the output of the GP model in terms of the mean and\n",
    "standard deviation of the posterior functions between 1870 and 2030. We\n",
    "plot the mean function and the standard deviation at 200 locations. We\n",
    "can obtain the predictions using `y_mean, y_var = m_full.predict(xt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.linspace(1870,2030,200)[:,np.newaxis]\n",
    "yt_mean, yt_var = m_full.predict(xt)\n",
    "yt_sd=np.sqrt(yt_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the results using the helper function in `teaching_plots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, scale=scale, offset=offset, ax=ax, xlabel='year', ylabel='pace min/km', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='olympic-marathon-gp.svg', \n",
    "                  directory = './gp',\n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/olympic-marathon-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fit to the Olympic Marathon data. The error\n",
    "bars are too large, perhaps due to the outlier from 1904.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Quality\n",
    "-----------\n",
    "\n",
    "In the fit we see that the error bars (coming mainly from the noise\n",
    "variance) are quite large. This is likely due to the outlier point in\n",
    "1904, ignoring that point we can see that a tighter fit is obtained. To\n",
    "see this make a version of the model, `m_clean`, where that point is\n",
    "removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean=np.vstack((x[0:2, :], x[3:, :]))\n",
    "y_clean=np.vstack((y[0:2, :], y[3:, :]))\n",
    "\n",
    "m_clean = GPy.models.GPRegression(x_clean,y_clean)\n",
    "_ = m_clean.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_clean, scale=scale, offset=offset, ax=ax, xlabel='year', ylabel='pace min/km', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/olympic-marathon-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateObject(rotationMatrix, handle):\n",
    "for i = 1:prod(size(handle))\n",
    "    type = get(handle(i), 'type');\n",
    "    if strcmp(type, 'text'):\n",
    "        xy = get(handle(i), 'position');\n",
    "        xy(1:2) = rotationMatrix*xy(1:2)';\n",
    "        set(handle(i), 'position', xy);\n",
    "    else:\n",
    "        xd = get(handle(i), 'xdata');\n",
    "        yd = get(handle(i), 'ydata');\n",
    "        new = rotationMatrix*[xd(:)'; yd(:)'];\n",
    "        set(handle(i), 'xdata', new(1, :));\n",
    "        set(handle(i), 'ydata', new(2, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Covariance Parameters\n",
    "------------------------------\n",
    "\n",
    "Can we determine covariance parameters from the data?\n",
    "\n",
    "$$\n",
    "\\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=\\frac{1}{(2\\pi)^\\frac{n}{2}{\\det{\\mathbf{K}}^{\\frac{1}{2}}}}{\\exp\\left(-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=\\frac{1}{(2\\pi)^\\frac{n}{2}\\color{blue}{\\det{\\mathbf{K}}^{\\frac{1}{2}}}}\\color{red}{\\exp\\left(-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\log \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=&\\color{blue}{-\\frac{1}{2}\\log\\det{\\mathbf{K}}}\\color{red}{-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}} \\\\ &-\\frac{n}{2}\\log2\\pi\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(\\boldsymbol{ \\theta}) = \\color{blue}{\\frac{1}{2}\\log\\det{\\mathbf{K}}} + \\color{red}{\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      clf\n",
    "      lambda1 = 3;\n",
    "      lambda2 = 1;\n",
    "      t = linspace(-pi, pi, 200);\n",
    "      R = [sqrt(2)/2 -sqrt(2)/2; sqrt(2)/2 sqrt(2)/2];\n",
    "      xy = R*[lambda1*sin(t); lambda2*cos(t)];\n",
    "      line(xy(1, :), xy(2, :), 'linewidth', 3, 'color', blackColor);\n",
    "      axis off, axis equal\n",
    "      a = arrow([0 lambda1*R(1, 1)], [0 lambda1*R(2, 1)]);\n",
    "      set(a, 'linewidth', 3, 'color', blueColor);\n",
    "      a = arrow([0 lambda2*R(1, 2)], [0 lambda2*R(2, 2)]);\n",
    "      set(a, 'linewidth', 3, 'color', blueColor);\n",
    "      xlim = get(gca, 'xlim');\n",
    "      xspan = xlim(2) - xlim(1);\n",
    "      ylim = get(gca, 'ylim');\n",
    "      yspan = ylim(2) - ylim(1);\n",
    "      text(lambda1*0.5*R(1, 1)-0.05*xspan, lambda1*0.5*R(2, 1)-yspan*0.05, '$\\eigenvalue_1$')\n",
    "      text(lambda2*0.5*R(1, 2)-0.05*xspan, lambda2*0.5*R(2, 2)-yspan*0.05, '$\\eigenvalue_2$')\n",
    "      fileName = 'gpOptimiseEigen';\n",
    "      printLatexPlot(fileName, directory, 0.45*textWidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capacity Control through the Determinant\n",
    "----------------------------------------\n",
    "\n",
    "The parameters are *inside* the covariance function (matrix).\n",
    "$$k_{i, j} = k(\\mathbf{ x}_i, \\mathbf{ x}_j; \\boldsymbol{ \\theta})$$\n",
    "\n",
    "$$\\mathbf{K}= \\mathbf{R}\\boldsymbol{ \\Lambda}^2 \\mathbf{R}^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpoptimizePlot1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimize-eigen.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "$\\boldsymbol{ \\Lambda}$ represents distance on axes. $\\mathbf{R}$ gives\n",
    "rotation.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "-   $\\boldsymbol{ \\Lambda}$ is *diagonal*,\n",
    "    $\\mathbf{R}^\\top\\mathbf{R}= \\mathbf{I}$.\n",
    "-   Useful representation since\n",
    "    $\\det{\\mathbf{K}} = \\det{\\boldsymbol{ \\Lambda}^2} = \\det{\\boldsymbol{ \\Lambda}}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = './gp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_capacity(rotate_angle=np.pi/4, lambda1 = 0.5, lambda2 = 0.3, diagrams = './gp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise-determinant009.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The determinant of the covariance is dependent only on the\n",
    "eigenvalues. It represents the ‘footprint’ of the Gaussian.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf\n",
    "    includeText = [];\n",
    "    counter = 0;\n",
    "    plotWidth = 0.6*textWidth;\n",
    "    lambda1 = 3;\n",
    "    lambda2 = 1;\n",
    "    t = linspace(-pi, pi, 200);\n",
    "    R = [sqrt(2)/2 -sqrt(2)/2; sqrt(2)/2 sqrt(2)/2];\n",
    "    xy = [lambda1*sin(t); lambda2*cos(t)];\n",
    "    contourHand = line(xy(1, :), xy(2, :), 'color', blackColor);\n",
    "    xy = [lambda1*sin(t); lambda2*cos(t)]*2;\n",
    "    lim = [-1 1]*max([lambda1 lambda2])*2.2;\n",
    "    set(gca, 'xlim', lim, 'ylim', lim)\n",
    "    axis equal\n",
    "\n",
    "\n",
    "    contourHand = [contourHand line(xy(1, :), xy(2, :), 'color', blackColor)];\n",
    "    set(contourHand, 'linewidth', 2, 'color', redColor)\n",
    "    arrowHand = arrow([0 lambda1], [0 0]);\n",
    "    arrowHand = [arrowHand arrow([0 0], [0 lambda2])];\n",
    "    set(arrowHand, 'linewidth', 3, 'color', blackColor);\n",
    "    xlim = get(gca, 'xlim');\n",
    "    xspan = xlim(2) - xlim(1);\n",
    "    ylim = get(gca, 'ylim');\n",
    "    yspan = ylim(2) - ylim(1);\n",
    "    eigLabel = text(lambda1*0.5, -yspan*0.05, '$\\eigenvalue_1$', 'horizontalalignment', 'center');\n",
    "    eigLabel = [eigLabel text(-0.05*xspan, lambda2*0.5, '$\\eigenvalue_2$', 'horizontalalignment', 'center')];\n",
    "    xlabel('$\\dataScalar_1$')\n",
    "    ylabel('$\\dataScalar_2$')\n",
    "    \n",
    "    box off\n",
    "    xlim = get(gca, 'xlim');\n",
    "    ylim = get(gca, 'ylim');\n",
    "    line([xlim(1) xlim(1)], ylim, 'color', blackColor)\n",
    "    line(xlim, [ylim(1) ylim(1)], 'color', blackColor)\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "\n",
    "    y = [1.2 1.4];\n",
    "    dataHand = line(y(1), y(2), 'marker', 'x', 'markersize', markerSize, 'linewidth', markerWidth, 'color', blackColor);\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "\n",
    "    \n",
    "    rotateObject(rotationMatrix, arrowHand);\n",
    "    rotateObject(rotationMatrix, contourHand);\n",
    "    rotateObject(rotationMatrix, eigLabel);\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "    \n",
    "    printLatexText(includeText, 'gpOptimiseQuadraticIncludeText.tex', directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise-quadratic002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The data fit term of the Gaussian process is a quadratic loss\n",
    "centered around zero. This has eliptical contours, the principal axes of\n",
    "which are given by the covariance matrix.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Data Fit\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Fit Term\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import gp_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(125)\n",
    "diagrams = './gp'\n",
    "\n",
    "black_color=[0., 0., 0.]\n",
    "red_color=[1., 0., 0.]\n",
    "blue_color=[0., 0., 1.]\n",
    "magenta_color=[1., 0., 1.]\n",
    "fontsize=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lim = [-2.2, 2.2]\n",
    "y_ticks = [-2, -1, 0, 1, 2]\n",
    "x_lim = [-2, 2]\n",
    "x_ticks = [-2, -1, 0, 1, 2]\n",
    "err_y_lim = [-12, 20]\n",
    "\n",
    "linewidth=3\n",
    "markersize=15\n",
    "markertype='.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 6)[:, np.newaxis]\n",
    "xtest = np.linspace(x_lim[0], x_lim[1], 200)[:, np.newaxis]\n",
    "\n",
    "# True data\n",
    "true_kern = GPy.kern.RBF(1) + GPy.kern.White(1)\n",
    "true_kern.rbf.lengthscale = 1.0\n",
    "true_kern.white.variance = 0.01\n",
    "K = true_kern.K(x) \n",
    "y = np.random.multivariate_normal(np.zeros((6,)), K, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted model\n",
    "kern = GPy.kern.RBF(1) + GPy.kern.White(1)\n",
    "kern.rbf.lengthscale = 1.0\n",
    "kern.white.variance = 0.01\n",
    "\n",
    "lengthscales = np.asarray([0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 4, 8, 16, 100])\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=plot.one_figsize)    \n",
    "fig2, ax2 = plt.subplots(figsize=plot.one_figsize)    \n",
    "line = ax2.semilogx(np.NaN, np.NaN, 'x-', \n",
    "                    color=black_color)\n",
    "ax.set_ylim(err_y_lim)\n",
    "ax.set_xlim([0.025, 32])\n",
    "ax.grid(True)\n",
    "ax.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "ax.set_xticklabels(['$10^{-2}$', '$10^{-1}$', '$10^0$', '$10^1$', '$10^2$'])\n",
    "\n",
    "\n",
    "err = np.zeros_like(lengthscales)\n",
    "err_log_det = np.zeros_like(lengthscales)\n",
    "err_fit = np.zeros_like(lengthscales)\n",
    "\n",
    "counter = 0\n",
    "for i, ls in enumerate(lengthscales):\n",
    "        kern.rbf.lengthscale=ls\n",
    "        K = kern.K(x) \n",
    "        invK, L, Li, log_det_K = GPy.util.linalg.pdinv(K)\n",
    "        err[i] = 0.5*(log_det_K + np.dot(np.dot(y.T,invK),y))\n",
    "        err_log_det[i] = 0.5*log_det_K\n",
    "        err_fit[i] = 0.5*np.dot(np.dot(y.T,invK), y)\n",
    "        Kx = kern.K(x, xtest)\n",
    "        ypred_mean = np.dot(np.dot(Kx.T, invK), y)\n",
    "        ypred_var = kern.Kdiag(xtest) - np.sum((np.dot(Kx.T,invK))*Kx.T, 1)\n",
    "        ypred_sd = np.sqrt(ypred_var)\n",
    "        ax1.clear()\n",
    "        _ = gp_tutorial.gpplot(xtest.flatten(),\n",
    "                               ypred_mean.flatten(),\n",
    "                               ypred_mean.flatten()-2*ypred_sd.flatten(),\n",
    "                               ypred_mean.flatten()+2*ypred_sd.flatten(), \n",
    "                               ax=ax1)\n",
    "        x_lim = ax1.get_xlim()\n",
    "        ax1.set_ylabel('$f(x)$', fontsize=fontsize)\n",
    "        ax1.set_xlabel('$x$', fontsize=fontsize)\n",
    "\n",
    "        p = ax1.plot(x, y, markertype, color=black_color, markersize=markersize, linewidth=linewidth)\n",
    "        ax1.set_ylim(y_lim)\n",
    "        ax1.set_xlim(x_lim)                                    \n",
    "        ax1.set_xticks(x_ticks)\n",
    "        #ax.set(box=False)\n",
    "           \n",
    "        ax1.plot([x_lim[0], x_lim[0]], y_lim, color=black_color)\n",
    "        ax1.plot(x_lim, [y_lim[0], y_lim[0]], color=black_color)\n",
    "\n",
    "        file_name = 'gp-optimise{counter:0>3}.svg'.format(counter=counter)\n",
    "        mlai.write_figure(os.path.join(diagrams, file_name),\n",
    "                          figure=fig1,\n",
    "                          transparent=True)\n",
    "        counter += 1\n",
    "\n",
    "        ax2.clear()\n",
    "        t = ax2.semilogx(lengthscales[0:i+1], err[0:i+1], 'x-', \n",
    "                        color=magenta_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        t2 = ax2.semilogx(lengthscales[0:i+1], err_log_det[0:i+1], 'x-', \n",
    "                         color=blue_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        t3 = ax2.semilogx(lengthscales[0:i+1], err_fit[0:i+1], 'x-', \n",
    "                         color=red_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        ax2.set_ylim(err_y_lim)\n",
    "        ax2.set_xlim([0.025, 32])\n",
    "        ax2.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "        ax2.set_xticklabels(['$10^{-2}$', '$10^{-1}$', '$10^0$', '$10^1$', '$10^2$'])\n",
    "\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax2.set_ylabel('negative log likelihood', fontsize=fontsize)\n",
    "        ax2.set_xlabel('length scale, $\\ell$', fontsize=fontsize)\n",
    "        file_name = 'gp-optimise{counter:0>3}.svg'.format(counter=counter)\n",
    "        mlai.write_figure(os.path.join(diagrams, file_name),\n",
    "                          figure=fig2,\n",
    "                          transparent=True)\n",
    "        counter += 1\n",
    "        #ax.set_box(False)\n",
    "        xlim = ax2.get_xlim()\n",
    "        ax2.plot([xlim[0], xlim[0]], err_y_lim, color=black_color)\n",
    "        ax2.plot(xlim, [err_y_lim[0], err_y_lim[0]], color=black_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise006.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise010.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise016.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise021.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Variation in the data fit term, the capacity term and the\n",
    "negative log likelihood for different lengthscales.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gene Expression Example\n",
    "-----------------------\n",
    "\n",
    "We now consider an example in gene expression. Gene expression is the\n",
    "measurement of mRNA levels expressed in cells. These mRNA levels show\n",
    "which genes are ‘switched on’ and producing data. In the example we will\n",
    "use a Gaussian process to determine whether a given gene is active, or\n",
    "we are merely observing a noise response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Della Gatta Gene Data\n",
    "---------------------\n",
    "\n",
    "-   Given given expression levels in the form of a time series from\n",
    "    Della Gatta et al. (2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.della_gatta_TRP63_gene_expression(data_set='della_gatta',gene_number=937)\n",
    "\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (-20,260)\n",
    "ylim = (5, 7.5)\n",
    "yhat = (y-offset)/scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('time/min', fontsize=20)\n",
    "ax.set_ylabel('expression', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(figure=fig, \n",
    "                  filename='./datasets/della-gatta-gene.svg', \n",
    "                  transparent=True, \n",
    "                  frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/datasets/della-gatta-gene.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gene expression levels over time for a gene from data\n",
    "provided by Della Gatta et al. (2008). We would like to understand\n",
    "whether there is signal in the data, or we are only observing noise.</i>\n",
    "\n",
    "-   Want to detect if a gene is expressed or not, fit a GP to each gene\n",
    "    Kalaitzis and Lawrence (2011).\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip1\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Freddie Kalaitzis\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/freddie-kalaitzis.jpg\" clip-path=\"url(#clip1)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/1471-2105-12-180_1.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>The example is taken from the paper “A Simple Approach to\n",
    "Ranking Differentially Expressed Gene Expression Time Courses through\n",
    "Gaussian Process Regression.” Kalaitzis and Lawrence (2011).</i>\n",
    "\n",
    "<center>\n",
    "\n",
    "<a href=\"http://www.biomedcentral.com/1471-2105/12/180\" class=\"uri\">http://www.biomedcentral.com/1471-2105/12/180</a>\n",
    "\n",
    "</center>\n",
    "\n",
    "Our first objective will be to perform a Gaussian process fit to the\n",
    "data, we’ll do this using the [GPy\n",
    "software](https://github.com/SheffieldML/GPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "m_full.kern.lengthscale=50\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the length scale parameter (which here actually represents a\n",
    "*time scale* of the covariance function) to a reasonable value. Default\n",
    "would be 1, but here we set it to 50 minutes, given points are arriving\n",
    "across zero to 250 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.linspace(-20,260,200)[:,np.newaxis]\n",
    "yt_mean, yt_var = m_full.predict(xt)\n",
    "yt_sd=np.sqrt(yt_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the results using the helper function in `teaching_plots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, scale=scale, offset=offset, ax=ax, xlabel='time/min', ylabel='expression', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title('log likelihood: {ll:.3}'.format(ll=m_full.log_likelihood()), fontsize=20)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/della-gatta-gene-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/della-gatta-gene-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Result of the fit of the Gaussian process model with the time\n",
    "scale parameter initialized to 50 minutes.</i>\n",
    "\n",
    "Now we try a model initialized with a longer length scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full2 = GPy.models.GPRegression(x,yhat)\n",
    "m_full2.kern.lengthscale=2000\n",
    "_ = m_full2.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full2, scale=scale, offset=offset, ax=ax, xlabel='time/min', ylabel='expression', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title('log likelihood: {ll:.3}'.format(ll=m_full2.log_likelihood()), fontsize=20)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/della-gatta-gene-gp2.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/della-gatta-gene-gp2.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Result of the fit of the Gaussian process model with the time\n",
    "scale parameter initialized to 2000 minutes.</i>\n",
    "\n",
    "Now we try a model initialized with a lower noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full3 = GPy.models.GPRegression(x,yhat)\n",
    "m_full3.kern.lengthscale=20\n",
    "m_full3.likelihood.variance=0.001\n",
    "_ = m_full3.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full3, scale=scale, offset=offset, ax=ax, xlabel='time/min', ylabel='expression', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title('log likelihood: {ll:.3}'.format(ll=m_full3.log_likelihood()), fontsize=20)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/della-gatta-gene-gp3.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/della-gatta-gene-gp3.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Result of the fit of the Gaussian process model with the\n",
    "noise initialized low (standard deviation 0.1) and the time scale\n",
    "parameter initialized to 20 minutes.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.multiple_optima(diagrams='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/multiple-optima000.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i></i>\n",
    "\n",
    "<!--\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/multiple-optima001.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Prediction of Malaria Incidence in Uganda\n",
    "--------------------------------------------------\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip2\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Martin Mubangizi\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/martin-mubangizi.png\" clip-path=\"url(#clip2)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip3\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Ricardo Andrade Pacecho\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/ricardo-andrade-pacheco.png\" clip-path=\"url(#clip3)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip4\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "John Quinn\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/john-quinn.jpg\" clip-path=\"url(#clip4)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "As an example of using Gaussian process models within the full pipeline\n",
    "from data to decsion, we’ll consider the prediction of Malaria incidence\n",
    "in Uganda. For the purposes of this study malaria reports come in two\n",
    "forms, HMIS reports from health centres and Sentinel data, which is\n",
    "curated by the WHO. There are limited sentinel sites and many HMIS\n",
    "sites.\n",
    "\n",
    "The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in\n",
    "collaboration with John Quinn and Martin Mubangizi (Andrade-Pacheco et\n",
    "al., 2014; Mubangizi et al., 2014). John and Martin were initally from\n",
    "the AI-DEV group from the University of Makerere in Kampala and more\n",
    "latterly they were based at UN Global Pulse in Kampala. You can see the\n",
    "work summarized on the UN Global Pulse [disease outbreaks project site\n",
    "here](https://diseaseoutbreaks.unglobalpulse.net/uganda/).\n",
    "\n",
    "-   See [UN Global Pulse Disease Outbreaks\n",
    "    Site](https://diseaseoutbreaks.unglobalpulse.net/uganda/)\n",
    "\n",
    "Malaria data is spatial data. Uganda is split into districts, and health\n",
    "reports can be found for each district. This suggests that models such\n",
    "as conditional random fields could be used for spatial modelling, but\n",
    "there are two complexities with this. First of all, occasionally\n",
    "districts split into two. Secondly, sentinel sites are a specific\n",
    "location within a district, such as Nagongera which is a sentinel site\n",
    "based in the Tororo district.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/uganda-districts-2006.png\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>Ugandan districs. Data SRTM/NASA from\n",
    "<a href=\"https://dds.cr.usgs.gov/srtm/version2_1\" class=\"uri\">https://dds.cr.usgs.gov/srtm/version2_1</a>.</i>\n",
    "\n",
    "<span style=\"text-align:right\">(Andrade-Pacheco et al., 2014; Mubangizi\n",
    "et al., 2014)</span>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/Kapchorwa_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Kapchorwa District, home district of Stephen\n",
    "Kiprotich.</i>\n",
    "\n",
    "Stephen Kiprotich, the 2012 gold medal winner from the London Olympics,\n",
    "comes from Kapchorwa district, in eastern Uganda, near the border with\n",
    "Kenya.\n",
    "\n",
    "The common standard for collecting health data on the African continent\n",
    "is from the Health management information systems (HMIS). However, this\n",
    "data suffers from missing values (Gething et al., 2006) and diagnosis of\n",
    "diseases like typhoid and malaria may be confounded.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/Tororo_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Tororo district, where the sentinel site, Nagongera, is\n",
    "located.</i>\n",
    "\n",
    "[World Health Organization Sentinel Surveillance\n",
    "systems](https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/)\n",
    "are set up “when high-quality data are needed about a particular disease\n",
    "that cannot be obtained through a passive system”. Several sentinel\n",
    "sites give accurate assessment of malaria disease levels in Uganda,\n",
    "including a site in Nagongera.\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/sentinel_nagongera.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>Sentinel and HMIS data along with rainfall and temperature\n",
    "for the Nagongera sentinel station in the Tororo district.</i>\n",
    "\n",
    "In collaboration with the AI Research Group at Makerere we chose to\n",
    "investigate whether Gaussian process models could be used to assimilate\n",
    "information from these two different sources of disease informaton.\n",
    "Further, we were interested in whether local information on rainfall and\n",
    "temperature could be used to improve malaria estimates.\n",
    "\n",
    "The aim of the project was to use WHO Sentinel sites, alongside rainfall\n",
    "and temperature, to improve predictions from HMIS data of levels of\n",
    "malaria.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/Mubende_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Mubende District.</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/mubende.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Prediction of malaria incidence in Mubende.</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gpss/1157497_513423392066576_1845599035_n.jpg\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>The project arose out of the Gaussian process summer school\n",
    "held at Makerere in Kampala in 2013. The school led, in turn, to the\n",
    "Data Science Africa initiative.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Warning Systems\n",
    "---------------------\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/Kabarole_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Kabarole district in Uganda.</i>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/kabarole.gif\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>Estimate of the current disease situation in the Kabarole\n",
    "district over time. Estimate is constructed with a Gaussian process with\n",
    "an additive covariance funciton.</i>\n",
    "\n",
    "Health monitoring system for the Kabarole district. Here we have fitted\n",
    "the reports with a Gaussian process with an additive covariance\n",
    "function. It has two components, one is a long time scale component (in\n",
    "red above) the other is a short time scale component (in blue).\n",
    "\n",
    "Monitoring proceeds by considering two aspects of the curve. Is the blue\n",
    "line (the short term report signal) above the red (which represents the\n",
    "long term trend? If so we have higher than expected reports. If this is\n",
    "the case *and* the gradient is still positive (i.e. reports are going\n",
    "up) we encode this with a *red* color. If it is the case and the\n",
    "gradient of the blue line is negative (i.e. reports are going down) we\n",
    "encode this with an *amber* color. Conversely, if the blue line is below\n",
    "the red *and* decreasing, we color *green*. On the other hand if it is\n",
    "below red but increasing, we color *yellow*.\n",
    "\n",
    "This gives us an early warning system for disease. Red is a bad\n",
    "situation getting worse, amber is bad, but improving. Green is good and\n",
    "getting better and yellow good but degrading.\n",
    "\n",
    "Finally, there is a gray region which represents when the scale of the\n",
    "effect is small.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/health/monitor.gif\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>The map of Ugandan districts with an overview of the Malaria\n",
    "situation in each district.</i>\n",
    "\n",
    "These colors can now be observed directly on a spatial map of the\n",
    "districts to give an immediate impression of the current status of the\n",
    "disease across the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additive Covariance\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s linear_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s add_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=add_cov,\n",
    "                     name='Additive',\n",
    "                     shortname='add',                     \n",
    "                     formula='\\kernelScalar_f(\\inputVector, \\inputVector^\\prime) = \\kernelScalar_g(\\inputVector, \\inputVector^\\prime) + \\kernelScalar_h(\\inputVector, \\inputVector^\\prime)', \n",
    "                     kerns=[linear_cov, eq_cov], \n",
    "                     kern_args=[{'variance': 25}, {'lengthscale' : 0.2}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_func(kernel=kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additive covariance function is derived from considering the result\n",
    "of summing two Gaussian processes together. If the first Gaussian\n",
    "process is $g(\\cdot)$, governed by covariance $k_g(\\cdot, \\cdot)$ and\n",
    "the second process is $h(\\cdot)$, governed by covariance\n",
    "$k_h(\\cdot, \\cdot)$ then the combined process\n",
    "$f(\\cdot) = g(\\cdot) + h(\\cdot)$ is govererned by a covariance function,\n",
    "$$\n",
    "k_f(\\mathbf{ x}, \\mathbf{ x}^\\prime) = k_g(\\mathbf{ x}, \\mathbf{ x}^\\prime) + k_h(\\mathbf{ x}, \\mathbf{ x}^\\prime)\n",
    "$$\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k_f(\\mathbf{ x}, \\mathbf{ x}^\\prime) = k_g(\\mathbf{ x}, \\mathbf{ x}^\\prime) + k_h(\\mathbf{ x}, \\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/add_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/add_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>An additive covariance function formed by combining a linear\n",
    "and an exponentiated quadratic covariance functions.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of US Birth Rates\n",
    "--------------------------\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip5\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Aki Vehtari\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/aki-vehtari.jpg\" clip-path=\"url(#clip5)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/bialik-fridaythe13th-1.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>This is a retrospective analysis of US births by Aki Vehtari.\n",
    "The challenges of forecasting. Even with seasonal and weekly effects\n",
    "removed there are significant effects on holidays, weekends, etc.</i>\n",
    "\n",
    "There’s a nice analysis of US birth rates by Gaussian processes with\n",
    "additive covariances in Gelman et al. (2013). A combination of\n",
    "covariance functions are used to take account of weekly and yearly\n",
    "trends. The analysis is summarized on the cover of the book.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/bda_cover_1.png\" style=\"width:80%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/bda_cover.png\" style=\"width:80%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Two different editions of Bayesian Data Analysis (Gelman et\n",
    "al., 2013).</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Function Covariance\n",
    "-------------------------\n",
    "\n",
    "The fixed basis function covariance just comes from the properties of a\n",
    "multivariate Gaussian, if we decide $$\n",
    "\\mathbf{ f}=\\boldsymbol{ \\Phi}\\mathbf{ w}\n",
    "$$ and then we assume $$\n",
    "\\mathbf{ w}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha\\mathbf{I}\\right)\n",
    "$$ then it follows from the properties of a multivariate Gaussian that\n",
    "$$\n",
    "\\mathbf{ f}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha\\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top\\right)\n",
    "$$ meaning that the vector of observations from the function is jointly\n",
    "distributed as a Gaussian process and the covariance matrix is\n",
    "$\\mathbf{K}= \\alpha\\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top$, each\n",
    "element of the covariance matrix can then be found as the inner product\n",
    "between two rows of the basis funciton matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s basis_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s radial mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = mlai.Basis(function=radial, \n",
    "                   number=3,\n",
    "                   data_limits=[-0.5, 0.5], \n",
    "                   width=0.125)\n",
    "kernel = mlai.Kernel(function=basis_cov,\n",
    "                     name='Basis',\n",
    "                     shortname='basis',                  \n",
    "                     formula='\\kernel(\\inputVector, \\inputVector^\\prime) = \\basisVector(\\inputVector)^\\top \\basisVector(\\inputVector^\\prime)',\n",
    "                     basis=basis)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\boldsymbol{ \\phi}(\\mathbf{ x})^\\top \\boldsymbol{ \\phi}(\\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/basis_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/basis_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>A covariance function based on a non-linear basis given by\n",
    "$\\boldsymbol{ \\phi}(\\mathbf{ x})$.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brownian Covariance\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s brownian_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.linspace(0, 2, 200)[:, np.newaxis]\n",
    "kernel = mlai.Kernel(function=brownian_cov,\n",
    "                     name='Brownian',\n",
    "                     formula='\\kernelScalar(t, t^\\prime)=\\alpha \\min(t, t^\\prime)',\n",
    "                     shortname='brownian')\n",
    "plot.covariance_func(kernel, t, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brownian motion is also a Gaussian process. It follows a Gaussian random\n",
    "walk, with diffusion occuring at each time point driven by a Gaussian\n",
    "input. This implies it is both Markov and Gaussian. The covariance\n",
    "function for Brownian motion has the form $$\n",
    "k(t, t^\\prime)=\\alpha \\min(t, t^\\prime)\n",
    "$$\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k(t, t^\\prime)=\\alpha \\min(t, t^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/brownian_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/brownian_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Brownian motion covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Covariance\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s mlp_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=mlp_cov,\n",
    "                     name='Multilayer Perceptron',\n",
    "                     shortname='mlp',                    \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\arcsin\\left(\\frac{w \\inputVector^\\top \\inputVector^\\prime + b}{\\sqrt{\\left(w \\inputVector^\\top \\inputVector + b + 1\\right)\\left(w \\left.\\inputVector^\\prime\\right.^\\top \\inputVector^\\prime + b + 1\\right)}}\\right)',\n",
    "                     w=5, b=0.5)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-layer perceptron (MLP) covariance, also known as the neural\n",
    "network covariance or the arcsin covariance, is derived by considering\n",
    "the infinite limit of a neural network.\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\arcsin\\left(\\frac{w \\mathbf{ x}^\\top \\mathbf{ x}^\\prime + b}{\\sqrt{\\left(w \\mathbf{ x}^\\top \\mathbf{ x}+ b + 1\\right)\\left(w \\left.\\mathbf{ x}^\\prime\\right.^\\top \\mathbf{ x}^\\prime + b + 1\\right)}}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/mlp_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/mlp_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>The multi-layer perceptron covariance function. This is\n",
    "derived by considering the infinite limit of a neural network with\n",
    "probit activation functions.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RELU Covariance\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s relu_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=relu_cov,\n",
    "                     name='RELU',\n",
    "                     shortname='relu',                   \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\arcsin\\left(\\frac{w \\inputVector^\\top \\inputVector^\\prime + b}{\\sqrt{\\left(w \\inputVector^\\top \\inputVector + b + 1\\right)\\left(w \\left.\\inputVector^\\prime\\right.^\\top \\inputVector^\\prime + b + 1\\right)}}\\right)',\n",
    "                     w=5, b=0.5)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \n",
    "\\alpha \\arcsin\\left(\\frac{w \\mathbf{ x}^\\top \\mathbf{ x}^\\prime + b}\n",
    "{\\sqrt{\\left(w \\mathbf{ x}^\\top \\mathbf{ x}+ b + 1\\right)\n",
    "\\left(w \\left.\\mathbf{ x}^\\prime\\right.^\\top \\mathbf{ x}^\\prime + b + 1\\right)}}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/relu_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/relu_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Rectified linear unit covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinc Covariance\n",
    "---------------\n",
    "\n",
    "Another approach to developing covariance function exploits Bochner’s\n",
    "theorem Bochner (1959). Bochner’s theorem tells us that any positve\n",
    "filter in Fourier space implies has an associated Gaussian process with\n",
    "a stationary covariance function. The covariance function is the\n",
    "*inverse Fourier transform* of the filter applied in Fourier space.\n",
    "\n",
    "For example, in signal processing, *band limitations* are commonly\n",
    "applied as an assumption. For example, we may believe that no frequency\n",
    "above $w=2$ exists in the signal. This is equivalent to a rectangle\n",
    "function being applied as a the filter in Fourier space.\n",
    "\n",
    "The inverse Fourier transform of the rectangle function is the\n",
    "$\\text{sinc}(\\cdot)$ function. So the sinc is a valid covariance\n",
    "function, and it represents *band limited* signals.\n",
    "\n",
    "Note that other covariance functions we’ve introduced can also be\n",
    "interpreted in this way. For example, the exponentiated quadratic\n",
    "covariance function can be Fourier transformed to see what the implied\n",
    "filter in Fourier space is. The Fourier transform of the exponentiated\n",
    "quadratic is an exponentiated quadratic, so the standard EQ-covariance\n",
    "implies a EQ filter in Fourier space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s sinc_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=sinc_cov,\n",
    "                     name='Sinc',\n",
    "                     shortname='sinc',                   \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\text{sinc}\\left(\\pi w r\\right)',\n",
    "                     w=2)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\text{sinc}\\left(\\pi w r\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/sinc_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/sinc_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Sinc covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Covariance\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s polynomial_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=polynomial_cov,\n",
    "                     name='Polynomial',\n",
    "                     shortname='polynomial',                     \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha(w \\inputVector^\\top\\inputVector^\\prime + b)^d',\n",
    "                     degree=5)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha(w \\mathbf{ x}^\\top\\mathbf{ x}^\\prime + b)^d$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/polynomial_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/polynomial_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Polynomial covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periodic Covariance\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s periodic_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=periodic_cov,\n",
    "                     name='Periodic',\n",
    "                     shortname='periodic',                   \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha\\exp\\left(\\frac{-2\\sin(\\pi rw)^2}{\\lengthScale^2}\\right)',\n",
    "                     lengthscale=1.0)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha\\exp\\left(\\frac{-2\\sin(\\pi rw)^2}{\\ell^2}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/periodic_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/periodic_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Periodic covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model of Coregionalization Covariance\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s lmc_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         kernel=lmc_cov, subkernel=eq_cov,\n",
    "                                         B = np.asarray([[1, 0.5],[0.5, 1.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='./kern', \n",
    "                    filename='lmc_covariance.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(i, j, \\mathbf{ x}, \\mathbf{ x}^\\prime) = b_{i,j} k(\\mathbf{ x}, \\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/lmc_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/lmc_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Linear model of coregionalization covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic Coregionalization Model Covariance\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s icm_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         kernel=icm_cov, subkernel=eq_cov,\n",
    "                                         B = np.asarray([[1, 0.5],[0.5, 1.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='./kern', \n",
    "                    filename='icm_covariance.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(i, j, \\mathbf{ x}, \\mathbf{ x}^\\prime) = b_{i,j} k(\\mathbf{ x}, \\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/icm_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/icm_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Intrinsic coregionalization model covariance function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions\n",
    "----------\n",
    "\n",
    "We’ll cover extensions to Gaussian processes including approximate\n",
    "inference in non Gaussian models, large data (Bui et al., 2017; Hensman\n",
    "et al., n.d.), multiple output GPs (Álvarez et al., 2012), Bayesian\n",
    "optimisation (Snoek et al., 2012) and Deep GPs (Damianou and Lawrence,\n",
    "2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.\n",
    "Consistent mapping of government malaria records across a changing\n",
    "territory delimitation. Malaria Journal 13.\n",
    "<https://doi.org/10.1186/1475-2875-13-S1-P5>\n",
    "\n",
    "Álvarez, M.A., Rosasco, L., Lawrence, N.D., 2012. Kernels for\n",
    "vector-valued functions: A review. Foundations and Trends in Machine\n",
    "Learning 4, 195–266. <https://doi.org/10.1561/2200000036>\n",
    "\n",
    "Bochner, S., 1959. Lectures on Fourier integrals. Princeton University\n",
    "Press.\n",
    "\n",
    "Bui, T.D., Yan, J., Turner, R.E., 2017. A unifying framework for\n",
    "Gaussian process pseudo-point approximations using power expectation\n",
    "propagation. Journal of Machine Learning Research 18, 1–72.\n",
    "\n",
    "Cho, Y., Saul, L.K., 2009. Kernel methods for deep learning, in: Bengio,\n",
    "Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I., Culotta, A.\n",
    "(Eds.), Advances in Neural Information Processing Systems 22. Curran\n",
    "Associates, Inc., pp. 342–350.\n",
    "\n",
    "Damianou, A., Lawrence, N.D., 2013. Deep Gaussian processes, in:. pp.\n",
    "207–215.\n",
    "\n",
    "Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D.,\n",
    "Missero, C., Bernardo, D. di, 2008. Direct targets of the trp63\n",
    "transcription factor revealed by a combination of gene expression\n",
    "profiling and reverse engineering. Genome Research 18, 939–948.\n",
    "<https://doi.org/10.1101/gr.073601.107>\n",
    "\n",
    "Gelman, A., Carlin, J.B., Stern, H.S., Rubin, D.B., 2013. Bayesian data\n",
    "analysis, 3rd ed. Chapman; Hall.\n",
    "\n",
    "Gething, P.W., Noor, A.M., Gikandi, P.W., Ogara, E.A.A., Hay, S.I.,\n",
    "Nixon, M.S., Snow, R.W., Atkinson, P.M., 2006. Improving imperfect data\n",
    "from health management information systems in Africa using space–time\n",
    "geostatistics. PLoS Medicine 3.\n",
    "<https://doi.org/10.1371/journal.pmed.0030271>\n",
    "\n",
    "Hensman, J., Fusi, N., Lawrence, N.D., n.d. Gaussian processes for big\n",
    "data, in:.\n",
    "\n",
    "Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep\n",
    "network training by reducing internal covariate shift, in: Bach, F.,\n",
    "Blei, D. (Eds.), Proceedings of the 32nd International Conference on\n",
    "Machine Learning, Proceedings of Machine Learning Research. PMLR, Lille,\n",
    "France, pp. 448–456.\n",
    "\n",
    "Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking\n",
    "differentially expressed gene expression time courses through Gaussian\n",
    "process regression. BMC Bioinformatics 12.\n",
    "<https://doi.org/10.1186/1471-2105-12-180>\n",
    "\n",
    "Laplace, P.S., 1814. Essai philosophique sur les probabilités, 2nd ed.\n",
    "Courcier, Paris.\n",
    "\n",
    "MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis).\n",
    "California Institute of Technology.\n",
    "\n",
    "McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas\n",
    "immanent in nervous activity. Bulletin of Mathematical Biophysics 5,\n",
    "115–133.\n",
    "\n",
    "Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,\n",
    "N.D., 2014. Malaria surveillance with multiple data sources using\n",
    "Gaussian process models, in: 1st International Conference on the Use of\n",
    "Mobile ICT in Africa.\n",
    "\n",
    "Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis).\n",
    "Dept. of Computer Science, University of Toronto.\n",
    "\n",
    "Pearl, J., 1995. From Bayesian networks to causal networks, in:\n",
    "Gammerman, A. (Ed.), Probabilistic Reasoning and Bayesian Belief\n",
    "Networks. Alfred Waller, pp. 1–31.\n",
    "\n",
    "Snoek, J., Larochelle, H., Adams, R.P., 2012. Practical Bayesian\n",
    "optimization of machine learning algorithms, in: Pereira, F., Burges,\n",
    "C.J.C., Bottou, L., Weinberger, K.Q. (Eds.), Advances in Neural\n",
    "Information Processing Systems 25. Curran Associates, Inc., pp.\n",
    "2951–2959.\n",
    "\n",
    "Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson,\n",
    "E., Avital, I., Stojadinovic, A., 2012. Using machine-learned Bayesian\n",
    "belief networks to predict perioperative risk of clostridium difficile\n",
    "infection following colon surgery. Interact J Med Res 1, e6.\n",
    "<https://doi.org/10.2196/ijmr.2131>\n",
    "\n",
    "Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component\n",
    "analysis. Journal of the Royal Statistical Society, B 6, 611–622.\n",
    "<https://doi.org/doi:10.1111/1467-9868.00196>"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
