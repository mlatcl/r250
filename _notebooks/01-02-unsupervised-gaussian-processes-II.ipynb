{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R250: Unsupervised Learning with Gaussian Processes II\n",
    "======================================================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge\n",
    "\n",
    "### 2021-01-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In this talk we give an introduction to Unsupervised\n",
    "Learning and Gaussian processes for students who are interested in\n",
    "working with Unsupervised GPs for the the R250 module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "-----\n",
    "\n",
    "First we download some libraries and files to support the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--setupplotcode{import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('colorblind')}-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pods\n",
    "----\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science’. Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/sods/ods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on github:\n",
    "<a href=\"https://github.com/sods/ods\" class=\"uri\">https://github.com/sods/ods</a>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis\n",
    "----------------------------\n",
    "\n",
    "-   PCA (Hotelling (1933)) is a linear embedding.\n",
    "\n",
    "-   Today its presented as:\n",
    "\n",
    "    -   Rotate to find ‘directions’ in data with maximal variance.\n",
    "    -   How do we find these directions?\n",
    "\n",
    "-   Algorithmically we do this by diagonalizing the sample covariance\n",
    "    matrix $$\n",
    "    \\mathbf{S}=\\frac{1}{n}\\sum_{i=1}^n\\left(\\mathbf{ y}_{i, :}-\\boldsymbol{ \\mu}\\right)\\left(\\mathbf{ y}_{i, :} - \\boldsymbol{ \\mu}\\right)^\\top\n",
    "    $$\n",
    "\n",
    "-   Find directions in the data, $\\mathbf{ z}= \\mathbf{U}\\mathbf{ y}$,\n",
    "    for which variance is maximized.\n",
    "\n",
    "-   Solution is found via constrained optimisation (which uses [Lagrange\n",
    "    multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier)): $$\n",
    "    L\\left(\\mathbf{u}_{1},\\lambda_{1}\\right)=\\mathbf{u}_{1}^{\\top}\\mathbf{S}\\mathbf{u}_{1}+\\lambda_{1}\\left(1-\\mathbf{u}_{1}^{\\top}\\mathbf{u}_{1}\\right)\n",
    "    $$\n",
    "\n",
    "-   Gradient with respect to $\\mathbf{u}_{1}$\n",
    "    $$\\frac{\\text{d}L\\left(\\mathbf{u}_{1},\\lambda_{1}\\right)}{\\text{d}\\mathbf{u}_{1}}=2\\mathbf{S}\\mathbf{u}_{1}-2\\lambda_{1}\\mathbf{u}_{1}$$\n",
    "    rearrange to form\n",
    "    $$\\mathbf{S}\\mathbf{u}_{1}=\\lambda_{1}\\mathbf{u}_{1}.$$ Which is\n",
    "    known as an [*eigenvalue\n",
    "    problem*](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).\n",
    "\n",
    "-   Further directions that are *orthogonal* to the first can also be\n",
    "    shown to be eigenvectors of the covariance.\n",
    "\n",
    "-   Represent data, $\\mathbf{Y}$, with a lower dimensional set of latent\n",
    "    variables $\\mathbf{Z}$.\n",
    "\n",
    "-   Assume a linear relationship of the form $$\n",
    "    \\mathbf{ y}_{i,:}=\\mathbf{W}\\mathbf{ z}_{i,:}+\\boldsymbol{ \\epsilon}_{i,:},\n",
    "    $$ where $$\n",
    "    \\boldsymbol{ \\epsilon}_{i,:} \\sim \\mathcal{N}\\left(\\mathbf{0},\\sigma^2\\mathbf{I}\\right)\n",
    "    $$\n",
    "\n",
    "**Probabilistic PCA**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width>\n",
    "\n",
    "-   Define *linear-Gaussian relationship* between latent variables and\n",
    "    data.\n",
    "-   **Standard** Latent variable approach:\n",
    "    -   Define Gaussian prior over *latent space*, $\\mathbf{Z}$.\n",
    "-   Integrate out *latent variables*. `{=html}     </td>`\n",
    "    `{=html}     <td width=\"\">`\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/ppca_graph.png\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Graphical model representing probabilistic PCA.</i>\n",
    "\n",
    "$$\n",
    "p\\left(\\mathbf{Y}|\\mathbf{Z},\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{ y}_{i,:}|\\mathbf{W}\\mathbf{ z}_{i,:},\\sigma^2\\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p\\left(\\mathbf{Z}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{ z}_{i,:}|\\mathbf{0},\\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{ y}_{i,:}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic PCA\n",
    "=================\n",
    "\n",
    "In 1997 [Tipping and\n",
    "Bishop](http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf)\n",
    "(Tipping and Bishop, 1999a) and\n",
    "[Roweis](https://www.cs.nyu.edu/~roweis/papers/empca.pdf) (Roweis, n.d.)\n",
    "independently revisited Hotelling’s model and considered the case where\n",
    "the noise variance was finite, but *shared* across all output dimensons.\n",
    "Their model can be thought of as a factor analysis where $$\n",
    "\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}.\n",
    "$$ This leads to a marginal likelihood of the form $$\n",
    "p(\\mathbf{Y}|\\mathbf{W}, \\sigma^2)\n",
    "= \\prod_{i=1}^n\\mathcal{N}\\left(\\mathbf{ y}_{i, :}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right)\n",
    "$$ where the limit of $\\sigma^2\\rightarrow 0$ is *not* taken. This\n",
    "defines a proper probabilistic model. Tippping and Bishop then went on\n",
    "to prove that the *maximum likelihood* solution of this model with\n",
    "respect to $\\mathbf{W}$ is given by an eigenvalue problem. In the\n",
    "probabilistic PCA case the eigenvalues and eigenvectors are given as\n",
    "follows. $$\n",
    "\\mathbf{W}= \\mathbf{U}\\mathbf{L} \\mathbf{R}^\\top\n",
    "$$ where $\\mathbf{U}$ is the eigenvectors of the empirical covariance\n",
    "matrix $$\n",
    "\\mathbf{S} = \\sum_{i=1}^n(\\mathbf{ y}_{i, :} - \\boldsymbol{ \\mu})(\\mathbf{ y}_{i,\n",
    ":} - \\boldsymbol{ \\mu})^\\top,\n",
    "$$ which can be written\n",
    "$\\mathbf{S} = \\frac{1}{n} \\mathbf{Y}^\\top\\mathbf{Y}$ if the data is zero\n",
    "mean. The matrix $\\mathbf{L}$ is diagonal and is dependent on the\n",
    "*eigenvalues* of $\\mathbf{S}$, $\\boldsymbol{\\Lambda}$. If the $i$th\n",
    "diagonal element of this matrix is given by $\\lambda_i$ then the\n",
    "corresponding element of $\\mathbf{L}$ is $$\n",
    "\\ell_i = \\sqrt{\\lambda_i - \\sigma^2}\n",
    "$$ where $\\sigma^2$ is the noise variance. Note that if $\\sigma^2$ is\n",
    "larger than any particular eigenvalue, then that eigenvalue (along with\n",
    "its corresponding eigenvector) is *discarded* from the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Implementation of Probabilistic PCA\n",
    "------------------------------------------\n",
    "\n",
    "We will now implement this algorithm in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic PCA algorithm\n",
    "def ppca(Y, q):\n",
    "    # remove mean\n",
    "    Y_cent = Y - Y.mean(0)\n",
    "\n",
    "    # Comute covariance\n",
    "    S = np.dot(Y_cent.T, Y_cent)/Y.shape[0]\n",
    "    lambd, U = np.linalg.eig(S)\n",
    "\n",
    "    # Choose number of eigenvectors\n",
    "    sigma2 = np.sum(lambd[q:])/(Y.shape[1]-q)\n",
    "    l = np.sqrt(lambd[:q]-sigma2)\n",
    "    W = U[:, :q]*l[None, :]\n",
    "    return W, sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we may not wish to compute the eigenvectors of the\n",
    "covariance matrix directly. This is because it requires us to estimate\n",
    "the covariance, which involves a sum of squares term, before estimating\n",
    "the eigenvectors. We can estimate the eigenvectors directly either\n",
    "through [QR\n",
    "decomposition](http://en.wikipedia.org/wiki/QR_decomposition) or\n",
    "[singular value\n",
    "decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition).\n",
    "We saw a similar issue arise when , where we also wished to avoid\n",
    "computation of $\\mathbf{Z}^\\top\\mathbf{Z}$ (or in the case of\n",
    "$\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}$).\n",
    "\n",
    "::: {.cell .markdown}\n",
    "\n",
    "Posterior for Principal Component Analysis\n",
    "==========================================\n",
    "\n",
    "Under the latent variable model justification for principal component\n",
    "analysis, we are normally interested in inferring something about the\n",
    "latent variables given the data. This is the distribution, $$\n",
    "p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :})\n",
    "$$ for any given data point. Determining this density turns out to be\n",
    "very similar to the approach for determining the Bayesian posterior of\n",
    "$\\mathbf{ w}$ in Bayesian linear regression, only this time we place the\n",
    "prior density over $\\mathbf{ z}_{i, :}$ instead of $\\mathbf{ w}$. The\n",
    "posterior is proportional to the joint density as follows, $$\n",
    "p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) \\propto p(\\mathbf{ y}_{i,\n",
    ":}|\\mathbf{W}, \\mathbf{ z}_{i, :}, \\sigma^2) p(\\mathbf{ z}_{i, :})\n",
    "$$ And as in the Bayesian linear regression case we first consider the\n",
    "log posterior, $$\n",
    "\\log p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) = \\log p(\\mathbf{ y}_{i, :}|\\mathbf{W},\n",
    "\\mathbf{ z}_{i, :}, \\sigma^2) + \\log p(\\mathbf{ z}_{i, :}) + \\text{const}\n",
    "$$ where the constant is not dependent on $\\mathbf{ z}$. As before we\n",
    "collect the quadratic terms in $\\mathbf{ z}_{i, :}$ and we assemble them\n",
    "into a Gaussian density over $\\mathbf{ z}$. $$\n",
    "\\log p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) =\n",
    "-\\frac{1}{2\\sigma^2} (\\mathbf{ y}_{i, :} - \\mathbf{W}\\mathbf{ z}_{i,\n",
    ":})^\\top(\\mathbf{ y}_{i, :} - \\mathbf{W}\\mathbf{ z}_{i, :}) - \\frac{1}{2}\n",
    "\\mathbf{ z}_{i, :}^\\top \\mathbf{ z}_{i, :} + \\text{const}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Multiply out the terms in the brackets. Then collect the quadratic term\n",
    "and the linear terms together. Show that the posterior has the form $$\n",
    "\\mathbf{ z}_{i, :} | \\mathbf{W}\\sim \\mathcal{N}\\left(\\boldsymbol{ \\mu}_x,\\mathbf{C}_x\\right)\n",
    "$$ where $$\n",
    "\\mathbf{C}_x = \\left(\\sigma^{-2}\n",
    "\\mathbf{W}^\\top\\mathbf{W}+ \\mathbf{I}\\right)^{-1}\n",
    "$$ and $$\n",
    "\\boldsymbol{ \\mu}_x\n",
    "= \\mathbf{C}_x \\sigma^{-2}\\mathbf{W}^\\top \\mathbf{ y}_{i, :} \n",
    "$$ Compare this to the posterior for the Bayesian linear regression from\n",
    "last week, do they have similar forms? What matches and what differs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Implementation of the Posterior\n",
    "--------------------------------------\n",
    "\n",
    "Now let’s implement the system in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use the values for $\\mathbf{W}$ and $\\sigma^2$ you have computed, along\n",
    "with the data set $\\mathbf{Y}$ to compute the posterior density over\n",
    "$\\mathbf{Z}$. Write a function of the form\n",
    "\n",
    "``` python\n",
    "mu_x, C_x = posterior(Y, W, sigma2)\n",
    "```\n",
    "\n",
    "where `mu_x` and `C_x` are the posterior mean and posterior covariance\n",
    "for the given $\\mathbf{Y}$.\n",
    "\n",
    "Don’t forget to subtract the mean of the data `Y` inside your function\n",
    "before computing the posterior: remember we assumed at the beginning of\n",
    "our analysis that the data had been centred (i.e. the mean was removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 2 here\n",
    "\n",
    "\n",
    "# Answer Code\n",
    "# Write code for you answer to this exercise in this box\n",
    "# Do not delete these comments, otherwise you will get zero for this answer.\n",
    "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking.\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "def posterior(Y, W, sigma2):\n",
    "    Y_cent = Y - Y.mean(0)\n",
    "    # Compute posterior over X\n",
    "    C_x = \n",
    "    mu_x = \n",
    "    return mu_x, C_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerically Stable and Efficient Version\n",
    "----------------------------------------\n",
    "\n",
    "Just as we saw for and computation of a matrix such as\n",
    "$\\mathbf{Y}^\\top\\mathbf{Y}$ (or its centred version) can be a bad idea\n",
    "in terms of loss of numerical accuracy. Fortunately, we can find the\n",
    "eigenvalues and eigenvectors of the matrix $\\mathbf{Y}^\\top\\mathbf{Y}$\n",
    "without direct computation of the matrix. This can be done with the\n",
    "[*singular value\n",
    "decomposition*](http://en.wikipedia.org/wiki/Singular_value_decomposition).\n",
    "The singular value decompsition takes a matrix, $\\mathbf{Z}$ and\n",
    "represents it in the form, $$\n",
    "\\mathbf{Z} = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^\\top\n",
    "$$ where $\\mathbf{U}$ is a matrix of orthogonal vectors in the columns,\n",
    "meaning $\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$. It has the same number\n",
    "of rows and columns as $\\mathbf{Z}$. The matrices $\\mathbf{\\Lambda}$ and\n",
    "$\\mathbf{V}$ are both square with dimensionality given by the number of\n",
    "columns of $\\mathbf{Z}$. The matrix $\\mathbf{\\Lambda}$ is *diagonal* and\n",
    "$\\mathbf{V}$ is an orthogonal matrix so\n",
    "$\\mathbf{V}^\\top\\mathbf{V} = \\mathbf{V}\\mathbf{V}^\\top = \\mathbf{I}$.\n",
    "The eigenvalues of the matrix $\\mathbf{Y}^\\top\\mathbf{Y}$ are then given\n",
    "by the singular values of the matrix $\\mathbf{Y}^\\top$ squared and the\n",
    "eigenvectors are given by $\\mathbf{U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution for $\\mathbf{W}$\n",
    "-------------------------\n",
    "\n",
    "Given the singular value decomposition of $\\mathbf{Y}$ then we have $$\n",
    "\\mathbf{W}=\n",
    "\\mathbf{U}\\mathbf{L}\\mathbf{R}^\\top\n",
    "$$ where $\\mathbf{R}$ is an arbitrary rotation matrix. This implies that\n",
    "the posterior is given by $$\n",
    "\\mathbf{C}_x =\n",
    "\\left[\\sigma^{-2}\\mathbf{R}\\mathbf{L}^2\\mathbf{R}^\\top + \\mathbf{I}\\right]^{-1}\n",
    "$$ because $\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}$. Since, by\n",
    "convention, we normally take $\\mathbf{R} = \\mathbf{I}$ to ensure that\n",
    "the principal components are orthonormal we can write $$\n",
    "\\mathbf{C}_x = \\left[\\sigma^{-2}\\mathbf{L}^2 +\n",
    "\\mathbf{I}\\right]^{-1}\n",
    "$$ which implies that $\\mathbf{C}_x$ is actually diagonal with elements\n",
    "given by $$\n",
    "c_i = \\frac{\\sigma^2}{\\sigma^2 + \\ell^2_i}\n",
    "$$ and allows us to write $$\n",
    "\\boldsymbol{ \\mu}_x = [\\mathbf{L}^2 + \\sigma^2\n",
    "\\mathbf{I}]^{-1} \\mathbf{L} \\mathbf{U}^\\top \\mathbf{ y}_{i, :}\n",
    "$$ $$\n",
    "\\boldsymbol{ \\mu}_x = \\mathbf{D}\\mathbf{U}^\\top \\mathbf{ y}_{i, :}\n",
    "$$ where $\\mathbf{D}$ is a diagonal matrix with diagonal elements given\n",
    "by $d_{i} = \\frac{\\ell_i}{\\sigma^2 + \\ell_i^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic PCA algorithm using SVD\n",
    "def ppca(Y, q, center=True):\n",
    "    \"\"\"Probabilistic PCA through singular value decomposition\"\"\"\n",
    "    # remove mean\n",
    "    if center:\n",
    "        Y_cent = Y - Y.mean(0)\n",
    "    else:\n",
    "        Y_cent = Y\n",
    "        \n",
    "    # Comute singluar values, discard 'R' as we will assume orthogonal\n",
    "    U, sqlambd, _ = sp.linalg.svd(Y_cent.T,full_matrices=False)\n",
    "    lambd = (sqlambd**2)/Y.shape[0]\n",
    "    # Compute residual and extract eigenvectors\n",
    "    sigma2 = np.sum(lambd[q:])/(Y.shape[1]-q)\n",
    "    ell = np.sqrt(lambd[:q]-sigma2)\n",
    "    return U[:, :q], ell, sigma2\n",
    "\n",
    "def posterior(Y, U, ell, sigma2, center=True):\n",
    "    \"\"\"Posterior computation for the latent variables given the eigendecomposition.\"\"\"\n",
    "    if center:\n",
    "        Y_cent = Y - Y.mean(0)\n",
    "    else:\n",
    "        Y_cent = Y\n",
    "    C_x = np.diag(sigma2/(sigma2+ell**2))\n",
    "    d = ell/(sigma2+ell**2)\n",
    "    mu_x = np.dot(Y_cent, U)*d[None, :]\n",
    "    return mu_x, C_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robot Navigation Example\n",
    "------------------------\n",
    "\n",
    "In the next example we will load in data from a robot navigation\n",
    "problem. The data consists of wireless access point strengths as\n",
    "recorded by a robot performing a loop around the University of\n",
    "Washington’s Computer Science department in Seattle. The robot records\n",
    "all the wireless access points it can cache and stores their signal\n",
    "strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.robot_wireless()\n",
    "Y = data['Y']\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 215 observations of 30 different access points. In this case\n",
    "the model is suggesting that the access point signal strength should be\n",
    "linearly dependent on the location in the map. In other words we are\n",
    "expecting the access point strength for the $j$th access point at robot\n",
    "position $x_{i, :}$ to be represented by\n",
    "$y_{i, j} = \\mathbf{ w}_{j, :}^\\top \\mathbf{ z}_{i, :} + \\epsilon_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 2\n",
    "U, ell, sigma2 = ppca(Y, q)\n",
    "mu_x, C_x = posterior(Y, U, ell, sigma2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.plot(mu_x[:, 0], mu_x[:, 1], 'rx-')\n",
    "ax.set_title('Latent Variable: Robot Inferred Locations')\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "W = U*ell[None, :]\n",
    "ax.plot(W[:, 0], W[:, 1], 'bo')\n",
    "ax.set_title('Access Point Inferred Locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, ell, sigma2 = ppca(Y.T, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretations of Principal Component Analysis\n",
    "==============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship to Matrix Factorization\n",
    "------------------------------------\n",
    "\n",
    "We can use the robot naviation example to realise that PCA (and factor\n",
    "analysis) are very reminiscient of the that we used for introducing\n",
    "objective functions. In that system we used slightly different notation,\n",
    "$\\mathbf{u}_{i, :}$ for *user* location in our metaphorical library and\n",
    "$\\mathbf{v}_{j, :}$ for *item* location in our metaphorical library. To\n",
    "see how these systems are somewhat analagous, now let us think about the\n",
    "user as the robot and the items as the wifi access points. We can plot\n",
    "the relative location of both. This process is known as “SLAM”:\n",
    "simultaneous *localisation* and *mapping*. A latent variable model of\n",
    "the type we have developed is one way of performing SLAM. We have an\n",
    "estimate of the *landmarks* in the system (in this case WIFI access\n",
    "points) and we have an estimate of the robot position. These are\n",
    "analagous to the estimate of the user’s position and the estimate of the\n",
    "items positions in the library. In the matrix factorisation example\n",
    "users are informing us what items they are ‘close’ to by expressing\n",
    "their preferences, in the robot localization example the robot is\n",
    "informing us what access point it is close to by measuring signal\n",
    "strength.\n",
    "\n",
    "From a personal perspective, I find this analogy quite comforting. I\n",
    "think it is very arguable that one of the mechanisms through which we\n",
    "(as humans) may have developed higher reasoning is through the need to\n",
    "navigate around our environment, identifying landmarks and associating\n",
    "them with our search for food. If such a system were to exist, the idea\n",
    "that it could be readily adapted to other domains such as categorising\n",
    "the nature of the different foodstuffs we were able to forage is\n",
    "intriguing.\n",
    "\n",
    "From an algorithmic perspective, we also can now realise that matrix\n",
    "factorization and latent variable modelling are effectively the same\n",
    "thing. The only difference is the objective function and our\n",
    "probabilistic (or lack of probabilistic) treatment of the variables. But\n",
    "the prediction function for both systems, $$\n",
    "f_{i, j} =\n",
    "\\mathbf{u}_{i, :}^\\top \\mathbf{v}_{j, :} \n",
    "$$ for matrix factorization or $$\n",
    "f_{i, j} = \\mathbf{ z}_{i, :}^\\top \\mathbf{ w}_{j, :} \n",
    "$$ for probabilistic PCA and factor analysis are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Interpretations of PCA: Separating Model and Algorithm\n",
    "------------------------------------------------------------\n",
    "\n",
    "Since Hotelling first introduced his perspective on factor analysis as\n",
    "PCA there has been somewhat of a conflation of the idea of the model\n",
    "underlying PCA (for which it was very clear that Hotelling was inspired\n",
    "by Factor Analysis) and the algorithm that is used to fit that model:\n",
    "the eigenvalues and eigenvectors of the covariance matrix. The\n",
    "eigenvectors of an ellipsoid have been known since the middle of the\n",
    "19th century as the principal axes of the elipsoid, and they arise\n",
    "through the following additional ideas: seeking the orthogonal\n",
    "directions of *maximum variance* in a dataset. Pearson in 1901 arrived\n",
    "at the same algorithm driven by a desire to seek a *symmetric\n",
    "regression* between two covariate/response variables $x$ and $y$\n",
    "(Pearson, 1901). He is, therefore, often credited with the invention of\n",
    "principal component analysis, but to me this seems disengenous. His aim\n",
    "was very different from Hotellings, it was just happened that the\n",
    "optimal solution for his model was coincident with that of Hotelling.\n",
    "The approach is also known as the [Karhunen Loeve\n",
    "Transform](http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem)\n",
    "in stochastic process theory and in classical multidimensional scaling\n",
    "the same operation can be shown to be minimising a particular objective\n",
    "function based on interpoint distances in the data and the latent space\n",
    "(see the section on Classical Multidimensional Scaling in [Mardia, Kent\n",
    "and\n",
    "Bibby](http://store.elsevier.com/Multivariate-Analysis/Kanti-%20Mardia/isbn-9780124712522/))\n",
    "(Mardia et al., 1979). One of my own contributions to machine learning\n",
    "was deriving yet another model whose linear variant was solved by\n",
    "finding the principal subspace of the covariance matrix (an approach I\n",
    "termed dual probabilistic PCA or probabilistic principal coordinate\n",
    "analysis). Finally, the approach is sometimes referred to simply as\n",
    "singular value decomposition (SVD). The singular value decomposition of\n",
    "a data set has the following form, $$\n",
    "\\mathbf{Y}= \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{U}^\\top\n",
    "$$ where $\\mathbf{V}\\in\\Re^{n\\times n}$ and\n",
    "$\\mathbf{U}^\\in \\Re^{p\\times p}$ are square orthogonal matrices and\n",
    "$\\mathbf{\\Lambda}^{n \\times p}$ is zero apart from its first $p$\n",
    "diagonal entries. Singularvalue decomposition gives a diagonalisation of\n",
    "the covariance matrix, because under the SVD we have $$\n",
    "\\mathbf{Y}^\\top\\mathbf{Y}=\n",
    "\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^\\top\\mathbf{V} \\boldsymbol{\\Lambda}\n",
    "\\mathbf{U}^\\top = \\mathbf{U}\\boldsymbol{\\Lambda}^2 \\mathbf{U}^\\top\n",
    "$$ where $\\boldsymbol{\\Lambda}^2$ is now the eigenvalues of the\n",
    "covariane matrix and $\\mathbf{U}$ are the eigenvectors. So performing\n",
    "the SVD can simply be seen as another approach to determining the\n",
    "principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating Model and Algorithm\n",
    "------------------------------\n",
    "\n",
    "I’ve given a fair amount of personal thought to this situation and my\n",
    "own opinion that this confusion about method arises because of a\n",
    "conflation of model and algorithm. The model of Hotelling, that which he\n",
    "termed principal component analysis, was really a variant of factor\n",
    "analysis, and it was unfortunate that he chose to rename it. However,\n",
    "the algorithm he derived was a very convenient way of optimising a\n",
    "(simplified) factor analysis, and it’s therefore become very popular.\n",
    "The algorithm is also the optimal solution for many other models of the\n",
    "data, even some which might seem initally to be unrelated (e.g. seeking\n",
    "directions of maximum variance). It is only through the mathematics of\n",
    "this linear system (which also contains some intersting symmetries) that\n",
    "all these ides become related. However, as soon as we choose to\n",
    "non-linearise the system (e.g. through basis functions) we find that\n",
    "each of the non-linear intepretations we can derive for the different\n",
    "models each leads to a very different algorithm (if such an algorithm is\n",
    "possible). For example [principal\n",
    "curves](http://web.stanford.edu/~hastie/Papers/Principal_Curves.pdf) of\n",
    "Hastie and Stuetzle (1989) attempt to non-linearise the maximum variance\n",
    "interpretation, [kernel\n",
    "PCA](http://en.wikipedia.org/wiki/Kernel_principal_component_analysis)\n",
    "of Schölkopf et al. (1998) uses basis functions to form the eigenvalue\n",
    "problem in a nonlinear space, and my own work in this area\n",
    "[non-linearises the dual probabilistic\n",
    "PCA](http://jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf)\n",
    "(Lawrence, 2005).\n",
    "\n",
    "My conclusion is that when you are doing machine learning you should\n",
    "always have it clear in your mind what your *model* is and what your\n",
    "*algorithm* is. You can recognise your model because it normally\n",
    "contains a prediction function and an objective function. The algorithm\n",
    "on the other hand is the sequence of steps you implement on the computer\n",
    "to solve for the parameters of this model. For efficient implementation,\n",
    "we often modify our model to allow for faster algorithms, and this is a\n",
    "perfectly valid pragmatist’s approach, so conflation of model and\n",
    "algorithm is not always a bad thing. But for clarity of thinking and\n",
    "understanding it is necessary to maintain the separation and to maintain\n",
    "a handle on when and why we perform the conflation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA in Practice\n",
    "===============\n",
    "\n",
    "Principal component analysis is so effective in practice that there has\n",
    "almost developed a mini-industry in renaming the method itself (which is\n",
    "ironic, given its origin). In particular [Latent Semantic\n",
    "Indexing](http://en.wikipedia.org/wiki/Latent_semantic_indexing) in text\n",
    "processing is simply PCA on a particular representation of the term\n",
    "frequencies of the document. There is a particular fad to rename the\n",
    "eigenvectors after the nature of the data you are examining, perhaps\n",
    "initially triggered by [Turk and\n",
    "Pentland’s](http://www.face-rec.org/algorithms/PCA/jcn.pdf) paper on\n",
    "eigenfaces, but also with\n",
    "[eigenvoices](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1_2007_8/kuhn-%20junqua-eigenvoice-icslp1998.pdf)\n",
    "and [eigengenes](http://www.biomedcentral.com/1752-0509/1/54). This\n",
    "seems to be an instantiation of a wider, and hopefully subconcious,\n",
    "tendency in academia to attempt to differentiate one idea from the same\n",
    "idea in related fields in order to emphasise the novelty. The\n",
    "unfortunate result is somewhat of a confusing literature for relatively\n",
    "simple model. My recommendations would be as follows. If you have\n",
    "multivariate data, applying some form of principal component would seem\n",
    "to be a very good idea as a first step. Even if you intend to later\n",
    "perform classification or regression on your data, it can give you\n",
    "understanding of the structure of the underlying data and help you to\n",
    "develop your intuitions about the nature of your data. Intelligent\n",
    "plotting and interaction with your data is always a good think, and for\n",
    "high dimensional data that means that you need some way of\n",
    "visualisation, PCA is typically a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPCA Marginal Likelihood\n",
    "========================\n",
    "\n",
    "We have developed the posterior density over the latent variables given\n",
    "the data and the parameters, and due to symmetries in the underlying\n",
    "prediction function, it has a very similar form to its sister density,\n",
    "the posterior of the weights given the data from Bayesian regression.\n",
    "Two key differences are as follows. If we were to do a Bayesian multiple\n",
    "output regression we would find that the marginal likelihood of the data\n",
    "is independent across the features and correlated across the data, $$\n",
    "p(\\mathbf{Y}|\\mathbf{Z})\n",
    "= \\prod_{j=1}^p \\mathcal{N}\\left(\\mathbf{ y}_{:, j}|\\mathbf{0},\n",
    "\\alpha\\mathbf{Z}\\mathbf{Z}^\\top + \\sigma^2 \\mathbf{I}\\right)\n",
    "$$ where $\\mathbf{ y}_{:, j}$ is a column of the data matrix and the\n",
    "independence is across the *features*, in probabilistic PCA the marginal\n",
    "likelihood has the form, $$\n",
    "p(\\mathbf{Y}|\\mathbf{W}) = \\prod_{i=1}^n\\mathcal{N}\\left(\\mathbf{ y}_{i,\n",
    ":}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right)\n",
    "$$ where $\\mathbf{ y}_{i, :}$ is a row of the data matrix $\\mathbf{Y}$\n",
    "and the independence is across the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the Log Likelihood\n",
    "=================================\n",
    "\n",
    "The quality of the model can be assessed using the log likelihood of\n",
    "this Gaussian form. $$\n",
    "\\log p(\\mathbf{Y}|\\mathbf{W}) = -\\frac{n}{2} \\log \\left|\n",
    "\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right| -\\frac{1}{2}\n",
    "\\sum_{i=1}^n\\mathbf{ y}_{i, :}^\\top \\left(\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2\n",
    "\\mathbf{I}\\right)^{-1} \\mathbf{ y}_{i, :} +\\text{const}\n",
    "$$ but this can be computed more rapidly by exploiting the low rank form\n",
    "of the covariance covariance,\n",
    "$\\mathbf{C}= \\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}$ and the\n",
    "fact that $\\mathbf{W}= \\mathbf{U}\\mathbf{L}\\mathbf{R}^\\top$.\n",
    "Specifically, we first use the decomposition of $\\mathbf{W}$ to write:\n",
    "$$\n",
    "-\\frac{n}{2} \\log \\left| \\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right|\n",
    "= -\\frac{n}{2} \\sum_{i=1}^q \\log (\\ell_i^2 + \\sigma^2) - \\frac{n(p-q)}{2}\\log\n",
    "\\sigma^2,\n",
    "$$ where $\\ell_i$ is the $i$th diagonal element of $\\mathbf{L}$. Next,\n",
    "we use the [Woodbury matrix\n",
    "identity](http://en.wikipedia.org/wiki/Woodbury_matrix_identity) which\n",
    "allows us to write the inverse as a quantity which contains another\n",
    "inverse in a smaller matrix: $$\n",
    "(\\sigma^2 \\mathbf{I}+ \\mathbf{W}\\mathbf{W}^\\top)^{-1} =\n",
    "\\sigma^{-2}\\mathbf{I}-\\sigma^{-4}\\mathbf{W}{\\underbrace{(\\mathbf{I}+\\sigma^{-2}\\mathbf{W}^\\top\\mathbf{W})}_{\\mathbf{C}_x}}^{-1}\\mathbf{W}^\\top\n",
    "$$ So, it turns out that the original inversion of the $p \\times p$\n",
    "matrix can be done by forming a quantity which contains the inversion of\n",
    "a $q \\times q$ matrix which, moreover, turns out to be the quantity\n",
    "$\\mathbf{C}_x$ of the posterior.\n",
    "\n",
    "Now, we put everything together to obtain: $$\n",
    "\\log p(\\mathbf{Y}|\\mathbf{W}) = -\\frac{n}{2} \\sum_{i=1}^q\n",
    "\\log (\\ell_i^2 + \\sigma^2)\n",
    "- \\frac{n(p-q)}{2}\\log \\sigma^2 - \\frac{1}{2} \\text{tr}\\left(\\mathbf{Y}^\\top \\left(\n",
    "\\sigma^{-2}\\mathbf{I}-\\sigma^{-4}\\mathbf{W}\\mathbf{C}_x\n",
    "\\mathbf{W}^\\top \\right) \\mathbf{Y}\\right) + \\text{const},\n",
    "$$ where we used the fact that a scalar sum can be written as\n",
    "$\\sum_{i=1}^n\\mathbf{ y}_{i,:}^\\top \\mathbf{K}\\mathbf{ y}_{i,:} = \\text{tr}\\left(\\mathbf{Y}^\\top \\mathbf{K}\\mathbf{Y}\\right)$,\n",
    "for any matrix $\\mathbf{K}$ of appropriate dimensions. We now use the\n",
    "properties of the trace\n",
    "$\\text{tr}\\left(\\mathbf{A}+\\mathbf{B}\\right)=\\text{tr}\\left(\\mathbf{A}\\right)+\\text{tr}\\left(\\mathbf{B}\\right)$\n",
    "and\n",
    "$\\text{tr}\\left(c \\mathbf{A}\\right) = c \\text{tr}\\left(\\mathbf{A}\\right)$,\n",
    "where $c$ is a scalar and $\\mathbf{A},\\mathbf{B}$ matrices of compatible\n",
    "sizes. Therefore, the final log likelihood takes the form: $$\n",
    "\\log p(\\mathbf{Y}|\\mathbf{W}) = -\\frac{n}{2}\n",
    "\\sum_{i=1}^q \\log (\\ell_i^2 + \\sigma^2) - \\frac{n(p-q)}{2}\\log \\sigma^2 -\n",
    "\\frac{\\sigma^{-2}}{2} \\text{tr}\\left(\\mathbf{Y}^\\top \\mathbf{Y}\\right)\n",
    "+\\frac{\\sigma^{-4}}{2} \\text{tr}\\left(\\mathbf{B}\\mathbf{C}_x\\mathbf{B}^\\top\\right) +\n",
    "\\text{const}\n",
    "$$ where we also defined $\\mathbf{B}=\\mathbf{Y}^\\top\\mathbf{W}$.\n",
    "Finally, notice that\n",
    "$\\text{tr}\\left(\\mathbf{Y}\\mathbf{Y}^\\top\\right)=\\text{tr}\\left(\\mathbf{Y}^\\top\\mathbf{Y}\\right)$\n",
    "can be computed faster as the sum of all the elements of\n",
    "$\\mathbf{Y}\\circ\\mathbf{Y}$, where $\\circ$ denotes the element-wise (or\n",
    "[Hadamard](http://en.wikipedia.org/wiki/Hadamard_product_(matrices)))\n",
    "product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non Linear Latent Variable Models\n",
    "================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficulty for Probabilistic Approaches\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_3(diagrams='./dimred/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge for composition of probabilistic models is that you need\n",
    "to propagate a probability densities through non linear mappings. This\n",
    "allows you to create broader classes of probability density.\n",
    "Unfortunately it renders the resulting densities *intractable*.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A two dimensional grid mapped into three dimensions to form a\n",
    "two dimensional manifold.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_2(diagrams='./dimred/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A one dimensional line mapped into two dimensions by two\n",
    "separate independent functions. Each point can be mapped exactly through\n",
    "the mappings.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_1(diagrams='./dimred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/gaussian-through-nonlinear.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Gaussian density over the input of a non linear function\n",
    "leads to a very non Gaussian output. Here the output is multimodal.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dual Probabilistic PCA and GP-LVM\n",
    "================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dual Probabilistic PCA\n",
    "----------------------\n",
    "\n",
    "**Probabilistic PCA**\n",
    "\n",
    "-   We have seen that PCA has a probabilistic interpretation (Tipping\n",
    "    and Bishop, 1999b).\n",
    "-   It is difficult to \\`non-linearise’ directly.\n",
    "-   GTM and Density Networks are an attempt to do so.\n",
    "\n",
    "**Dual Probabilistic PCA**\n",
    "\n",
    "-   There is an alternative probabilistic interpretation of PCA\n",
    "    (Lawrence, 2005).\n",
    "-   This interpretation can be made non-linear.\n",
    "-   The result is non-linear probabilistic PCA.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "**Dual Probabilistic PCA**\n",
    "\n",
    "-   Define *linear-Gaussian relationship* between latent variables and\n",
    "    data.\n",
    "-   **Novel** Latent variable approach:\n",
    "-   Define Gaussian prior over , $\\mathbf{W}$.\n",
    "-   Integrate out *parameters*. `{=html}     </td>`\n",
    "    `{=html}     <td width=\"45%\">`\n",
    "    <img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/gplvm_graph.png\" style=\"width:50%\">\n",
    "    $$\n",
    "    p\\left(\\mathbf{Y}|\\mathbf{Z},\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{ y}_{i,:}|\\mathbf{W}\\mathbf{ z}_{i,:},\\sigma^{2}\\mathbf{I}\\right)\n",
    "    $$ $$\n",
    "     p\\left(\\mathbf{W}\\right)=\\prod_{i=1}^{p}\\mathcal{N}\\left(\\mathbf{ w}_{i,:}|\\mathbf{0},\\mathbf{I}\\right)$$\n",
    "    $$\n",
    "     p\\left(\\mathbf{Y}|\\mathbf{Z}\\right)=\\prod_{j=1}^{p}\\mathcal{N}\\left(\\mathbf{ y}_{:,j}|\\mathbf{0},\\mathbf{Z}\\mathbf{Z}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)$$\n",
    "    `{=html}     </td>` `{=html}     </tr>` `{=html}     </table>`\n",
    "\n",
    "**Dual** Probabilistic PCA Max. Likelihood Soln (Lawrence, n.d., p.\n",
    "@Lawrence:pnpca05)\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/gplvm_graph.png\" style=\"width:25%\">\n",
    "\n",
    "$$\n",
    "p\\left(\\mathbf{Y}|\\mathbf{Z}\\right)=\\prod_{j=1}^{p}\\mathcal{N}\\left(\\mathbf{ y}_{:,j}|\\mathbf{0},\\mathbf{Z}\\mathbf{Z}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p\\left(\\mathbf{Y}|\\mathbf{Z}\\right)=\\prod_{j=1}^{p}\\mathcal{N}\\left(\\mathbf{ y}_{:,j}|\\mathbf{0},\\mathbf{K}\\right),\\quad\\quad\\mathbf{K}=\\mathbf{Z}\\mathbf{\\mathbf{Z}}^{\\top}+\\sigma^{2}\\mathbf{I}\n",
    "$$ $$\n",
    "\\log p\\left(\\mathbf{Y}|\\mathbf{Z}\\right)=-\\frac{p}{2}\\log\\left|\\mathbf{K}\\right|-\\frac{1}{2}\\text{tr}\\left(\\mathbf{K}^{-1}\\mathbf{Y}\\mathbf{Y}^{\\top}\\right)+\\mbox{const.}\n",
    "$$ If $\\mathbf{U}_{q}^{\\prime}$ are first $q$ principal eigenvectors of\n",
    "$p^{-1}\\mathbf{Y}\\mathbf{Y}^{\\top}$ and the corresponding eigenvalues\n",
    "are $\\Lambda_{q}$, $$\n",
    "\\mathbf{Z}=\\mathbf{U^{\\prime}}_{q}\\mathbf{L}\\mathbf{R}^{\\top},\\quad\\quad\\mathbf{L}=\\left(\\Lambda_{q}-\\sigma^{2}\\mathbf{I}\\right)^{\\frac{1}{2}}\n",
    "$$ where $\\mathbf{R}$ is an arbitrary rotation matrix. $$\n",
    "p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{ y}_{i,:}|\\mathbf{0},\\mathbf{C}\\right),\\quad\\quad\\mathbf{C}=\\mathbf{W}\\mathbf{W}^{\\top}+\\sigma^{2}\\mathbf{I}$$\n",
    "$$\n",
    "\\log p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=-\\frac{n}{2}\\log\\left|\\mathbf{C}\\right|-\\frac{1}{2}\\text{tr}\\left(\\mathbf{C}^{-1}\\mathbf{Y}^{\\top}\\mathbf{Y}\\right)+\\mbox{const.}\n",
    "$$ If $\\mathbf{U}_{q}$ are first $q$ principal eigenvectors of\n",
    "$n^{-1}\\mathbf{Y}^{\\top}\\mathbf{Y}$ and the corresponding eigenvalues\n",
    "are $\\Lambda_{q}$, $$\n",
    "\\mathbf{W}=\\mathbf{U}_{q}\\mathbf{L}\\mathbf{R}^{\\top},\\quad\\quad\\mathbf{L}=\\left(\\Lambda_{q}-\\sigma^{2}\\mathbf{I}\\right)^{\\frac{1}{2}}\n",
    "$$ where $\\mathbf{R}$ is an arbitrary rotation matrix.\n",
    "\n",
    "**The Eigenvalue Problems are equivalent**\n",
    "\n",
    "-   Solution for Probabilistic PCA (solves for the mapping) $$\n",
    "    \\mathbf{Y}^{\\top}\\mathbf{Y}\\mathbf{U}_{q}=\\mathbf{U}_{q}\\Lambda_{q}\\quad\\quad\\quad\\mathbf{W}=\\mathbf{U}_{q}\\mathbf{L}\\mathbf{V}^{\\top}\n",
    "        $$\n",
    "\n",
    "-   Solution for Dual Probabilistic PCA (solves for the latent\n",
    "    positions) $$\n",
    "    \\mathbf{Y}\\mathbf{Y}^{\\top}\\mathbf{U}_{q}^{\\prime}=\\mathbf{U}_{q}^{\\prime}\\Lambda_{q}\\quad\\quad\\quad\\mathbf{Z}=\\mathbf{U}_{q}^{\\prime}\\mathbf{L}\\mathbf{V}^{\\top}\n",
    "      $$\n",
    "\n",
    "-   Equivalence is from $$\n",
    "    \\mathbf{U}_{q}=\\mathbf{Y}^{\\top}\\mathbf{U}_{q}^{\\prime}\\Lambda_{q}^{-\\frac{1}{2}}\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Inference by Rejection Sampling\n",
    "----------------------------------------\n",
    "\n",
    "One view of Bayesian inference is to assume we are given a mechanism for\n",
    "generating samples, where we assume that mechanism is representing on\n",
    "accurate view on the way we believe the world works.\n",
    "\n",
    "This mechanism is known as our *prior* belief.\n",
    "\n",
    "We combine our prior belief with our observations of the real world by\n",
    "discarding all those samples that are inconsistent with our prior. The\n",
    "*likelihood* defines mathematically what we mean by inconsistent with\n",
    "the prior. The higher the noise level in the likelihood, the looser the\n",
    "notion of consistent.\n",
    "\n",
    "The samples that remain are considered to be samples from the\n",
    "*posterior*.\n",
    "\n",
    "This approach to Bayesian inference is closely related to two sampling\n",
    "techniques known as *rejection sampling* and *importance sampling*. It\n",
    "is realized in practice in an approach known as *approximate Bayesian\n",
    "computation* (ABC) or likelihood-free inference.\n",
    "\n",
    "In practice, the algorithm is often too slow to be practical, because\n",
    "most samples will be inconsistent with the data and as a result the\n",
    "mechanism has to be operated many times to obtain a few posterior\n",
    "samples.\n",
    "\n",
    "However, in the Gaussian process case, when the likelihood also assumes\n",
    "Gaussian noise, we can operate this mechanism mathematically, and obtain\n",
    "the posterior density *analytically*. This is the benefit of Gaussian\n",
    "processes.\n",
    "\n",
    "First we will load in two python functions for computing the covariance\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov,\n",
    "                     name='Exponentiated Quadratic',\n",
    "                     shortname='eq',                     \n",
    "                     lengthscale=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sample from a multivariate normal density (a multivariate\n",
    "Gaussian), using the covariance function as the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rejection_samples(kernel=kernel, \n",
    "    diagrams='./gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('gp_rejection_sample{sample:0>3}.png', \n",
    "                            directory='./gp', \n",
    "                            sample=IntSlider(1,1,5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample003.png\" style=\"width:100%\">\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample004.png\" style=\"width:100%\">\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp_rejection_sample005.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>One view of Bayesian inference is we have a machine for\n",
    "generating samples (the *prior*), and we discard all samples\n",
    "inconsistent with our data, leaving the samples of interest (the\n",
    "*posterior*). This is a rejection sampling view of Bayesian inference.\n",
    "The Gaussian process allows us to do this analytically by multiplying\n",
    "the *prior* by the *likelihood*.</i>\n",
    "\n",
    "**Prior for Functions**\n",
    "\n",
    "-   Probability Distribution over Functions\n",
    "\n",
    "-   Functions are infinite dimensional.\n",
    "\n",
    "    -   Prior distribution over *instantiations* of the function: finite\n",
    "        dimensional objects.\n",
    "    -   Can prove by induction that GP is ‘consistent’.\n",
    "\n",
    "-   Mean and Covariance Functions\n",
    "\n",
    "-   Instead of mean and covariance matrix, GP is defined by mean\n",
    "    function and covariance function.\n",
    "\n",
    "    -   Mean function often taken to be zero or constant.\n",
    "    -   Covariance function must be *positive definite*.\n",
    "    -   Class of valid covariance functions is the same as the class of\n",
    "        *Mercer kernels*.\n",
    "\n",
    "**Zero mean Gaussian Process**\n",
    "\n",
    "-   A (zero mean) Gaussian process likelihood is of the form$$\n",
    "      p\\left(\\mathbf{ y}|\\mathbf{Z}\\right)=N\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right),$$\n",
    "    where $\\mathbf{K}$ is the covariance function or .\n",
    "\n",
    "-   The with noise has the form$$\n",
    "      \\mathbf{K}=\\mathbf{Z}\\mathbf{Z}^{\\top}+\\sigma^{2}\\mathbf{I}$$\n",
    "\n",
    "-   Priors over non-linear functions are also possible.\n",
    "\n",
    "    -   To see what functions look like, we can sample from the prior\n",
    "        process.\n",
    "\n",
    "**Posterior Distribution over Functions**\n",
    "\n",
    "-   Gaussian processes are often used for regression.\n",
    "-   We are given a known inputs $\\mathbf{Z}$ and targets $\\mathbf{Y}$.\n",
    "-   We assume a prior distribution over functions by selecting a kernel.\n",
    "-   Combine the prior with data to get a distribution over functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiated Quadratic Covariance\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s Kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov,\n",
    "                     name='Exponentiated Quadratic',\n",
    "                     shortname='eq',                     \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\exp\\left(-\\frac{\\ltwoNorm{\\inputVector-\\inputVector^\\prime}^2}{2\\lengthScale^2}\\right)',\n",
    "                     lengthscale=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_func(kernel=kernel, diagrams='./kern/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponentiated quadratic covariance, also known as the Gaussian\n",
    "covariance or the RBF covariance and the squared exponential. Covariance\n",
    "between two points is related to the negative exponential of the squared\n",
    "distnace between those points. This covariance function can be derived\n",
    "in a few different ways: as the infinite limit of a radial basis\n",
    "function neural network, as diffusion in the heat equation, as a\n",
    "Gaussian filter in *Fourier space* or as the composition as a series of\n",
    "linear filters applied to a base function.\n",
    "\n",
    "The covariance takes the following form, $$\n",
    "k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right)\n",
    "$$ where $\\ell$ is the *length scale* or *time scale* of the process and\n",
    "$\\alpha$ represents the overall process variance.\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/eq_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/eq_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>The exponentiated quadratic covariance function.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateObject(rotationMatrix, handle):\n",
    "for i = 1:prod(size(handle))\n",
    "    type = get(handle(i), 'type');\n",
    "    if strcmp(type, 'text'):\n",
    "        xy = get(handle(i), 'position');\n",
    "        xy(1:2) = rotationMatrix*xy(1:2)';\n",
    "        set(handle(i), 'position', xy);\n",
    "    else:\n",
    "        xd = get(handle(i), 'xdata');\n",
    "        yd = get(handle(i), 'ydata');\n",
    "        new = rotationMatrix*[xd(:)'; yd(:)'];\n",
    "        set(handle(i), 'xdata', new(1, :));\n",
    "        set(handle(i), 'ydata', new(2, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Covariance Parameters\n",
    "------------------------------\n",
    "\n",
    "Can we determine covariance parameters from the data?\n",
    "\n",
    "$$\n",
    "\\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=\\frac{1}{(2\\pi)^\\frac{n}{2}{\\det{\\mathbf{K}}^{\\frac{1}{2}}}}{\\exp\\left(-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=\\frac{1}{(2\\pi)^\\frac{n}{2}\\color{blue}{\\det{\\mathbf{K}}^{\\frac{1}{2}}}}\\color{red}{\\exp\\left(-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\log \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=&\\color{blue}{-\\frac{1}{2}\\log\\det{\\mathbf{K}}}\\color{red}{-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}} \\\\ &-\\frac{n}{2}\\log2\\pi\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(\\boldsymbol{ \\theta}) = \\color{blue}{\\frac{1}{2}\\log\\det{\\mathbf{K}}} + \\color{red}{\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      clf\n",
    "      lambda1 = 3;\n",
    "      lambda2 = 1;\n",
    "      t = linspace(-pi, pi, 200);\n",
    "      R = [sqrt(2)/2 -sqrt(2)/2; sqrt(2)/2 sqrt(2)/2];\n",
    "      xy = R*[lambda1*sin(t); lambda2*cos(t)];\n",
    "      line(xy(1, :), xy(2, :), 'linewidth', 3, 'color', blackColor);\n",
    "      axis off, axis equal\n",
    "      a = arrow([0 lambda1*R(1, 1)], [0 lambda1*R(2, 1)]);\n",
    "      set(a, 'linewidth', 3, 'color', blueColor);\n",
    "      a = arrow([0 lambda2*R(1, 2)], [0 lambda2*R(2, 2)]);\n",
    "      set(a, 'linewidth', 3, 'color', blueColor);\n",
    "      xlim = get(gca, 'xlim');\n",
    "      xspan = xlim(2) - xlim(1);\n",
    "      ylim = get(gca, 'ylim');\n",
    "      yspan = ylim(2) - ylim(1);\n",
    "      text(lambda1*0.5*R(1, 1)-0.05*xspan, lambda1*0.5*R(2, 1)-yspan*0.05, '$\\eigenvalue_1$')\n",
    "      text(lambda2*0.5*R(1, 2)-0.05*xspan, lambda2*0.5*R(2, 2)-yspan*0.05, '$\\eigenvalue_2$')\n",
    "      fileName = 'gpOptimiseEigen';\n",
    "      printLatexPlot(fileName, directory, 0.45*textWidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capacity Control through the Determinant\n",
    "----------------------------------------\n",
    "\n",
    "The parameters are *inside* the covariance function (matrix).\n",
    "$$k_{i, j} = k(\\mathbf{ x}_i, \\mathbf{ x}_j; \\boldsymbol{ \\theta})$$\n",
    "\n",
    "$$\\mathbf{K}= \\mathbf{R}\\boldsymbol{ \\Lambda}^2 \\mathbf{R}^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpoptimizePlot1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimize-eigen.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "$\\boldsymbol{ \\Lambda}$ represents distance on axes. $\\mathbf{R}$ gives\n",
    "rotation.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "-   $\\boldsymbol{ \\Lambda}$ is *diagonal*,\n",
    "    $\\mathbf{R}^\\top\\mathbf{R}= \\mathbf{I}$.\n",
    "-   Useful representation since\n",
    "    $\\det{\\mathbf{K}} = \\det{\\boldsymbol{ \\Lambda}^2} = \\det{\\boldsymbol{ \\Lambda}}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = './gp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_capacity(rotate_angle=np.pi/4, lambda1 = 0.5, lambda2 = 0.3, diagrams = './gp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise-determinant009.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The determinant of the covariance is dependent only on the\n",
    "eigenvalues. It represents the ‘footprint’ of the Gaussian.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf\n",
    "    includeText = [];\n",
    "    counter = 0;\n",
    "    plotWidth = 0.6*textWidth;\n",
    "    lambda1 = 3;\n",
    "    lambda2 = 1;\n",
    "    t = linspace(-pi, pi, 200);\n",
    "    R = [sqrt(2)/2 -sqrt(2)/2; sqrt(2)/2 sqrt(2)/2];\n",
    "    xy = [lambda1*sin(t); lambda2*cos(t)];\n",
    "    contourHand = line(xy(1, :), xy(2, :), 'color', blackColor);\n",
    "    xy = [lambda1*sin(t); lambda2*cos(t)]*2;\n",
    "    lim = [-1 1]*max([lambda1 lambda2])*2.2;\n",
    "    set(gca, 'xlim', lim, 'ylim', lim)\n",
    "    axis equal\n",
    "\n",
    "\n",
    "    contourHand = [contourHand line(xy(1, :), xy(2, :), 'color', blackColor)];\n",
    "    set(contourHand, 'linewidth', 2, 'color', redColor)\n",
    "    arrowHand = arrow([0 lambda1], [0 0]);\n",
    "    arrowHand = [arrowHand arrow([0 0], [0 lambda2])];\n",
    "    set(arrowHand, 'linewidth', 3, 'color', blackColor);\n",
    "    xlim = get(gca, 'xlim');\n",
    "    xspan = xlim(2) - xlim(1);\n",
    "    ylim = get(gca, 'ylim');\n",
    "    yspan = ylim(2) - ylim(1);\n",
    "    eigLabel = text(lambda1*0.5, -yspan*0.05, '$\\eigenvalue_1$', 'horizontalalignment', 'center');\n",
    "    eigLabel = [eigLabel text(-0.05*xspan, lambda2*0.5, '$\\eigenvalue_2$', 'horizontalalignment', 'center')];\n",
    "    xlabel('$\\dataScalar_1$')\n",
    "    ylabel('$\\dataScalar_2$')\n",
    "    \n",
    "    box off\n",
    "    xlim = get(gca, 'xlim');\n",
    "    ylim = get(gca, 'ylim');\n",
    "    line([xlim(1) xlim(1)], ylim, 'color', blackColor)\n",
    "    line(xlim, [ylim(1) ylim(1)], 'color', blackColor)\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "\n",
    "    y = [1.2 1.4];\n",
    "    dataHand = line(y(1), y(2), 'marker', 'x', 'markersize', markerSize, 'linewidth', markerWidth, 'color', blackColor);\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "\n",
    "    \n",
    "    rotateObject(rotationMatrix, arrowHand);\n",
    "    rotateObject(rotationMatrix, contourHand);\n",
    "    rotateObject(rotationMatrix, eigLabel);\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "    \n",
    "    printLatexText(includeText, 'gpOptimiseQuadraticIncludeText.tex', directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise-quadratic002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The data fit term of the Gaussian process is a quadratic loss\n",
    "centered around zero. This has eliptical contours, the principal axes of\n",
    "which are given by the covariance matrix.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Data Fit\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Fit Term\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import gp_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(125)\n",
    "diagrams = './gp'\n",
    "\n",
    "black_color=[0., 0., 0.]\n",
    "red_color=[1., 0., 0.]\n",
    "blue_color=[0., 0., 1.]\n",
    "magenta_color=[1., 0., 1.]\n",
    "fontsize=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lim = [-2.2, 2.2]\n",
    "y_ticks = [-2, -1, 0, 1, 2]\n",
    "x_lim = [-2, 2]\n",
    "x_ticks = [-2, -1, 0, 1, 2]\n",
    "err_y_lim = [-12, 20]\n",
    "\n",
    "linewidth=3\n",
    "markersize=15\n",
    "markertype='.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 6)[:, np.newaxis]\n",
    "xtest = np.linspace(x_lim[0], x_lim[1], 200)[:, np.newaxis]\n",
    "\n",
    "# True data\n",
    "true_kern = GPy.kern.RBF(1) + GPy.kern.White(1)\n",
    "true_kern.rbf.lengthscale = 1.0\n",
    "true_kern.white.variance = 0.01\n",
    "K = true_kern.K(x) \n",
    "y = np.random.multivariate_normal(np.zeros((6,)), K, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted model\n",
    "kern = GPy.kern.RBF(1) + GPy.kern.White(1)\n",
    "kern.rbf.lengthscale = 1.0\n",
    "kern.white.variance = 0.01\n",
    "\n",
    "lengthscales = np.asarray([0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 4, 8, 16, 100])\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=plot.one_figsize)    \n",
    "fig2, ax2 = plt.subplots(figsize=plot.one_figsize)    \n",
    "line = ax2.semilogx(np.NaN, np.NaN, 'x-', \n",
    "                    color=black_color)\n",
    "ax.set_ylim(err_y_lim)\n",
    "ax.set_xlim([0.025, 32])\n",
    "ax.grid(True)\n",
    "ax.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "ax.set_xticklabels(['$10^{-2}$', '$10^{-1}$', '$10^0$', '$10^1$', '$10^2$'])\n",
    "\n",
    "\n",
    "err = np.zeros_like(lengthscales)\n",
    "err_log_det = np.zeros_like(lengthscales)\n",
    "err_fit = np.zeros_like(lengthscales)\n",
    "\n",
    "counter = 0\n",
    "for i, ls in enumerate(lengthscales):\n",
    "        kern.rbf.lengthscale=ls\n",
    "        K = kern.K(x) \n",
    "        invK, L, Li, log_det_K = GPy.util.linalg.pdinv(K)\n",
    "        err[i] = 0.5*(log_det_K + np.dot(np.dot(y.T,invK),y))\n",
    "        err_log_det[i] = 0.5*log_det_K\n",
    "        err_fit[i] = 0.5*np.dot(np.dot(y.T,invK), y)\n",
    "        Kx = kern.K(x, xtest)\n",
    "        ypred_mean = np.dot(np.dot(Kx.T, invK), y)\n",
    "        ypred_var = kern.Kdiag(xtest) - np.sum((np.dot(Kx.T,invK))*Kx.T, 1)\n",
    "        ypred_sd = np.sqrt(ypred_var)\n",
    "        ax1.clear()\n",
    "        _ = gp_tutorial.gpplot(xtest.flatten(),\n",
    "                               ypred_mean.flatten(),\n",
    "                               ypred_mean.flatten()-2*ypred_sd.flatten(),\n",
    "                               ypred_mean.flatten()+2*ypred_sd.flatten(), \n",
    "                               ax=ax1)\n",
    "        x_lim = ax1.get_xlim()\n",
    "        ax1.set_ylabel('$f(x)$', fontsize=fontsize)\n",
    "        ax1.set_xlabel('$x$', fontsize=fontsize)\n",
    "\n",
    "        p = ax1.plot(x, y, markertype, color=black_color, markersize=markersize, linewidth=linewidth)\n",
    "        ax1.set_ylim(y_lim)\n",
    "        ax1.set_xlim(x_lim)                                    \n",
    "        ax1.set_xticks(x_ticks)\n",
    "        #ax.set(box=False)\n",
    "           \n",
    "        ax1.plot([x_lim[0], x_lim[0]], y_lim, color=black_color)\n",
    "        ax1.plot(x_lim, [y_lim[0], y_lim[0]], color=black_color)\n",
    "\n",
    "        file_name = 'gp-optimise{counter:0>3}.svg'.format(counter=counter)\n",
    "        mlai.write_figure(os.path.join(diagrams, file_name),\n",
    "                          figure=fig1,\n",
    "                          transparent=True)\n",
    "        counter += 1\n",
    "\n",
    "        ax2.clear()\n",
    "        t = ax2.semilogx(lengthscales[0:i+1], err[0:i+1], 'x-', \n",
    "                        color=magenta_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        t2 = ax2.semilogx(lengthscales[0:i+1], err_log_det[0:i+1], 'x-', \n",
    "                         color=blue_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        t3 = ax2.semilogx(lengthscales[0:i+1], err_fit[0:i+1], 'x-', \n",
    "                         color=red_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        ax2.set_ylim(err_y_lim)\n",
    "        ax2.set_xlim([0.025, 32])\n",
    "        ax2.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "        ax2.set_xticklabels(['$10^{-2}$', '$10^{-1}$', '$10^0$', '$10^1$', '$10^2$'])\n",
    "\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax2.set_ylabel('negative log likelihood', fontsize=fontsize)\n",
    "        ax2.set_xlabel('length scale, $\\ell$', fontsize=fontsize)\n",
    "        file_name = 'gp-optimise{counter:0>3}.svg'.format(counter=counter)\n",
    "        mlai.write_figure(os.path.join(diagrams, file_name),\n",
    "                          figure=fig2,\n",
    "                          transparent=True)\n",
    "        counter += 1\n",
    "        #ax.set_box(False)\n",
    "        xlim = ax2.get_xlim()\n",
    "        ax2.plot([xlim[0], xlim[0]], err_y_lim, color=black_color)\n",
    "        ax2.plot(xlim, [err_y_lim[0], err_y_lim[0]], color=black_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise006.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise010.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise016.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gp-optimise021.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Variation in the data fit term, the capacity term and the\n",
    "negative log likelihood for different lengthscales.</i>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "**Dual Probabilistic PCA** \\* Define *linear-Gaussian relationship*\n",
    "between latent variables and data. \\* **Novel** Latent variable\n",
    "approach: \\* Define Gaussian prior over *parameteters*, $\\mathbf{W}$. \\*\n",
    "Integrate out *parameters*.\n",
    "\n",
    "-   Inspection of the marginal likelihood shows …\n",
    "    -   The covariance matrix is a covariance function.\n",
    "    -   We recognise it as the ‘linear kernel’.\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/gplvm_graph.png\" style=\"width:50%\">\n",
    "$$\n",
    " p\\left(\\mathbf{Y}|\\mathbf{Z},\\mathbf{W}\\right)=\\prod_{i=1}^{n}N\\left(\\mathbf{ y}_{i,:}|\\mathbf{W}\\mathbf{ z}_{i,:},\\sigma^{2}\\mathbf{I}\\right)$$\n",
    "$$\n",
    " p\\left(\\mathbf{W}\\right)=\\prod_{i=1}^{d}N\\left(\\mathbf{ w}_{i,:}|\\mathbf{0},\\mathbf{I}\\right)$$\n",
    "\n",
    "$$\n",
    " p\\left(\\mathbf{Y}|\\mathbf{Z}\\right)=\\prod_{j=1}^{d}N\\left(\\mathbf{ y}_{:,j}|\\mathbf{0},\\mathbf{Z}\\mathbf{Z}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)$$\n",
    "$$\n",
    " p\\left(\\mathbf{Y}|\\mathbf{Z}\\right)=\\prod_{j=1}^{d}N\\left(\\mathbf{ y}_{:,j}|\\mathbf{0},\\mathbf{K}\\right)$$\n",
    "$$\n",
    "\\mathbf{K}=\\mathbf{Z}\\mathbf{Z}^{\\top}+\\sigma^{2}\\mathbf{I}$$ This is a\n",
    "product of Gaussian processes with linear kernels. $$\n",
    "\\mathbf{K}=?\n",
    "$$ Replace linear kernel with non-linear kernel for non-linear model.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "**EQ Kernel**\n",
    "\n",
    "-   The RBF kernel has the form\n",
    "    $k_{i,j}=k\\left(\\mathbf{ z}_{i,:},\\mathbf{ z}_{j,:}\\right),$ where\n",
    "    $$\n",
    "    k\\left(\\mathbf{ z}_{i,:},\\mathbf{ z}_{j,:}\\right)=\\alpha\\exp\\left(-\\frac{\\left(\\mathbf{ z}_{i,:}-\\mathbf{ z}_{j,:}\\right)^{\\top}\\left(\\mathbf{ z}_{i,:}-\\mathbf{ z}_{j,:}\\right)}{2\\ell^{2}}\\right).\n",
    "    $$\n",
    "\n",
    "-   No longer possible to optimise wrt $\\mathbf{Z}$ via an eigenvalue\n",
    "    problem.\n",
    "\n",
    "-   Instead find gradients with respect to $\\mathbf{Z},\\alpha,\\ell$ and\n",
    "    $\\sigma^{2}$ and optimise using gradient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oil Data\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stick Man Data\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications\n",
    "------------\n",
    "\n",
    "-   Style based inverse kinematics (Grochow et al., 2004).\n",
    "\n",
    "-   Prior distributions for tracking (Urtasun et al., 2006, p.\n",
    "    Urtasun:priors05).\n",
    "\n",
    "-   Assisted drawing (Baxter and Anjyo, 2006).\n",
    "\n",
    "-   GPLVM based on a dual probabilistic interpretation of PCA.\n",
    "\n",
    "-   Straightforward to non-linearise it using Gaussian processes.\n",
    "\n",
    "-   Result is a non-linear probabilistic PCA.\n",
    "\n",
    "-   *Optimise latent variables* rather than integrate them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPy: A Gaussian Process Framework in Python\n",
    "-------------------------------------------\n",
    "\n",
    "Gaussian processes are a flexible tool for non-parametric analysis with\n",
    "uncertainty. The GPy software was started in Sheffield to provide a easy\n",
    "to use interface to GPs. One which allowed the user to focus on the\n",
    "modelling rather than the mathematics.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gpy.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>GPy is a BSD licensed software code base for implementing\n",
    "Gaussian process models in Python. It is designed for teaching and\n",
    "modelling. We welcome contributions which can be made through the Github\n",
    "repository\n",
    "<a href=\"https://github.com/SheffieldML/GPy\" class=\"uri\">https://github.com/SheffieldML/GPy</a></i>\n",
    "\n",
    "GPy is a BSD licensed software code base for implementing Gaussian\n",
    "process models in python. This allows GPs to be combined with a wide\n",
    "variety of software libraries.\n",
    "\n",
    "The software itself is available on\n",
    "[GitHub](https://github.com/SheffieldML/GPy) and the team welcomes\n",
    "contributions.\n",
    "\n",
    "The aim for GPy is to be a probabilistic-style programming language,\n",
    "i.e. you specify the model rather than the algorithm. As well as a large\n",
    "range of covariance functions the software allows for non-Gaussian\n",
    "likelihoods, multivariate outputs, dimensionality reduction and\n",
    "approximations for larger data sets.\n",
    "\n",
    "The documentation for GPy can be found\n",
    "[here](https://gpy.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started and Downloading Data\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for plotting and to prepare the bigger models for\n",
    "later useage. If you are interested, you can have a look, but this is\n",
    "not essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#3FCC94\", \"#DD4F23\", \"#C6D63B\", \"#D44271\", \n",
    "          \"#E4A42C\", \"#4F9139\", \"#6DDA4C\", \"#85831F\", \n",
    "          \"#B36A29\", \"#CF4E4A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(X, which_dims, labels):\n",
    "    fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "    X = X[:,which_dims]\n",
    "    ulabs = []\n",
    "    for lab in labels:\n",
    "        if not lab in ulabs:\n",
    "            ulabs.append(lab)\n",
    "            pass\n",
    "        pass\n",
    "    for i, lab in enumerate(ulabs):\n",
    "        ax.scatter(*X[labels==lab].T,marker='o',color=colors[i],label=lab)\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we’ll use a data set containing all handwritten digits\n",
    "from $0 \\cdots 9$ handwritten, provided by de Campos et al. (2009). We\n",
    "will only use some of the digits for the demonstrations in this lab\n",
    "class, but you can edit the code below to select different subsets of\n",
    "the digit data as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = [0,1,2,6,7,9] # which digits to work on\n",
    "data = pods.datasets.decampos_digits(which_digits=which)\n",
    "Y = data['Y']\n",
    "labels = data['str_lbls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to plot some of the digits using `plt.matshow` (the digit\n",
    "images have size `16x16`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis\n",
    "----------------------------\n",
    "\n",
    "Principal component analysis (PCA) finds a rotation of the observed\n",
    "outputs, such that the rotated principal component (PC) space maximizes\n",
    "the variance of the data observed, sorted from most to least important\n",
    "(most to least variable in the corresponding PC).\n",
    "\n",
    "In order to apply PCA in an easy way, we have included a PCA module in\n",
    "pca.py. You can import the module by import \\<path.to.pca\\> (without the\n",
    "ending .py!). To run PCA on the digits we have to reshape (Hint:\n",
    "np.reshape ) digits .\n",
    "\n",
    "-   What is the right shape $n\\times p$ to use?\n",
    "\n",
    "We will call the reshaped observed outputs $\\mathbf{Y}$ in the\n",
    "following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yn = Y#Y-Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s run PCA on the reshaped dataset $\\mathbf{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.util import pca\n",
    "p = pca.pca(Y) # create PCA class with digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot will show the lower dimensional representation of the\n",
    "digits in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot_fracs(20) # plot first 20 eigenvalue fractions\n",
    "p.plot_2d(Y,labels=labels.flatten(), colors=colors)\n",
    "pb.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Latent Variable Model\n",
    "--------------------------------------\n",
    "\n",
    "The Gaussian Process Latent Variable Model (GP-LVM) (Lawrence, 2005)\n",
    "embeds PCA into a Gaussian process framework, where the latent inputs\n",
    "$\\mathbf{Z}$ are learnt as hyperparameters and the mapping variables\n",
    "$\\mathbf{W}$ are integrated out. The advantage of this interpretation is\n",
    "it allows PCA to be generalized in a non linear way by replacing the\n",
    "resulting *linear* covariance witha non linear covariance. But first,\n",
    "let’s see how GPLVM is equivalent to PCA using an automatic relevance\n",
    "determination (ARD, see e.g. Bishop (2006)) linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4 # How many latent dimensions to use\n",
    "kernel = GPy.kern.Linear(input_dim, ARD=True) # ARD kernel\n",
    "m = GPy.models.GPLVM(Yn, input_dim=input_dim, kernel=kernel)\n",
    "\n",
    "m.optimize(messages=1, max_iters=1000) # optimize for 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.kern.plot_ARD()\n",
    "plot_model(m.X, m.linear.variances.argsort()[-2:], labels.flatten())\n",
    "pb.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the solution with a linear kernel is the same as the PCA\n",
    "solution with the exception of rotational changes and axis flips.\n",
    "\n",
    "For the sake of time, the solution you see was only running for 1000\n",
    "iterations, thus it might not be converged fully yet. The GP-LVM\n",
    "proceeds by iterative optimization of the *inputs* to the covariance. As\n",
    "we saw in the lecture earlier, for the linear covariance, these latent\n",
    "points can be optimized with an eigenvalue problem, but generally, for\n",
    "non-linear covariance functions, we are obliged to use gradient based\n",
    "optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMU Mocap Database\n",
    "------------------\n",
    "\n",
    "Motion capture data from the CMU motion capture data base (CMU Motion\n",
    "Capture Labb, 2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download any subject and motion from the data set. Here we will\n",
    "download motion `01` from subject `35`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='35' \n",
    "motion=['01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.cmu_mocap(subject, motion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary contains the keys ‘Y’ and ‘skel’, which represent\n",
    "the data and the skeleton.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Y'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was used in the hierarchical GP-LVM paper (Lawrence and Moore,\n",
    "2007) in an experiment that was also recreated in the Deep Gaussian\n",
    "process paper (Damianou and Lawrence, 2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['citation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extra information about the data is included, as standard, under the\n",
    "keys `info` and `details`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['info'])\n",
    "print()\n",
    "print(data['details'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data has the figure moving across the floor during the\n",
    "motion capture sequence. We can make the figure walk ‘in place’, by\n",
    "setting the x, y, z positions of the root node to zero. This makes it\n",
    "easier to visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make figure move in place.\n",
    "data['Y'][:, 0:3] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove the mean of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['Y']\n",
    "Y_mean = Y.mean(0)\n",
    "Y_std = Y.std(0)\n",
    "Yhat = (Y-Y_mean)/Y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the GP-LVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPy.models.GPLVM(Yhat, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimize(messages=True, max_f_eval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = model.plot_latent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.Y[0, :]\n",
    "data_show = GPy.plotting.matplot_dep.visualize.skeleton_show(y[np.newaxis, :], \n",
    "                                                             data['skel'])\n",
    "lvm_visualizer = GPy.plotting.matplot_dep.visualize.lvm(model.X[0].copy(), \n",
    "                                                        model, \n",
    "                                                        data_show, \n",
    "                                                        latent_axes=ax)\n",
    "input('Press enter to finish')\n",
    "lvm_visualizer.close()\n",
    "data_show.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Latent Doodle Space\n",
    "----------------------------\n",
    "\n",
    "[![](attachment:https://i.vimeocdn.com/video/\\id_640x480.jpg)](https://vimeo.com/3235882#t=)\n",
    "\n",
    "Figure: <i>The latent doodle space idea of Baxter and Anjyo (2006)\n",
    "manages to build a smooth mapping across very sparse data.</i>\n",
    "\n",
    "**Generalization with much less Data than Dimensions**\n",
    "\n",
    "-   Powerful uncertainly handling of GPs leads to surprising properties.\n",
    "\n",
    "-   Non-linear models can be used where there are fewer data points than\n",
    "    dimensions *without overfitting*.\n",
    "\n",
    "<span style=\"text-align:right\">(Baxter and Anjyo, 2006)</span>"
   ],
   "attachments": {
    "https://i.vimeocdn.com/video/\\id_640x480.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAUFBQUFBQUGBgUICAcICAsKCQkKCxEMDQwNDBEaEBMQ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\nBsY0806EUzXY91yXcaf0ToSNzA2MaeadCKZrsef/2Q==\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Continuous Character Control\n",
    "-------------------------------------\n",
    "\n",
    "-   Graph diffusion prior for enforcing connectivity between motions.\n",
    "    $$\\log p(\\mathbf{X}) = w_c \\sum_{i,j} \\log K_{ij}^d$$ with the graph\n",
    "    diffusion kernel $\\mathbf{K}^d$ obtain from\n",
    "    $$K_{ij}^d = \\exp(\\beta \\mathbf{H})\n",
    "    \\qquad \\text{with} \\qquad \\mathbf{H} = -\\mathbf{T}^{-1/2} \\mathbf{L} \\mathbf{T}^{-1/2}$$\n",
    "    the graph Laplacian, and $\\mathbf{T}$ is a diagonal matrix with\n",
    "    $T_{ii} = \\sum_j w(\\mathbf{ x}_i, \\mathbf{ x}_j)$,\n",
    "    $$L_{ij} = \\begin{cases} \\sum_k w(\\mathbf{ x}_i,\\mathbf{ x}_k) & \\text{if $i=j$}\n",
    "    \\\\\n",
    "    -w(\\mathbf{ x}_i,\\mathbf{ x}_j) &\\text{otherwise.}\n",
    "    \\end{cases}$$ and\n",
    "    $w(\\mathbf{ x}_i,\\mathbf{ x}_j) = || \\mathbf{ x}_i - \\mathbf{ x}_j||^{-p}$\n",
    "    measures similarity.\n",
    "\n",
    "(Levine et al., 2012){style=“text-align:right”}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Control: Results\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('hr3pdDl5IAg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Character control in the latent space described the the\n",
    "GP-LVM Levine et al. (2012).</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for Blastocyst Development in Mice: Single Cell TaqMan Arrays\n",
    "------------------------------------------------------------------\n",
    "\n",
    "Now we analyze some single cell data from Guo et al. (2010). Tey\n",
    "performed qPCR TaqMan array on single cells from the developing\n",
    "blastocyst in mouse. The data is taken from the early stages of\n",
    "development when the Blastocyst is forming. At the 32 cell stage the\n",
    "data is already separated into the trophectoderm (TE) which goes onto\n",
    "form the placenta and the inner cellular mass (ICM). The ICM further\n",
    "differentiates into the epiblast (EPI)—which gives rise to the endoderm,\n",
    "mesoderm and ectoderm—and the primitive endoderm (PE) which develops\n",
    "into the amniotic sack. Guo et al. (2010) selected 48 genes for\n",
    "expression measurement. They labelled the resulting cells and their\n",
    "labels are included as an aide to visualization.\n",
    "\n",
    "They first visualized their data using principal component analysis. In\n",
    "the first two principal components this fails to separate the domains.\n",
    "This is perhaps because the principal components are dominated by the\n",
    "variation in the 64 cell systems. This in turn may be because there are\n",
    "more cells from the data set in that regime, and may be because the\n",
    "natural variation is greater. We first recreate their visualization\n",
    "using principal component analysis.\n",
    "\n",
    "In this notebook we will perform PCA on the original data, showing that\n",
    "the different regimes do not separate.\n",
    "\n",
    "Next we load in the data. We’ve provided a convenience function for\n",
    "loading in the data with `pods`. It is loaded in as a `pandas`\n",
    "DataFrame. This allows us to summarize it with the `describe` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.singlecell()\n",
    "Y = data['Y']\n",
    "Y.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis\n",
    "----------------------------\n",
    "\n",
    "Now we follow Guo et al. (2010) in performing PCA on the data. Rather\n",
    "than calling a ‘PCA routine’, here we break the algorithm down into its\n",
    "steps: compute the data covariance, compute the eigenvalues and\n",
    "eigenvectors and sort according to magnitude of eigenvalue. Because we\n",
    "want to visualize the data, we’ve chose to compute the eigenvectors of\n",
    "the *inner product matrix* rather than the covariance matrix. This\n",
    "allows us to plot the eigenvalues directly. However, this is less\n",
    "efficient (in this case because the number of genes is smaller than the\n",
    "number of data) than computing the eigendecomposition of the covariance\n",
    "matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a centred version of data.\n",
    "centredY = Y - Y.mean()\n",
    "# compute inner product matrix\n",
    "C = np.dot(centredY,centredY.T)\n",
    "# perform eigendecomposition\n",
    "V, U = np.linalg.eig(C)\n",
    "# sort eigenvalues and vectors according to size\n",
    "ind = V.argsort()\n",
    "ev = V[ind[::-1]]\n",
    "U = U[:, ind[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the result, we now construct a simple helper function. This\n",
    "will ensure that the plots have the same legends as the GP-LVM plots we\n",
    "use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(ax, x, y, labels, symbols):\n",
    "    \"\"\"A small helper function for plotting with labels\"\"\"\n",
    "    # make sure labels are in order of input:\n",
    "    ulabels = []\n",
    "    for lab in labels:\n",
    "        if not lab in ulabels:\n",
    "            ulabels.append(lab)\n",
    "    for i, label in enumerate(ulabels):\n",
    "        symbol = symbols[i % len(symbols)]\n",
    "        ind = labels == label\n",
    "        ax.plot(x[ind], y[ind], symbol)\n",
    "    ax.legend(ulabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Result\n",
    "----------\n",
    "\n",
    "Now, using the helper function we can plot the results with appropriate\n",
    "labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "plot_labels(ax, U[:, 0], U[:, 1], data['labels'], '<>^vsd')\n",
    "\n",
    "mlai.write_figure('singlecell-data-pca.svg', directory='./datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/datasets/singlecell-data-pca.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>First two principal compoents of the Guo et al. (2010)\n",
    "blastocyst development data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GP-LVM on the Data\n",
    "------------------\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip0\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Max Zwiessele\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/max-zwiessele.jpg\" clip-path=\"url(#clip0)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip1\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Oliver Stegle\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/oliver-stegle.jpg\" clip-path=\"url(#clip1)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "Work done as a collaboration between Max Zwiessele, Oliver Stegle and\n",
    "Neil D. Lawrence.\n",
    "\n",
    "Then, we follow Buettner and Theis (2012) in applying the GP-LVM to the\n",
    "data. There is a slight pathology in the result, one which they fixed by\n",
    "using priors that were dependent on the developmental stage. We then\n",
    "show how the Bayesian GP-LVM doesn’t exhibit those pathologies and gives\n",
    "a nice results that seems to show the lineage of the cells.\n",
    "\n",
    "They used modified prior to ensure that small differences between cells\n",
    "at the same differential stage were preserved. Here we apply a standard\n",
    "GP-LVM (no modified prior) to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GPy.kern.RBF(2)+GPy.kern.Bias(2)\n",
    "model = GPy.models.GPLVM(Y.values, 2, kernel=kernel)\n",
    "model.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "model.plot_latent(ax=ax, labels=data['labels'], marker='<>^vsd')\n",
    "\n",
    "mlai.write_figure('singlecell-gplvm.svg', directory='./gplvm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gplvm/singlecell-gplvm.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Visualisation of the Guo et al. (2010) blastocyst development\n",
    "data with the GP-LVM.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "model.kern.plot_ARD(ax=ax)\n",
    "\n",
    "mlai.write_figure('singlecell-gplvm-ard.svg', directory='./gplvm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gplvm/singlecell-gplvm-ard.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The ARD parameters of the GP-LVM for the Guo et al. (2010)\n",
    "blastocyst development data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blastocyst Data: Isomap\n",
    "-----------------------\n",
    "\n",
    "Isomap first builds a neighbourhood graph, and then uses distances along\n",
    "this graph to approximate the geodesic distance between points. These\n",
    "distances are then visualized by performing classical multidimensional\n",
    "scaling (which involves computing the eigendecomposition of the centred\n",
    "distance matrix). As the neighborhood size is increased to match the\n",
    "data, principal component analysis is recovered (or strictly speaking,\n",
    "principal coordinate analysis). The fewer the neighbors, the more\n",
    "‘non-linear’ the isomap embeddings are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10\n",
    "model = sklearn.manifold.Isomap(n_neighbors=n_neighbors, n_components=2)\n",
    "X = model.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "plot_labels(ax, X[:, 0], X[:, 1], data['labels'], '<>^vsd')\n",
    "\n",
    "\n",
    "mlai.write_figure('singlecell-isomap.svg', directory='./dimred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/singlecell-isomap.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Visualisation of the Guo et al. (2010) blastocyst development\n",
    "data with Isomap.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blastocyst Data: Locally Linear Embedding\n",
    "-----------------------------------------\n",
    "\n",
    "Next we try locally linear embedding. In locally linear embedding a\n",
    "neighborhood is also computed. Each point is then reconstructed by it’s\n",
    "neighbors using a linear weighting. This implies a locally linear patch\n",
    "is being fitted to the data in that region. These patches are\n",
    "assimilated into a large $n\\times n$ matrix and a lower dimensional data\n",
    "set which reflects the same relationships is then sought. Quite a large\n",
    "number of neighbours needs to be selected for the data to not collapse\n",
    "in on itself. When a large number of neighbours is selected the\n",
    "embedding is more linear and begins to look like PCA. However, the\n",
    "algorithm does *not* converge to PCA in the limit as the number of\n",
    "neighbors approaches $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 50\n",
    "model = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=2)\n",
    "X = model.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "model.plot_latent(ax=ax, X[:, 0], X[:, 1], data['labels'], '<>^vsd')\n",
    "\n",
    "mlai.write_figure('singlecell-lle.svg', directory='./dimred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/singlecell-lle.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Visualisation of the Guo et al. (2010) blastocyst development\n",
    "data with a locally linear embedding.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baxter, W.V., Anjyo, K.-I., 2006. Latent doodle space, in: EUROGRAPHICS.\n",
    "Vienna, Austria, pp. 477–485.\n",
    "<https://doi.org/10.1111/j.1467-8659.2006.00967.x>\n",
    "\n",
    "Bishop, C.M., 2006. Pattern recognition and machine learning. springer.\n",
    "\n",
    "Buettner, F., Theis, F.J., 2012. A novel approach for resolving\n",
    "differences in single-cell gene expression patterns from zygote to\n",
    "blastocyst. Bioinformatics 28, i626–i632.\n",
    "<https://doi.org/10.1093/bioinformatics/bts385>\n",
    "\n",
    "CMU Motion Capture Labb, 2003. The CMU mocap database.\n",
    "\n",
    "Damianou, A., Lawrence, N.D., 2013. Deep Gaussian processes, in:. pp.\n",
    "207–215.\n",
    "\n",
    "de Campos, T.E., Babu, B.R., Varma, M., 2009. Character recognition in\n",
    "natural images, in: Proceedings of the Fourth International Conference\n",
    "on Computer Vision Theory and Applications - Volume 2: VISAPP,\n",
    "(Visigrapp 2009). INSTICC; SciTePress, pp. 273–280.\n",
    "<https://doi.org/10.5220/0001770102730280>\n",
    "\n",
    "Grochow, K., Martin, S.L., Hertzmann, A., Popovic, Z., 2004. Style-based\n",
    "inverse kinematics, in: ACM Transactions on Graphics (Siggraph 2004).\n",
    "pp. 522–531. <https://doi.org/10.1145/1186562.1015755>\n",
    "\n",
    "Guo, G., Huss, M., Tong, G.Q., Wang, C., Sun, L.L., Clarke, N.D.,\n",
    "Robsonemail, P., 2010. Resolution of cell fate decisions revealed by\n",
    "single-cell gene expression analysis from zygote to blastocyst.\n",
    "Developmental Cell 18, 675–685.\n",
    "<https://doi.org/10.1016/j.devcel.2010.02.012>\n",
    "\n",
    "Hastie, T., Stuetzle, W., 1989. Principal curves. Journal of the\n",
    "American Statistical Association 84, 502–516.\n",
    "\n",
    "Hotelling, H., 1933. Analysis of a complex of statistical variables into\n",
    "principal components. Journal of Educational Psychology 24, 417–441.\n",
    "\n",
    "Lawrence, N.D., n.d. Gaussian process models for visualisation of high\n",
    "dimensional data, in:. pp. 329–336.\n",
    "\n",
    "Lawrence, N.D., 2005. Probabilistic non-linear principal component\n",
    "analysis with Gaussian process latent variable models. Journal of\n",
    "Machine Learning Research 6, 1783–1816.\n",
    "\n",
    "Lawrence, N.D., Moore, A.J., 2007. Hierarchical Gaussian process latent\n",
    "variable models, in:. pp. 481–488.\n",
    "\n",
    "Levine, S., Wang, J.M., Haraux, A., Popović, Z., Koltun, V., 2012.\n",
    "Continuous character control with low-dimensional embeddings. ACM\n",
    "Transactions on Graphics (SIGGRAPH 2012) 31.\n",
    "\n",
    "Mardia, K.V., Kent, J.T., Bibby, J.M., 1979. Multivariate analysis.\n",
    "Academic Press, London.\n",
    "\n",
    "Pearson, K., 1901. On lines and planes of closest fit to systems of\n",
    "points in space. The London, Edinburgh and Dublin Philosophical Magazine\n",
    "and Journal of Science, Sixth Series 2, 559–572.\n",
    "\n",
    "Roweis, S.T., n.d. EM algorithms for PCA and SPCA, in:. pp. 626–632.\n",
    "\n",
    "Schölkopf, B., Smola, A., Müller, K.-R., 1998. Nonlinear component\n",
    "analysis as a kernel eigenvalue problem. Neural Computation 10,\n",
    "1299–1319. <https://doi.org/10.1162/089976698300017467>\n",
    "\n",
    "Tipping, M.E., Bishop, C.M., 1999a. Mixtures of probabilistic principal\n",
    "component analysers. Neural Computation 11, 443–482.\n",
    "\n",
    "Tipping, M.E., Bishop, C.M., 1999b. Probabilistic principal component\n",
    "analysis. Journal of the Royal Statistical Society, B 6, 611–622.\n",
    "<https://doi.org/doi:10.1111/1467-9868.00196>\n",
    "\n",
    "Urtasun, R., Fleet, D.J., Fua, P., 2006. 3D people tracking with\n",
    "Gaussian process dynamical models, in: Proceedings of the IEEE Computer\n",
    "Society Conference on Computer Vision and Pattern Recognition. IEEE\n",
    "Computer Society Press, New York, U.S.A., pp. 238–245.\n",
    "<https://doi.org/10.1109/CVPR.2006.15>"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
