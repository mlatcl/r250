{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R250: Unsupervised Learning with Gaussian Processes I\n",
    "=====================================================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge\n",
    "\n",
    "### 2021-01-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In this talk we give an introduction to Unsupervised\n",
    "Learning and Gaussian processes for students who are interested in\n",
    "working with Unsupervised GPs for the the R250 module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "-----\n",
    "\n",
    "First we download some libraries and files to support the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--setupplotcode{import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('colorblind')}-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pods\n",
    "----\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science’. Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/sods/ods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on github:\n",
    "<a href=\"https://github.com/sods/ods\" class=\"uri\">https://github.com/sods/ods</a>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Dimensional Data\n",
    "---------------------\n",
    "\n",
    "To introduce high dimensional data, we will first of all introduce a\n",
    "hand written digit from the U.S. Postal Service handwritten digit data\n",
    "set (originally collected from scanning enveolopes) and used in the\n",
    "first convolutional neural network paper (Le Cun et al., 1989).\n",
    "\n",
    "Le Cun et al. (1989) downscaled the images to $16 \\times 16$, here we\n",
    "use an image at the original scale, containing 64 rows and 57 columns.\n",
    "Since the pixels are binary, and the number of dimensions is 3,648, this\n",
    "space contains $2^{3,648}$ possible images. So this space contains a lot\n",
    "more than just one digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USPS Samples\n",
    "------------\n",
    "\n",
    "If we sample from this space, taking each pixel independently from a\n",
    "probability which is given by the number of pixels which are ‘on’ in the\n",
    "original image, over the total number of pixels, we see images that look\n",
    "nothing like the original digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "six_image = mlai.load_pgm('br1561_6.3.pgm', directory ='../slides/diagrams/ml')\n",
    "rows = six_image.shape[0]\n",
    "col = six_image.shape[1]\n",
    "      \n",
    "ax.imshow(six_image,interpolation='none').set_cmap('gray')\n",
    "mlai.write_figure('./dimred/dem_six000.png')\n",
    "for i in range(3):\n",
    "    rand_image = np.random.rand(rows, col)<((six_image>0).sum()/float(rows*col))\n",
    "    ax.imshow(rand_image,interpolation='none').set_cmap('gray')\n",
    "    mlai.write_figure('./dimred/dem_six{i:0>3}.png'.format(i=i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('dem_six{counter:0>3}.png', directory='./ml', counter=IntSlider(0, 0, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/dem_six000.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/dem_six001.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/dem_six002.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Even if we sample every nano second from now until the end of\n",
    "the universe we won’t see the original six again.</i>\n",
    "\n",
    "Even if we sample every nanosecond from now until the end of the\n",
    "universe you won’t see the original six again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Model of Digit\n",
    "---------------------\n",
    "\n",
    "So, an independent pixel model for this digit doesn’t seem sensible. The\n",
    "total space is enormous, and yet the space occupied by the type of data\n",
    "we’re interested in is relatively small.\n",
    "\n",
    "Consider a different type of model. One where we take a prototype six\n",
    "and we rotate it left and right to create new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imrotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_image = mlai.load_pgm('br1561_6.3.pgm', directory ='../slides/diagrams/dimred')\n",
    "six_image = np.hstack([np.zeros((rows, 3)), six_image, np.zeros((rows, 4))])\n",
    "dim_one = np.asarray(six_image.shape)\n",
    "angles = range(360)\n",
    "i = 0\n",
    "Y = np.zeros((len(angles), np.prod(dim_one)))\n",
    "for angle in angles:\n",
    "    rot_image = rotate(six_image, angle, mode='nearest')\n",
    "    dim_two = np.asarray(rot_image.shape)\n",
    "    start = [int(round((dim_two[0] - dim_one[0])/2)), int(round((dim_two[1] - dim_one[1])/2))]\n",
    "    crop_image = rot_image[start[0]+np.array(range(dim_one[0])), :][:, start[1]+np.array(range(dim_one[1]))]\n",
    "    Y[i, :] = crop_image.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('../slides/diagrams/dimred/dem_six_rotate{counter:0>3}.png', directory='./ml', counter=(0, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/dem_six_rotate001.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/dem_six_rotate003.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/dem_six_rotate005.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Rotate a prototype six to form a set of plausible sixes.</i>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/dem_manifold_print001.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/dem_manifold_print002.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>The rotated sixes projected onto the first two principal\n",
    "components of the ‘rotated data set’. The data lives on a one\n",
    "dimensional manifold in the 3,648 dimensional space.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low Dimensional Manifolds\n",
    "-------------------------\n",
    "\n",
    "Of course, in practice pure rotation of the image is too simple a model.\n",
    "Digits can undergo several distortions and retain their nature. For\n",
    "example, they can be scaled, they can go through translation, they can\n",
    "udnergo ‘thinning’. But, for data with ‘structure’ we expect fewer of\n",
    "these distortions than the dimension of the data. This means we expect\n",
    "data to live on a lower dimensonal manifold. This implies that we should\n",
    "deal with high dimensional data by looking for a lower dimensional\n",
    "(non-linear) embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction\n",
    "------------------------\n",
    "\n",
    "Dimensionality reduction methods compress the data by replacing the\n",
    "original data with a reduced number of continuous variables. One way of\n",
    "thinking of these methods is to imagine a marionette.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/marionette.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Thinking of dimensionality reduction as a marionette. We\n",
    "observe the high dimensional pose of the puppet, $\\mathbf{ x}$, but the\n",
    "movement of the puppeteer’s hand, $\\mathbf{ z}$ remains hidden to us.\n",
    "Dimensionality reduction aims to recover those hidden movements which\n",
    "generated the observations.</i>\n",
    "\n",
    "The position of each body part of a marionette could be thought of as\n",
    "our data, $\\mathbf{ x}_i$. So, each data point consists of the 3-D\n",
    "co-ordinates of all the different body parts of the marionette. Let’s\n",
    "say there are 13 different body parts (2 each of feet, knees, hips,\n",
    "hands, elbows, shoulders, one head). Each body part has an x, y, z\n",
    "position in Cartesian coordinates. So that’s 39 numbers associated with\n",
    "each observation.\n",
    "\n",
    "The movement of these 39 parts is determined by the puppeteer via\n",
    "strings. Let’s assume it’s a very simple puppet, with just one stick to\n",
    "control it. The puppeteer can move the stick up and down, left and\n",
    "right. And they can twist it. This gives three parameters in the\n",
    "puppeteers control. This implies that the 39 variables we see moving are\n",
    "controlled by only 3 variables. These 3 variables are often called the\n",
    "hidden or *latent variables*.\n",
    "\n",
    "Dimensionality reduction assumes something similar for real world data.\n",
    "It assumes that the data we observe is generated from some lower\n",
    "dimensional underlying process. It then seeks to recover the values\n",
    "associated with this low dimensional process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples in Social Sciences\n",
    "\n",
    "Dimensionality reduction techniques underpin a lot of psychological\n",
    "scoring tests such as IQ tests or personality tests. An IQ test can\n",
    "involve several hundred questions, potentially giving a rich, high\n",
    "dimensional, characterization of some aspects of your intelligence. It\n",
    "is then summarized by a single number. Similarly, the Myers-Briggs\n",
    "personality test involves answering questions about preferences which\n",
    "are reduced to a set of numbers reflecting personality.\n",
    "\n",
    "These tests are assuming that our intelligence is implicitly\n",
    "one-dimensional and that our personality is implicitly four dimensional.\n",
    "Other examples include political belief which is typically represented\n",
    "on a left to right scale. A one-dimensional distillation of an entire\n",
    "philosophy about how a country should be run. Our own leadership\n",
    "principles imply that our decisions have a fourteen-dimensional space\n",
    "underlying them. Each decision could be characterized by judging to what\n",
    "extent it embodies each of the principles.\n",
    "\n",
    "Political belief, personality, intelligence, leadership. None of these\n",
    "exist as a directly measurable quantity in the real world, rather they\n",
    "are inferred based on measurables. Dimensionality reduction is the\n",
    "process of allowing the computer to automatically find such underlying\n",
    "dimensions. This automatically allowing us to characterize each data\n",
    "point according to those explanatory variables. Each of these\n",
    "characteristics can be scored, and individuals can then be turned into\n",
    "vectors.\n",
    "\n",
    "This doesn’t only apply to individuals, in recent years work on language\n",
    "modeling has taken a similar approach to words. The\n",
    "[word2vec](https://arxiv.org/abs/1301.3781) algorithm performed a\n",
    "dimensionality reduction on words, now you can take any word and map it\n",
    "to a latent space where similar words exhibit similar characteristics. A\n",
    "‘personality space’ for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Variables and Linear Dimensionality Reduction\n",
    "------------------------------------------------------\n",
    "\n",
    "We will return to non-linear dimensionality reduction approaches\n",
    "shortly, but first we’re going to consider *linear* approaches to\n",
    "dimensionality reduction. To do so, we’ll first review some\n",
    "characteristics of the Gaussian density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Important Gaussian Properties\n",
    "---------------------------------\n",
    "\n",
    "The Gaussian density has many important properties, but for the moment\n",
    "we’ll review two of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of Gaussians\n",
    "----------------\n",
    "\n",
    "If we assume that a variable, $y_i$, is sampled from a Gaussian density,\n",
    "\n",
    "$$y_i \\sim \\mathcal{N}\\left(\\mu_i,\\sigma_i^2\\right)$$\n",
    "\n",
    "Then we can show that the sum of a set of variables, each drawn\n",
    "independently from such a density is also distributed as Gaussian. The\n",
    "mean of the resulting density is the sum of the means, and the variance\n",
    "is the sum of the variances,\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n\\mu_i,\\sum_{i=1}^n\\sigma_i^2\\right)\n",
    "$$\n",
    "\n",
    "Since we are very familiar with the Gaussian density and its properties,\n",
    "it is not immediately apparent how unusual this is. Most random\n",
    "variables, when you add them together, change the family of density they\n",
    "are drawn from. For example, the Gaussian is exceptional in this regard.\n",
    "Indeed, other random variables, if they are independently drawn and\n",
    "summed together tend to a Gaussian density. That is the [*central limit\n",
    "theorem*](https://en.wikipedia.org/wiki/Central_limit_theorem) which is\n",
    "a major justification for the use of a Gaussian density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling a Gaussian\n",
    "------------------\n",
    "\n",
    "Less unusual is the *scaling* property of a Gaussian density. If a\n",
    "variable, $y$, is sampled from a Gaussian density,\n",
    "\n",
    "$$y\\sim \\mathcal{N}\\left(\\mu,\\sigma^2\\right)$$ and we choose to scale\n",
    "that variable by a *deterministic* value, $w$, then the *scaled\n",
    "variable* is distributed as\n",
    "\n",
    "$$wy\\sim \\mathcal{N}\\left(w\\mu,w^2 \\sigma^2\\right).$$ Unlike the summing\n",
    "properties, where adding two or more random variables independently\n",
    "sampled from a family of densitites typically brings the summed variable\n",
    "*outside* that family, scaling many densities leaves the distribution of\n",
    "that variable in the same *family* of densities. Indeed, many densities\n",
    "include a *scale* parameter (e.g. the [Gamma\n",
    "density](https://en.wikipedia.org/wiki/Gamma_distribution)) which is\n",
    "purely for this purpose. In the Gaussian the standard deviation,\n",
    "$\\sigma$, is the scale parameter. To see why this makes sense, let’s\n",
    "consider, $$z \\sim \\mathcal{N}\\left(0,1\\right),$$ then if we scale by\n",
    "$\\sigma$ so we have, $y=\\sigma z$, we can write,\n",
    "$$y=\\sigma z \\sim \\mathcal{N}\\left(0,\\sigma^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Latent Variable Models\n",
    "=============================\n",
    "\n",
    "Let’s first of all review the properties of the multivariate Gaussian\n",
    "distribution that make linear Gaussian models easier to deal with. We’ll\n",
    "return to the, perhaps surprising, result on the parameters within the\n",
    "nonlinearity, $\\boldsymbol{ \\theta}$, shortly.\n",
    "\n",
    "To work with linear Gaussian models, to find the marginal likelihood all\n",
    "you need to know is the following rules. If $$\n",
    "\\mathbf{ y}= \\mathbf{W}\\mathbf{ x}+ \\boldsymbol{ \\epsilon},\n",
    "$$ where $\\mathbf{ y}$, $\\mathbf{ x}$ and $\\boldsymbol{ \\epsilon}$ are\n",
    "vectors and we assume that $\\mathbf{ x}$ and $\\boldsymbol{ \\epsilon}$\n",
    "are drawn from multivariate Gaussians, $$\n",
    "\\begin{align}\n",
    "\\mathbf{ x}& \\sim \\mathcal{N}\\left(\\boldsymbol{ \\mu},\\mathbf{C}\\right)\\\\\n",
    "\\boldsymbol{ \\epsilon}& \\sim \\mathcal{N}\\left(\\mathbf{0},\\boldsymbol{ \\Sigma}\\right)\n",
    "\\end{align}\n",
    "$$ then we know that $\\mathbf{ y}$ is also drawn from a multivariate\n",
    "Gaussian with, $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{W}\\boldsymbol{ \\mu},\\mathbf{W}\\mathbf{C}\\mathbf{W}^\\top + \\boldsymbol{ \\Sigma}\\right).\n",
    "$$\n",
    "\n",
    "With appropriately defined covariance, $\\boldsymbol{ \\Sigma}$, this is\n",
    "actually the marginal likelihood for Factor Analysis, or Probabilistic\n",
    "Principal Component Analysis (Tipping and Bishop, 1999), because we\n",
    "integrated out the inputs (or *latent* variables they would be called in\n",
    "that case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Variables\n",
    "================\n",
    "\n",
    "Latent means hidden, and hidden variables are simply *unobservable*\n",
    "variables. The idea of a latent variable is crucial to the concept of\n",
    "artificial intelligence, machine learning and experimental design. A\n",
    "latent variable could take many forms. We might observe a man walking\n",
    "along a road with a large bag of clothes and we might *infer* that the\n",
    "man is walking to the laundrette. Our observations are a highly complex\n",
    "data space, the response in our eyes is processed through our visual\n",
    "cortex, the combination of the individual’s limb movememnts and the\n",
    "direction they are walking in all conflate in our heads to cause us to\n",
    "infer that (perhaps) the individual is going to the laundrette. We don’t\n",
    "*know* that the man is walking to the laundrette, but we have a model of\n",
    "the world that suggests that it’s a likely outcome for the very complex\n",
    "data. In some ways the latent variable can be seen as a *compression* of\n",
    "this very complex scene. If I were writing a book, I might write that “A\n",
    "man tripped over whilst walking to the laundrette”. In the reader’s mind\n",
    "an image of a man, perhaps laden with dirty clothes, may occur. All\n",
    "these ideas come from our expectations of the world around us. We can\n",
    "make further inference about the man, some of it perhaps plausible\n",
    "others less so. The man may be going to the laundrette because his\n",
    "washing machine is broken, or because he doesn’t have a large enough\n",
    "flat to have a washing machine, or because he’s carrying a duvet, or\n",
    "because he doesn’t like ironing. All of these may *increase* in\n",
    "probability given our observation, but they are still *latent*\n",
    "variables. Unless we follow the man back to his appartment, or start\n",
    "making other enquirires about the man, we don’t know the true answer.\n",
    "\n",
    "It’s clear that to do inference about any complex system we *must*\n",
    "include latent variables. Latent variables are extremely powerful. In\n",
    "robotics, they are used to represent the *state* of the robot. The state\n",
    "of the robot may include its position (in x, y coordinates) its speed,\n",
    "its direction of facing. How are *these* variables unknown to the robot?\n",
    "Well the robot only posesses *sensors*, it can make observations of the\n",
    "nearest object in a certain direction, and it may have a map of its\n",
    "environment. If we represent the state of the robot as its position on a\n",
    "map, it may be uncertain of that position. If you go walking or running\n",
    "in the hills around Sheffield, you can take a very high quality ordnance\n",
    "survey map with you. However, unless you are a really excellent\n",
    "orienteer, when you are far from any given landmark, you will probably\n",
    "be *uncertain* about your true position on the map. These states are\n",
    "also latent variables.\n",
    "\n",
    "In statistical analysis of experiments you try to control for each\n",
    "aspect of the experiment, in particular by *randomization*. So if I’m\n",
    "interested in the ability of a particular fertilizer to improve the\n",
    "yield of a particular plant I may design an experiment where I apply the\n",
    "fertilizer to some plants (the treatment group) and withold the\n",
    "fertilizer from others (the control group). I then test to see whether\n",
    "the yield from the treatment group is better (or worse) than the control\n",
    "group. I may find that I have an excellent yield for the treatment\n",
    "group. However, what if I’d (unknowlingly) planted all my treatment\n",
    "plants in a sunny part of the field, and all the control plants in a\n",
    "shady part of the field. That would also be a latent variable, in this\n",
    "case known as a *confounder*. In statistical experimental design\n",
    "*randomization* is used to attempt to eliminate the correlated effects\n",
    "of these confounders: you aim to ensure that if these confounders *do*\n",
    "exist their effects are not correlated with treatment and contorl. This\n",
    "is known as a [randomized control\n",
    "trial](http://en.wikipedia.org/wiki/Randomized_controlled_trial).\n",
    "\n",
    "Greek philosophers worried a great deal about what was knowable and what\n",
    "was unknowable. Adherents of [philosophical\n",
    "Skeptisism](http://en.wikipedia.org/wiki/Skepticism) were inspired by\n",
    "the idea that since your senses sometimes give you contradictory\n",
    "information, they cannot be trusted, and in extreme cases they chose to\n",
    "*ignore* their senses. This is an acknowledgement that very often the\n",
    "true state of the world cannot be known with precision. Unfortunately,\n",
    "these philosophers didn’t have a good understanding of probability, so\n",
    "they were unable to encapsulate their ideas through a *degree* of\n",
    "belief.\n",
    "\n",
    "We often use language to express the compression of a complex behavior\n",
    "or patterns in a simpler way, for example we talk about motives as a\n",
    "useful distallation for a perhaps very complex patter of behavior. In\n",
    "physics we use principles of causation and simple laws to describe the\n",
    "world around us. Such motives or underlying principles are difficult to\n",
    "observe directly, our conclusions about them emerge over a period of\n",
    "time by observing indirect consequences of the latent variables.\n",
    "\n",
    "Epistemic uncertainty allows us to deal with these worries by\n",
    "associating our degree of belief about the state of the world with a\n",
    "probaiblity distribution. This core idea underpins state space\n",
    "modelling, probabilistic graphical models and the wider field of latent\n",
    "variable modelling. In this session we are going to explore the idea in\n",
    "a simple linear system and see how it relates to *factor analysis* and\n",
    "*principal component analysis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Personality\n",
    "================\n",
    "\n",
    "At the beginning of the 20th century there was a great deal of interest\n",
    "amoungst psychologists in formalizing patterns of thought. The approach\n",
    "they used became known as factor analysis. The principle is that we\n",
    "observe a potentially high dimensional vector of characteristics about\n",
    "an individual. To formalize this, social scientists designed\n",
    "questionaires. We can envisage many questions that we may ask, but the\n",
    "assumption is that underlying these questions there are only a few\n",
    "traits that dictate the behavior. These models are known as latent trait\n",
    "models and the analysis is sometimes known as factor analysis. The idea\n",
    "is that there are a few characteristic traits that we are looking to\n",
    "discern. These traits or factors can be extracted by assimilating the\n",
    "high dimensional characteristics of the individual into a few latent\n",
    "factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "Factor Analysis Model\n",
    "---------------------\n",
    "\n",
    "This causes us to consider a model as follows, if we are given a high\n",
    "dimensional vector of features (perhaps questionaire answers) associated\n",
    "with an individual, $\\mathbf{ y}$, we assume that these factors are\n",
    "actually generated from a low dimensional vector latent traits, or\n",
    "latent variables, which determine the personality. $$\n",
    "\\mathbf{ y}= \\mathbf{f}(\\mathbf{ z}) + \\boldsymbol{ \\epsilon},\n",
    "$$ where $\\mathbf{f}(\\mathbf{ z})$ is a *vector valued* function that is\n",
    "dependent on the latent traits and $\\boldsymbol{ \\epsilon}$ is some\n",
    "corrupting noise. For simplicity, we assume that the function is given\n",
    "by a *linear* relationship, $$\n",
    "\\mathbf{f}(\\mathbf{ z}) = \\mathbf{W}\\mathbf{ z}\n",
    "$$ where we have introduced a matrix $\\mathbf{W}$ that is sometimes\n",
    "referred to as the *factor loadings* but we also immediately see is\n",
    "related to our *multivariate linear regression* models from the . That\n",
    "is because our vector valued function is of the form\n",
    "\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{ z}) =\n",
    "\\begin{bmatrix} f_1(\\mathbf{ z}) \\\\ f_2(\\mathbf{ z}) \\\\ \\vdots \\\\\n",
    "f_p(\\mathbf{ z})\\end{bmatrix}\n",
    "$$ where there are $p$ features associated with the individual. If we\n",
    "consider any of these functions individually we have a prediction\n",
    "function that looks like a regression model, $$\n",
    "f_j(\\mathbf{ z}) =\n",
    "\\mathbf{ w}_{j, :}^\\top \\mathbf{ z},\n",
    "$$ for each element of the vector valued function, where\n",
    "$\\mathbf{ w}_{:, j}$ is the $j$th column of the matrix $\\mathbf{W}$. In\n",
    "that context each column of $\\mathbf{W}$ is a vector of *regression\n",
    "weights*. This is a multiple input and multiple output regression. Our\n",
    "inputs (or covariates) have dimensionality greater than 1 and our\n",
    "outputs (or response variables) also have dimensionality greater than\n",
    "one. Just as in a standard regression, we are assuming that we don’t\n",
    "observe the function directly (note that this *also* makes the function\n",
    "a *type* of latent variable), but we observe some corrupted variant of\n",
    "the function, where the corruption is given by $\\boldsymbol{ \\epsilon}$.\n",
    "Just as in linear regression we can assume that this corruption is given\n",
    "by Gaussian noise, where the noise for the $j$th element of\n",
    "$\\mathbf{ y}$ is by, $$\n",
    "\\epsilon_j \\sim \\mathcal{N}\\left(0,\\sigma^2_j\\right).\n",
    "$$ Of course, just as in a regression problem we also need to make an\n",
    "assumption across the individual data points to form our full\n",
    "likelihood. Our data set now consists of many observations of\n",
    "$\\mathbf{ y}$ for diffetent individuals. We store these observations in\n",
    "a *design matrix*, $\\mathbf{Y}$, where each *row* of $\\mathbf{Y}$\n",
    "contains the observation for one individual. To emphasize that\n",
    "$\\mathbf{ y}$ is a vector derived from a row of $\\mathbf{Y}$ we\n",
    "represent the observation of the features associated with the $i$th\n",
    "individual by $\\mathbf{ y}_{i, :}$, and place each individual in our\n",
    "data matrix,\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}\n",
    "= \\begin{bmatrix} \\mathbf{ y}_{1, :}^\\top \\\\ \\mathbf{ y}_{2, :}^\\top \\\\ \\vdots \\\\\n",
    "\\mathbf{ y}_{n, :}^\\top\\end{bmatrix},\n",
    "$$ where we have $n$ data points. Our data matrix therefore has $n$ rows\n",
    "and $p$ columns. The point to notice here is that each data obsesrvation\n",
    "appears as a row vector in the design matrix (thus the transpose\n",
    "operation inside the brackets). Our prediction functions are now\n",
    "actually a *matrix value* function, $$\n",
    "\\mathbf{F} = \\mathbf{Z}\\mathbf{W}^\\top,\n",
    "$$ where for each matrix the data points are in the rows and the data\n",
    "features are in the columns. This implies that if we have $q$ inputs to\n",
    "the function we have $\\mathbf{F}\\in \\Re^{n\\times p}$,\n",
    "$\\mathbf{W}\\in \\Re^{p \\times q}$ and $\\mathbf{Z}\\in \\Re^{n\\times q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Show that, given all the definitions above, if, $$\n",
    "\\mathbf{F} = \\mathbf{Z}\\mathbf{W}^\\top\n",
    "$$ and the elements of the vector valued function $\\mathbf{F}$ are given\n",
    "by $$\n",
    "f_{i, j} = f_j(\\mathbf{ z}_{i, :}),\n",
    "$$ where $\\mathbf{ z}_{i, :}$ is the $i$th row of the latent variables,\n",
    "$\\mathbf{Z}$, then show that $$\n",
    "f_j(\\mathbf{ z}_{i, :}) = \\mathbf{ w}_{j, :}^\\top\n",
    "\\mathbf{ z}_{i, :}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Variables vs Linear Regression\n",
    "-------------------------------------\n",
    "\n",
    "The difference between this model and a multiple output regression is\n",
    "that in the regression case we are provided with the covariates\n",
    "$\\mathbf{Z}$, here they are *latent variables*. These variables are\n",
    "unknown. Just as we have done in the past for unknowns, we now treat\n",
    "them with a probability distribution. In *factor analysis* we assume\n",
    "that the latent variables have a Gaussian density which is independent\n",
    "across both across the latent variables associated with the different\n",
    "data points, and across those associated with different data features,\n",
    "so we have, $$\n",
    "x_{i,j} \\sim\n",
    "\\mathcal{N}\\left(0,1\\right),\n",
    "$$ and we can write the density governing the latent variable associated\n",
    "with a single point as, $$\n",
    "\\mathbf{ z}_{i, :} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}\\right).\n",
    "$$ If we consider the values of the function for the $i$th data point as\n",
    "$$\n",
    "\\mathbf{f}_{i, :} =\n",
    "\\mathbf{f}(\\mathbf{ z}_{i, :}) = \\mathbf{W}\\mathbf{ z}_{i, :} \n",
    "$$ then we can use the rules for multivariate Gaussian relationships to\n",
    "write that $$\n",
    "\\mathbf{f}_{i, :} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top\\right)\n",
    "$$ which implies that the distribution for $\\mathbf{ y}_{i, :}$ is given\n",
    "by $$\n",
    "\\mathbf{ y}_{i, :} = \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\boldsymbol{\\Sigma}\\right)\n",
    "$$ where $\\boldsymbol{\\Sigma}$ the covariance of the noise variable,\n",
    "$\\epsilon_{i, :}$ which for factor analysis is a diagonal matrix\n",
    "(because we have assumed that the noise was *independent* across the\n",
    "features), $$\n",
    "\\boldsymbol{\\Sigma} = \\begin{bmatrix}\\sigma^2_{1} & 0 & 0 & 0\\\\\n",
    "0 & \\sigma^2_{2} & 0 & 0\\\\\n",
    "                                     0 & 0 & \\ddots &\n",
    "0\\\\\n",
    "                                     0 & 0 & 0 & \\sigma^2_p\\end{bmatrix}.\n",
    "$$ For completeness, we could also add in a *mean* for the data vector\n",
    "$\\boldsymbol{ \\mu}$, $$\n",
    "\\mathbf{ y}_{i, :} = \\mathbf{W}\\mathbf{ z}_{i, :} +\n",
    "\\boldsymbol{ \\mu}+ \\boldsymbol{ \\epsilon}_{i, :}\n",
    "$$ which would give our marginal distribution for $\\mathbf{ y}_{i, :}$ a\n",
    "mean $\\boldsymbol{ \\mu}$. However, the maximum likelihood solution for\n",
    "$\\boldsymbol{ \\mu}$ turns out to equal the empirical mean of the data,\n",
    "$$\n",
    "\\boldsymbol{ \\mu}= \\frac{1}{n} \\sum_{i=1}^n\n",
    "\\mathbf{ y}_{i, :},\n",
    "$$ *regardless* of the form of the covariance,\n",
    "$\\mathbf{C}= \\mathbf{W}\\mathbf{W}^\\top + \\boldsymbol{\\Sigma}$. As a\n",
    "result it is very common to simply preprocess the data and ensure it is\n",
    "zero mean. We will follow that convention for this session.\n",
    "\n",
    "The prior density over latent variables is independent, and the\n",
    "likelihood is independent, that means that the marginal likelihood here\n",
    "is also independent over the data points. Factor analysis was developed\n",
    "mainly in psychology and the social sciences for understanding\n",
    "personality and intelligence. [Charles\n",
    "Spearman](http://en.wikipedia.org/wiki/Charles_Spearman) was concerned\n",
    "with the measurements of “the abilities of man” and is credited with the\n",
    "earliest version of factor analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis\n",
    "============================\n",
    "\n",
    "In 1933 [Harold\n",
    "Hotelling](http://en.wikipedia.org/wiki/Harold_Hotelling) published on\n",
    "*principal component analysis* the first mention of this approach\n",
    "(Hotelling, 1933). Hotelling’s inspiration was to provide mathematical\n",
    "foundation for factor analysis methods that were by then widely used\n",
    "within psychology and the social sciences. His model was a factor\n",
    "analysis model, but he considered the noiseless ‘limit’ of the model. In\n",
    "other words he took $\\sigma^2_i \\rightarrow 0$ so that he had\n",
    "\n",
    "$$\n",
    "\\mathbf{ y}_{i, :} \\sim \\lim_{\\sigma^2 \\rightarrow 0} \\mathcal{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right).\n",
    "$$ The paper had two unfortunate effects. Firstly, the resulting model\n",
    "is no longer valid probablistically, because the covariance of this\n",
    "Gaussian is ‘degenerate’. Because $\\mathbf{W}\\mathbf{W}^\\top$ has rank\n",
    "of at most $q$ where $q<p$ (due to the dimensionality reduction) the\n",
    "determinant of the covariance is zero, meaning the inverse doesn’t exist\n",
    "so the density, $$\n",
    "p(\\mathbf{ y}_{i, :}|\\mathbf{W}) =\n",
    "\\lim_{\\sigma^2 \\rightarrow 0} \\frac{1}{(2\\pi)^\\frac{p}{2}\n",
    "|\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}|^{\\frac{1}{2}}}\n",
    "\\exp\\left(-\\frac{1}{2}\\mathbf{ y}_{i, :}\\left[\\mathbf{W}\\mathbf{W}^\\top+ \\sigma^2\n",
    "\\mathbf{I}\\right]^{-1}\\mathbf{ y}_{i, :}\\right),\n",
    "$$ is *not* valid for $q<p$ (where $\\mathbf{W}\\in \\Re^{p\\times q}$).\n",
    "This mathematical consequence is a probability density which has no\n",
    "‘support’ in large regions of the space for $\\mathbf{ y}_{i, :}$. There\n",
    "are regions for which the probability of $\\mathbf{ y}_{i, :}$ is zero.\n",
    "These are any regions that lie off the hyperplane defined by mapping\n",
    "from $\\mathbf{ z}$ to $\\mathbf{ y}$ with the matrix $\\mathbf{W}$. In\n",
    "factor analysis the noise corruption, $\\boldsymbol{ \\epsilon}$, allows\n",
    "for points to be found away from the hyperplane. In Hotelling’s PCA the\n",
    "noise variance is zero, so there is only support for points that fall\n",
    "precisely on the hyperplane. Secondly, Hotelling explicity chose to\n",
    "rename factor analysis as principal component analysis, arguing that the\n",
    "factors social scientist sought were different in nature to the concept\n",
    "of a mathematical factor. This was unfortunate because the factor\n",
    "loadings, $\\mathbf{W}$ can also be seen as factors in the mathematical\n",
    "sense because the model Hotelling defined is a Gaussian model with\n",
    "covariance given by $\\mathbf{C}= \\mathbf{W}\\mathbf{W}^\\top$ so\n",
    "$\\mathbf{W}$ is a *factor* of the covariance in the mathematical sense,\n",
    "as well as a factor loading.\n",
    "\n",
    "However, the paper had one great advantage over standard approaches to\n",
    "factor analysis. Despite the fact that the model was a special case that\n",
    "is subsumed by the more general approach of factor analysis it is this\n",
    "special case that leads to a particular algorithm, namely that the\n",
    "factor loadings (or principal components as Hotelling referred to them)\n",
    "are given by an *eigenvalue decomposition* of the empirical covariance\n",
    "matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the Marginal Likelihood\n",
    "--------------------------------------\n",
    "\n",
    "$$\n",
    "\\mathbf{ y}_{i,:}=\\mathbf{W}\\mathbf{ z}_{i,:}+\\boldsymbol{ \\epsilon}_{i,:},\\quad \\mathbf{ z}_{i,:} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}\\right), \\quad \\boldsymbol{ \\epsilon}_{i,:} \\sim \\mathcal{N}\\left(\\mathbf{0},\\sigma^{2}\\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{W}\\mathbf{ z}_{i,:} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{W}\\mathbf{ z}_{i, :} + \\boldsymbol{ \\epsilon}_{i, :} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "**Probabilistic PCA Max. Likelihood Soln** (Tipping and Bishop (1999))\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/dimred/ppca_graph.png\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Graphical model representing probabilistic PCA.</i>\n",
    "\n",
    "$$p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{ y}_{i, :}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalue Decomposition\n",
    "------------------------\n",
    "\n",
    "Eigenvalue problems are widespreads in physics and mathematics, they are\n",
    "often written as a matrix/vector equation but we prefer to write them as\n",
    "a full matrix equation. In an eigenvalue problem you are looking to find\n",
    "a matrix of eigenvectors, $\\mathbf{U}$ and a *diagonal* matrix of\n",
    "eigenvalues, $\\boldsymbol{\\Lambda}$ that satisfy the *matrix* equation\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{U} = \\mathbf{U}\\boldsymbol{\\Lambda}.\n",
    "$$ where $\\mathbf{A}$ is your matrix of interest. This equation is not\n",
    "trivially solvable through matrix inverse because matrix multiplication\n",
    "is not [commutative](http://en.wikipedia.org/wiki/Commutative_property),\n",
    "so premultiplying by $\\mathbf{U}^{-1}$ gives $$\n",
    "\\mathbf{U}^{-1}\\mathbf{A}\\mathbf{U}\n",
    "= \\boldsymbol{\\Lambda}, \n",
    "$$ where we remember that $\\boldsymbol{\\Lambda}$ is a *diagonal* matrix,\n",
    "so the eigenvectors can be used to *diagonalise* the matrix. When\n",
    "performing the eigendecomposition on a Gaussian covariances,\n",
    "diagonalisation is very important because it returns the covariance to a\n",
    "form where there is no correlation between points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "Positive Definite\n",
    "-----------------\n",
    "\n",
    "We are interested in the case where $\\mathbf{A}$ is a covariance matrix,\n",
    "which implies it is *positive definite*. A positive definite matrix is\n",
    "one for which the inner product, $$\n",
    "\\mathbf{ w}^\\top \\mathbf{C}\\mathbf{ w}\n",
    "$$ is positive for *all* values of the vector $\\mathbf{ w}$ other than\n",
    "the zero vector. One way of creating a positive definite matrix is to\n",
    "assume that the symmetric and positive definite matrix\n",
    "$\\mathbf{C}\\in \\Re^{p\\times p}$ is factorised into,\n",
    "$\\mathbf{A}in \\Re^{p\\times p}$, a *full rank* matrix, so that $$\n",
    "\\mathbf{C}= \\mathbf{A}^\\top\n",
    "\\mathbf{A}.\n",
    "$$ This ensures that $\\mathbf{C}$ must be positive definite because $$\n",
    "\\mathbf{ w}^\\top \\mathbf{C}\\mathbf{ w}=\\mathbf{ w}^\\top\n",
    "\\mathbf{A}^\\top\\mathbf{A}\\mathbf{ w}\n",
    "$$ and if we now define a new *vector* $\\mathbf{b}$ as $$\n",
    "\\mathbf{b} = \\mathbf{A}\\mathbf{ w}\n",
    "$$ we can now rewrite as $$\n",
    "\\mathbf{ w}^\\top \\mathbf{C}\\mathbf{ w}= \\mathbf{b}^\\top\\mathbf{b} = \\sum_{i}\n",
    "b_i^2\n",
    "$$ which, since it is a sum of squares, is positive or zero. The\n",
    "constraint that $\\mathbf{A}$ must be *full rank* ensures that there is\n",
    "no vector $\\mathbf{ w}$, other than the zero vector, which causes the\n",
    "vector $\\mathbf{b}$ to be all zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "If $\\mathbf{C}=\\mathbf{A}^\\top \\mathbf{A}$ then express $c_{i,j}$, the\n",
    "value of the element at the $i$th row and the $j$th column of\n",
    "$\\mathbf{C}$, in terms of the columns of $\\mathbf{A}$. Use this to show\n",
    "that (i) the matrix is symmetric and (ii) the matrix has positive\n",
    "elements along its diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors of a Symmetric Matric\n",
    "----------------------------------\n",
    "\n",
    "Symmetric matrices have *orthonormal* eigenvectors. This means that\n",
    "$\\mathbf{U}$ is an [orthogonal\n",
    "matrix](http://en.wikipedia.org/wiki/Orthogonal_matrix),\n",
    "$\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$. This implies that\n",
    "$\\mathbf{u}_{:, i} ^\\top \\mathbf{u}_{:, j}$ is equal to 0 if $i\\neq j$\n",
    "and 1 if $i=j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hotelling, H., 1933. Analysis of a complex of statistical variables into\n",
    "principal components. Journal of Educational Psychology 24, 417–441.\n",
    "\n",
    "Le Cun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E.,\n",
    "Hubbard, W., Jackel, L.D., 1989. Backpropagation applied to handwritten\n",
    "zip code recognition. Neural Computation 1, 541–551.\n",
    "<https://doi.org/10.1162/neco.1989.1.4.541>\n",
    "\n",
    "Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component\n",
    "analysis. Journal of the Royal Statistical Society, B 6, 611–622.\n",
    "<https://doi.org/doi:10.1111/1467-9868.00196>"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
